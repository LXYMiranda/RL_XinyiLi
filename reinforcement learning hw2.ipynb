{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.3\n"
     ]
    }
   ],
   "source": [
    "# set the environment of the game\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import time                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     #os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the environment of the game\n",
    "env = gym.make('CartPole-v0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the training hyperparameters\n",
    "\n",
    "state_size = 4 # our input is an observation with four elements\n",
    "action_size = env.action_space.n  # number of actions (push to left or push to right)\n",
    "possible_actions = np.identity(2, dtype = int).tolist()\n",
    "\n",
    "\n",
    "# training hyperparameters\n",
    "learning_rate = 0.001\n",
    "\n",
    "number_epoch = 500 # number of epochs for training\n",
    "batch_size = 1000 # defines number of samples work though\n",
    "\n",
    "training = True \n",
    "hidden_size_1 = 53\n",
    "hidden_size_2 = 34\n",
    "\n",
    "max_steps = 200 # Max steps per episode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the discounted some of reward from current step onward\n",
    "def discount_rewards(r, gamma = 0.95, constant_baseline = False):\n",
    "    discounted_r = np.zeros_like(r) #make a vector of zeros with the size of input\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, len(r))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "        \n",
    "    if constant_baseline: # do normalization for reward to have more smooth gradient\n",
    "        mean = np.mean(discounted_r)\n",
    "        std = np.std(discounted_r)\n",
    "        discounted_r = (discounted_r - mean)/(std)\n",
    "        \n",
    "    return discounted_r \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the policy network \n",
    "\n",
    "class PGNetwork():\n",
    "    def __init__(self, state_size, action_size, learning_rate, hidden_size_1, hidden_size_2, name = 'PGNetwork'):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_size_1 = hidden_size_1\n",
    "        self.hidden_size_2 = hidden_size_2\n",
    "     \n",
    "        \n",
    "        # generate a network such that with a given state, the policy gives an action\n",
    "        with tf.variable_scope(name):\n",
    "            with tf.name_scope('inputs'):\n",
    "            # we create placeholder\n",
    "                self.inputs = tf.placeholder(tf.float32, shape = [None, state_size], name = 'inputs')\n",
    "                self.actions = tf.placeholder(tf.int32, shape = [None, action_size], name = 'actions')\n",
    "                self.discounted_episode_rewards = tf.placeholder(tf.float32, shape = [None, ], name = 'discounted_episode_rewards')\n",
    "            \n",
    "            # CNN is often used for image process, because here we are not dealing with image, we use general fully connected layers\n",
    "            with tf.name_scope('layer1'):\n",
    "                # filters gives the number of filters in the hidden nn\n",
    "                self.layer1 = tf.contrib.layers.fully_connected(inputs = self.inputs,\n",
    "                                             num_outputs = self.hidden_size_1,\n",
    "                                             activation_fn = tf.nn.elu,\n",
    "                                             weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "                                       \n",
    "\n",
    "            with tf.name_scope('layer2'):\n",
    "                # filters gives the number of filters in the hidden nn\n",
    "                self.layer2 = tf.contrib.layers.fully_connected(inputs = self.layer1,\n",
    "                                             num_outputs = self.hidden_size_2,   \n",
    "                                             activation_fn = tf.nn.elu,\n",
    "                                             weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    " \n",
    "            with tf.name_scope('logits'):\n",
    "                # get the action distribution from the fully connected NN\n",
    "                self.logits = tf.layers.dense(inputs = self.layer2,\n",
    "                                             kernel_initializer = tf.contrib.layers.xavier_initializer(),      \n",
    "                                             units = self.action_size, \n",
    "                                             activation = None)\n",
    "                \n",
    "            with tf.name_scope('softmax'):\n",
    "                self.action_distribution = tf.nn.softmax(self.logits)\n",
    "                \n",
    "            # define the loss function\n",
    "            with tf.name_scope('loss'):\n",
    "                self.cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.actions)\n",
    "                #self.log_action_probability = tf.nn.softmax_cross_entropy_with_logits(logits = self.outputlayer, labels = self.actions)\n",
    "                self.weighted_likelihoods = tf.multiply(self.cross_entropy, self.discounted_episode_rewards)\n",
    "                self.loss = tf.reduce_mean(self.weighted_likelihoods)\n",
    "                \n",
    "            with tf.name_scope('train'):\n",
    "                self.optimizer = tf.train.AdamOptimizer(self.learning_rate,  beta1=0.9, beta2=0.99)\n",
    "                self.train_opt = self.optimizer.minimize(self.loss)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value estimator network\n",
    "class VENetwork():\n",
    "    def __init__(self, state_size, learning_rate,  name = 'VENetwork'):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.output_size = 1\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_size_1 = 16\n",
    "        #self.hidden_size_2 = hidden_size_2\n",
    "     \n",
    "        \n",
    "        # generate a network such that with a given state, the policy gives an action\n",
    "        with tf.variable_scope(name):\n",
    "            with tf.name_scope('inputs'):\n",
    "            # we create placeholder\n",
    "                self.inputs = tf.placeholder(tf.float32, shape = [None, state_size], name = 'inputs')\n",
    "                self.discounted_episode_rewards = tf.placeholder(tf.float32, shape = [None, ], name = 'discounted_episode_rewards')\n",
    "            \n",
    "            # CNN is often used for image process, because here we are not dealing with image, we use general fully connected layers\n",
    "            with tf.name_scope('layer1'):\n",
    "                # filters gives the number of filters in the convolution nn\n",
    "                self.layer1 = tf.contrib.layers.fully_connected(inputs = self.inputs,\n",
    "                                             num_outputs = self.hidden_size_1,\n",
    "                                             activation_fn = tf.nn.elu,\n",
    "                                             weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "                                       \n",
    "            \n",
    "            #with tf.name_scope('layer2'):\n",
    "                # filters gives the number of filters in the convolution nn\n",
    "            #    self.layer2 = tf.contrib.layers.fully_connected(inputs = self.layer1,\n",
    "            #                                 num_outputs = self.hidden_size_2,   \n",
    "            #                                 activation_fn = tf.nn.elu,\n",
    "            #                                 weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "                \n",
    "             \n",
    "            with tf.name_scope('output'):\n",
    "                # get the action distribution from the fully connected NN\n",
    "                self.output_layer = tf.layers.dense(inputs = self.layer1,\n",
    "                                             kernel_initializer = tf.contrib.layers.xavier_initializer(),      \n",
    "                                             units = self.output_size, \n",
    "                                             activation = None)\n",
    "                \n",
    "                self.state_value_estimation = tf.squeeze(self.output_layer)\n",
    "                \n",
    "            # define the loss function\n",
    "            with tf.name_scope('loss'):\n",
    "\n",
    "                self.loss = tf.reduce_mean(tf.squared_difference(self.state_value_estimation, self.discounted_episode_rewards))\n",
    "                \n",
    "            with tf.name_scope('train'):\n",
    "                self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "                self.train_opt = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the environment \n",
    "# initialize network and session\n",
    "tf.reset_default_graph()\n",
    "PGN = PGNetwork(state_size, action_size, learning_rate, hidden_size_1, hidden_size_2)\n",
    "VEN = VENetwork(state_size, learning_rate)\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the policy until it reached maximum batch number and outputs information of each step (batch number)\n",
    "# for each episode\n",
    "def make_batch(batch_size):\n",
    "    states, actions, rewards_of_episode, rewards_of_batch,rewards_of_episode0, discounted_rewards = [],[],[],[],[],[]\n",
    "    # keep track of how many episodes in our batch (useful when we need to calculate the average reward)\n",
    "    episode_num = 1\n",
    "    # get a new state\n",
    "    state = env.reset()\n",
    "   \n",
    "    while True:\n",
    "        \n",
    "        action_probability_distribution = sess.run(PGN.action_distribution, feed_dict = {PGN.inputs: state.reshape(1, state_size)})\n",
    "        state_value_estimation = sess.run(VEN.state_value_estimation, feed_dict = {VEN.inputs: state.reshape(1, state_size)})\n",
    "        action = np.random.choice(range(action_probability_distribution.shape[1]), p = action_probability_distribution.ravel())\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        action_ = [0,0]\n",
    "        action_[action] = 1\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action_)\n",
    "        rewards_of_episode.append(reward)\n",
    "        rewards_of_episode0.append(reward-state_value_estimation)\n",
    "\n",
    "        if done:\n",
    "            rewards_of_batch.append(rewards_of_episode)\n",
    "    \n",
    "            discounted_rewards.append(discount_rewards(rewards_of_episode0, gamma = 0.95, constant_baseline = True))\n",
    "            \n",
    "            if len(np.concatenate(rewards_of_batch)) > batch_size:\n",
    "                break\n",
    "                \n",
    "            rewards_of_episode = []\n",
    "            rewards_of_episode0 = []\n",
    "            episode_num +=1\n",
    "            \n",
    "            state = env.reset()\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    return np.stack(np.array(states)), np.stack(np.array(actions)), np.concatenate(rewards_of_batch), np.concatenate(discounted_rewards), episode_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1028,) ()\n",
      "epoch 1\n",
      "====================================\n",
      "Epoch:  1 / 500\n",
      "------------\n",
      "Number of training episodes: 41\n",
      "Total reward:1028.0\n",
      "Mean Reward of that batch 25.073170731707318\n",
      "Average Reward of all training: 25.073170731707318\n",
      "Max reward for a batch so far: 1028.0\n",
      "check 1028.0\n",
      "Training loss:0.0067292326129972935\n",
      "Cross Entropy:[0.6949106  0.71694565 0.69459105 ... 0.7402917  0.67389506 0.642808  ]\n",
      "VE Training loss:1.0363086462020874\n",
      "check (1002,) ()\n",
      "epoch 2\n",
      "====================================\n",
      "Epoch:  2 / 500\n",
      "------------\n",
      "Number of training episodes: 43\n",
      "Total reward:1002.0\n",
      "Mean Reward of that batch 23.302325581395348\n",
      "Average Reward of all training: 24.187748156551333\n",
      "Max reward for a batch so far: 1028.0\n",
      "check 1002.0\n",
      "Training loss:-0.0003793962823692709\n",
      "Cross Entropy:[0.6999422  0.6911365  0.67745197 ... 0.64025176 0.77001095 0.6368575 ]\n",
      "VE Training loss:1.072463870048523\n",
      "check (1011,) ()\n",
      "epoch 3\n",
      "====================================\n",
      "Epoch:  3 / 500\n",
      "------------\n",
      "Number of training episodes: 48\n",
      "Total reward:1011.0\n",
      "Mean Reward of that batch 21.0625\n",
      "Average Reward of all training: 23.145998771034225\n",
      "Max reward for a batch so far: 1028.0\n",
      "check 1011.0\n",
      "Training loss:-0.002989135915413499\n",
      "Cross Entropy:[0.6755411  0.69518006 0.7095911  ... 0.6640363  0.7304095  0.6644502 ]\n",
      "VE Training loss:1.0195958614349365\n",
      "check (1005,) ()\n",
      "epoch 4\n",
      "====================================\n",
      "Epoch:  4 / 500\n",
      "------------\n",
      "Number of training episodes: 43\n",
      "Total reward:1005.0\n",
      "Mean Reward of that batch 23.372093023255815\n",
      "Average Reward of all training: 23.202522334089622\n",
      "Max reward for a batch so far: 1028.0\n",
      "check 1005.0\n",
      "Training loss:-0.0058892229571938515\n",
      "Cross Entropy:[0.72507614 0.6400087  0.6604171  ... 0.65505373 0.7047475  0.7483453 ]\n",
      "VE Training loss:1.0464246273040771\n",
      "check (1039,) ()\n",
      "epoch 5\n",
      "====================================\n",
      "Epoch:  5 / 500\n",
      "------------\n",
      "Number of training episodes: 36\n",
      "Total reward:1039.0\n",
      "Mean Reward of that batch 28.86111111111111\n",
      "Average Reward of all training: 24.33424008949392\n",
      "Max reward for a batch so far: 1039.0\n",
      "check 1039.0\n",
      "Training loss:-0.005285268649458885\n",
      "Cross Entropy:[0.73637974 0.78045857 0.8154522  ... 0.9852551  0.42223802 0.4485439 ]\n",
      "VE Training loss:0.9953294992446899\n",
      "check (1015,) ()\n",
      "epoch 6\n",
      "====================================\n",
      "Epoch:  6 / 500\n",
      "------------\n",
      "Number of training episodes: 40\n",
      "Total reward:1015.0\n",
      "Mean Reward of that batch 25.375\n",
      "Average Reward of all training: 24.507700074578267\n",
      "Max reward for a batch so far: 1039.0\n",
      "check 1015.0\n",
      "Training loss:-0.011245783418416977\n",
      "Cross Entropy:[0.64345306 0.6803995  0.74810314 ... 0.68804383 0.76845336 0.55319184]\n",
      "VE Training loss:0.9774361252784729\n",
      "check (1046,) ()\n",
      "epoch 7\n",
      "====================================\n",
      "Epoch:  7 / 500\n",
      "------------\n",
      "Number of training episodes: 37\n",
      "Total reward:1046.0\n",
      "Mean Reward of that batch 28.27027027027027\n",
      "Average Reward of all training: 25.045210102534266\n",
      "Max reward for a batch so far: 1046.0\n",
      "check 1046.0\n",
      "Training loss:-0.017646515741944313\n",
      "Cross Entropy:[0.63495535 0.7130682  0.79498655 ... 0.47297737 0.5250648  0.8178832 ]\n",
      "VE Training loss:1.0369199514389038\n",
      "check (1093,) ()\n",
      "epoch 8\n",
      "====================================\n",
      "Epoch:  8 / 500\n",
      "------------\n",
      "Number of training episodes: 32\n",
      "Total reward:1093.0\n",
      "Mean Reward of that batch 34.15625\n",
      "Average Reward of all training: 26.184090089717486\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1093.0\n",
      "Training loss:-0.01163424551486969\n",
      "Cross Entropy:[0.7605613  0.8571414  0.49160132 ... 0.71450925 0.82114285 0.49730316]\n",
      "VE Training loss:0.942741334438324\n",
      "check (1004,) ()\n",
      "epoch 9\n",
      "====================================\n",
      "Epoch:  9 / 500\n",
      "------------\n",
      "Number of training episodes: 34\n",
      "Total reward:1004.0\n",
      "Mean Reward of that batch 29.529411764705884\n",
      "Average Reward of all training: 26.55579249804953\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1004.0\n",
      "Training loss:-0.0048752957955002785\n",
      "Cross Entropy:[0.62057424 0.649165   0.76392436 ... 0.9083874  0.43765807 0.4894842 ]\n",
      "VE Training loss:1.0024834871292114\n",
      "check (1003,) ()\n",
      "epoch 10\n",
      "====================================\n",
      "Epoch:  10 / 500\n",
      "------------\n",
      "Number of training episodes: 26\n",
      "Total reward:1003.0\n",
      "Mean Reward of that batch 38.57692307692308\n",
      "Average Reward of all training: 27.757905555936883\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1003.0\n",
      "Training loss:0.001625072443857789\n",
      "Cross Entropy:[0.7891849  0.5003152  0.8000969  ... 0.4532333  0.90665233 0.43047163]\n",
      "VE Training loss:1.1139267683029175\n",
      "Model saved\n",
      "check (1039,) ()\n",
      "epoch 11\n",
      "====================================\n",
      "Epoch:  11 / 500\n",
      "------------\n",
      "Number of training episodes: 30\n",
      "Total reward:1039.0\n",
      "Mean Reward of that batch 34.63333333333333\n",
      "Average Reward of all training: 28.382944444791107\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1039.0\n",
      "Training loss:-0.015378113836050034\n",
      "Cross Entropy:[0.626634   0.76788735 0.91931653 ... 0.7933533  0.9585531  1.1220198 ]\n",
      "VE Training loss:1.0113592147827148\n",
      "check (1019,) ()\n",
      "epoch 12\n",
      "====================================\n",
      "Epoch:  12 / 500\n",
      "------------\n",
      "Number of training episodes: 28\n",
      "Total reward:1019.0\n",
      "Mean Reward of that batch 36.392857142857146\n",
      "Average Reward of all training: 29.050437169629944\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1019.0\n",
      "Training loss:-0.011958065442740917\n",
      "Cross Entropy:[0.78013563 0.48065934 0.5999863  ... 0.80988806 0.9932429  0.36827376]\n",
      "VE Training loss:1.0074574947357178\n",
      "check (1029,) ()\n",
      "epoch 13\n",
      "====================================\n",
      "Epoch:  13 / 500\n",
      "------------\n",
      "Number of training episodes: 30\n",
      "Total reward:1029.0\n",
      "Mean Reward of that batch 34.3\n",
      "Average Reward of all training: 29.454249695043025\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1029.0\n",
      "Training loss:-0.021698057651519775\n",
      "Cross Entropy:[0.79466164 0.46255347 0.8071854  ... 0.31965655 0.3905525  0.4773    ]\n",
      "VE Training loss:1.0306757688522339\n",
      "check (1001,) ()\n",
      "epoch 14\n",
      "====================================\n",
      "Epoch:  14 / 500\n",
      "------------\n",
      "Number of training episodes: 25\n",
      "Total reward:1001.0\n",
      "Mean Reward of that batch 40.04\n",
      "Average Reward of all training: 30.21037471682567\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1001.0\n",
      "Training loss:-0.0023142751306295395\n",
      "Cross Entropy:[0.7884009  1.0023727  0.3513115  ... 0.26976308 1.2494899  0.24560478]\n",
      "VE Training loss:1.0299731492996216\n",
      "check (1005,) ()\n",
      "epoch 15\n",
      "====================================\n",
      "Epoch:  15 / 500\n",
      "------------\n",
      "Number of training episodes: 29\n",
      "Total reward:1005.0\n",
      "Mean Reward of that batch 34.6551724137931\n",
      "Average Reward of all training: 30.506694563290164\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1005.0\n",
      "Training loss:-0.004126252606511116\n",
      "Cross Entropy:[0.83832604 0.41665468 0.8581269  ... 0.6132088  0.7684789  0.49472255]\n",
      "VE Training loss:1.0844825506210327\n",
      "check (1002,) ()\n",
      "epoch 16\n",
      "====================================\n",
      "Epoch:  16 / 500\n",
      "------------\n",
      "Number of training episodes: 28\n",
      "Total reward:1002.0\n",
      "Mean Reward of that batch 35.785714285714285\n",
      "Average Reward of all training: 30.83663329594167\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1002.0\n",
      "Training loss:-0.018133345991373062\n",
      "Cross Entropy:[0.5664643  0.7748143  1.0121028  ... 0.12533909 0.15372089 1.7591764 ]\n",
      "VE Training loss:0.9909431338310242\n",
      "check (1009,) ()\n",
      "epoch 17\n",
      "====================================\n",
      "Epoch:  17 / 500\n",
      "------------\n",
      "Number of training episodes: 20\n",
      "Total reward:1009.0\n",
      "Mean Reward of that batch 50.45\n",
      "Average Reward of all training: 31.99036074912157\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1009.0\n",
      "Training loss:-0.007275128737092018\n",
      "Cross Entropy:[0.60116947 0.5698052  0.7809537  ... 0.2005155  1.4734174  0.1764905 ]\n",
      "VE Training loss:1.123762845993042\n",
      "check (1036,) ()\n",
      "epoch 18\n",
      "====================================\n",
      "Epoch:  18 / 500\n",
      "------------\n",
      "Number of training episodes: 25\n",
      "Total reward:1036.0\n",
      "Mean Reward of that batch 41.44\n",
      "Average Reward of all training: 32.51534070750371\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1036.0\n",
      "Training loss:-0.0070114354602992535\n",
      "Cross Entropy:[0.5740796  0.59305537 0.84141266 ... 1.2150799  0.23313585 0.31185985]\n",
      "VE Training loss:1.041433572769165\n",
      "check (1009,) ()\n",
      "epoch 19\n",
      "====================================\n",
      "Epoch:  19 / 500\n",
      "------------\n",
      "Number of training episodes: 21\n",
      "Total reward:1009.0\n",
      "Mean Reward of that batch 48.04761904761905\n",
      "Average Reward of all training: 33.33282904119399\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1009.0\n",
      "Training loss:0.0009943920886144042\n",
      "Cross Entropy:[0.8437635  0.3835955  0.55490565 ... 0.14007586 0.18856809 0.25433618]\n",
      "VE Training loss:1.0693962574005127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1043,) ()\n",
      "epoch 20\n",
      "====================================\n",
      "Epoch:  20 / 500\n",
      "------------\n",
      "Number of training episodes: 21\n",
      "Total reward:1043.0\n",
      "Mean Reward of that batch 49.666666666666664\n",
      "Average Reward of all training: 34.14952092246762\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1043.0\n",
      "Training loss:-0.0027646103408187628\n",
      "Cross Entropy:[0.58640707 0.5555722  0.59223884 ... 0.76034594 1.1122868  1.5080118 ]\n",
      "VE Training loss:1.0403532981872559\n",
      "Model saved\n",
      "check (1055,) ()\n",
      "epoch 21\n",
      "====================================\n",
      "Epoch:  21 / 500\n",
      "------------\n",
      "Number of training episodes: 23\n",
      "Total reward:1055.0\n",
      "Mean Reward of that batch 45.869565217391305\n",
      "Average Reward of all training: 34.707618269844936\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1055.0\n",
      "Training loss:-0.0026362822391092777\n",
      "Cross Entropy:[0.5280139  0.61956453 0.5116076  ... 0.46530277 0.74461997 0.39985812]\n",
      "VE Training loss:1.1169158220291138\n",
      "check (1024,) ()\n",
      "epoch 22\n",
      "====================================\n",
      "Epoch:  22 / 500\n",
      "------------\n",
      "Number of training episodes: 23\n",
      "Total reward:1024.0\n",
      "Mean Reward of that batch 44.52173913043478\n",
      "Average Reward of all training: 35.15371467259902\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1024.0\n",
      "Training loss:-0.005382616072893143\n",
      "Cross Entropy:[0.858706   0.3656541  0.8444245  ... 0.34576055 0.48536652 0.7162959 ]\n",
      "VE Training loss:1.0447230339050293\n",
      "check (1028,) ()\n",
      "epoch 23\n",
      "====================================\n",
      "Epoch:  23 / 500\n",
      "------------\n",
      "Number of training episodes: 21\n",
      "Total reward:1028.0\n",
      "Mean Reward of that batch 48.95238095238095\n",
      "Average Reward of all training: 35.75365668476345\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1028.0\n",
      "Training loss:0.0004270934732630849\n",
      "Cross Entropy:[0.8921088  0.34720752 0.542279   ... 0.93317646 1.3454082  0.17877778]\n",
      "VE Training loss:1.1261522769927979\n",
      "check (1017,) ()\n",
      "epoch 24\n",
      "====================================\n",
      "Epoch:  24 / 500\n",
      "------------\n",
      "Number of training episodes: 14\n",
      "Total reward:1017.0\n",
      "Mean Reward of that batch 72.64285714285714\n",
      "Average Reward of all training: 37.290706703850695\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1017.0\n",
      "Training loss:-0.00017318800382781774\n",
      "Cross Entropy:[0.8541484  1.1961268  0.23678462 ... 0.43969744 0.6163489  0.5702228 ]\n",
      "VE Training loss:1.1315052509307861\n",
      "check (1063,) ()\n",
      "epoch 25\n",
      "====================================\n",
      "Epoch:  25 / 500\n",
      "------------\n",
      "Number of training episodes: 18\n",
      "Total reward:1063.0\n",
      "Mean Reward of that batch 59.05555555555556\n",
      "Average Reward of all training: 38.161300657918886\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1063.0\n",
      "Training loss:-0.016415784135460854\n",
      "Cross Entropy:[0.5925285  0.5180029  0.60580444 ... 0.3312382  0.9848919  0.27680475]\n",
      "VE Training loss:1.0968098640441895\n",
      "check (1079,) ()\n",
      "epoch 26\n",
      "====================================\n",
      "Epoch:  26 / 500\n",
      "------------\n",
      "Number of training episodes: 18\n",
      "Total reward:1079.0\n",
      "Mean Reward of that batch 59.94444444444444\n",
      "Average Reward of all training: 38.999113880477566\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1079.0\n",
      "Training loss:-0.00903092510998249\n",
      "Cross Entropy:[0.85206586 0.3445826  0.5401264  ... 0.96139157 0.29162905 0.41521642]\n",
      "VE Training loss:1.1050881147384644\n",
      "check (1043,) ()\n",
      "epoch 27\n",
      "====================================\n",
      "Epoch:  27 / 500\n",
      "------------\n",
      "Number of training episodes: 16\n",
      "Total reward:1043.0\n",
      "Mean Reward of that batch 65.1875\n",
      "Average Reward of all training: 39.96905410712654\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1043.0\n",
      "Training loss:-0.01302526704967022\n",
      "Cross Entropy:[0.5634817  0.54225254 0.84517395 ... 2.107562   0.07385062 0.10545358]\n",
      "VE Training loss:1.1541110277175903\n",
      "check (1026,) ()\n",
      "epoch 28\n",
      "====================================\n",
      "Epoch:  28 / 500\n",
      "------------\n",
      "Number of training episodes: 13\n",
      "Total reward:1026.0\n",
      "Mean Reward of that batch 78.92307692307692\n",
      "Average Reward of all training: 41.3602692076962\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1026.0\n",
      "Training loss:-0.025078851729631424\n",
      "Cross Entropy:[0.57413286 0.5220189  0.8110182  ... 0.95271087 1.4349141  0.15188584]\n",
      "VE Training loss:1.0651531219482422\n",
      "check (1081,) ()\n",
      "epoch 29\n",
      "====================================\n",
      "Epoch:  29 / 500\n",
      "------------\n",
      "Number of training episodes: 12\n",
      "Total reward:1081.0\n",
      "Mean Reward of that batch 90.08333333333333\n",
      "Average Reward of all training: 43.040374867200924\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1081.0\n",
      "Training loss:-0.004928320646286011\n",
      "Cross Entropy:[0.58517635 0.5249709  0.84811926 ... 0.16926819 1.5073682  0.13587885]\n",
      "VE Training loss:1.1278448104858398\n",
      "check (1043,) ()\n",
      "epoch 30\n",
      "====================================\n",
      "Epoch:  30 / 500\n",
      "------------\n",
      "Number of training episodes: 14\n",
      "Total reward:1043.0\n",
      "Mean Reward of that batch 74.5\n",
      "Average Reward of all training: 44.08902903829423\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1043.0\n",
      "Training loss:-0.02058829925954342\n",
      "Cross Entropy:[0.863085   0.3242949  0.5174743  ... 1.1288083  0.21610533 1.2908026 ]\n",
      "VE Training loss:1.1885178089141846\n",
      "Model saved\n",
      "check (1032,) ()\n",
      "epoch 31\n",
      "====================================\n",
      "Epoch:  31 / 500\n",
      "------------\n",
      "Number of training episodes: 13\n",
      "Total reward:1032.0\n",
      "Mean Reward of that batch 79.38461538461539\n",
      "Average Reward of all training: 45.22759633978846\n",
      "Max reward for a batch so far: 1093.0\n",
      "check 1032.0\n",
      "Training loss:0.004786591045558453\n",
      "Cross Entropy:[0.5794894  0.9239001  0.3049216  ... 0.58280635 0.5942008  0.948328  ]\n",
      "VE Training loss:1.0450764894485474\n",
      "check (1109,) ()\n",
      "epoch 32\n",
      "====================================\n",
      "Epoch:  32 / 500\n",
      "------------\n",
      "Number of training episodes: 15\n",
      "Total reward:1109.0\n",
      "Mean Reward of that batch 73.93333333333334\n",
      "Average Reward of all training: 46.12465062083673\n",
      "Max reward for a batch so far: 1109.0\n",
      "check 1109.0\n",
      "Training loss:-0.01847977377474308\n",
      "Cross Entropy:[0.67982525 1.0701724  0.24598256 ... 0.6915768  0.51245713 0.7827578 ]\n",
      "VE Training loss:1.0081802606582642\n",
      "check (1030,) ()\n",
      "epoch 33\n",
      "====================================\n",
      "Epoch:  33 / 500\n",
      "------------\n",
      "Number of training episodes: 11\n",
      "Total reward:1030.0\n",
      "Mean Reward of that batch 93.63636363636364\n",
      "Average Reward of all training: 47.56439950009513\n",
      "Max reward for a batch so far: 1109.0\n",
      "check 1030.0\n",
      "Training loss:0.0009411219507455826\n",
      "Cross Entropy:[0.74998707 0.37280187 0.7859436  ... 0.42153692 0.792676   0.33903694]\n",
      "VE Training loss:1.1831694841384888\n",
      "check (1038,) ()\n",
      "epoch 34\n",
      "====================================\n",
      "Epoch:  34 / 500\n",
      "------------\n",
      "Number of training episodes: 11\n",
      "Total reward:1038.0\n",
      "Mean Reward of that batch 94.36363636363636\n",
      "Average Reward of all training: 48.94084764314045\n",
      "Max reward for a batch so far: 1109.0\n",
      "check 1038.0\n",
      "Training loss:-0.017715459689497948\n",
      "Cross Entropy:[0.6221157  0.47414023 0.6042811  ... 0.34892663 0.53166854 0.80234236]\n",
      "VE Training loss:1.225992202758789\n",
      "check (1071,) ()\n",
      "epoch 35\n",
      "====================================\n",
      "Epoch:  35 / 500\n",
      "------------\n",
      "Number of training episodes: 8\n",
      "Total reward:1071.0\n",
      "Mean Reward of that batch 133.875\n",
      "Average Reward of all training: 51.3675377104793\n",
      "Max reward for a batch so far: 1109.0\n",
      "check 1071.0\n",
      "Training loss:-0.016924027353525162\n",
      "Cross Entropy:[0.72274685 0.4019806  0.6805556  ... 0.5255463  0.64607805 0.46514505]\n",
      "VE Training loss:1.0671285390853882\n",
      "check (1118,) ()\n",
      "epoch 36\n",
      "====================================\n",
      "Epoch:  36 / 500\n",
      "------------\n",
      "Number of training episodes: 10\n",
      "Total reward:1118.0\n",
      "Mean Reward of that batch 111.8\n",
      "Average Reward of all training: 53.04621721852154\n",
      "Max reward for a batch so far: 1118.0\n",
      "check 1118.0\n",
      "Training loss:-0.024547109380364418\n",
      "Cross Entropy:[0.6877388  0.42401937 0.6673801  ... 2.0092528  0.08651288 2.1604507 ]\n",
      "VE Training loss:1.0479490756988525\n",
      "check (1006,) ()\n",
      "epoch 37\n",
      "====================================\n",
      "Epoch:  37 / 500\n",
      "------------\n",
      "Number of training episodes: 10\n",
      "Total reward:1006.0\n",
      "Mean Reward of that batch 100.6\n",
      "Average Reward of all training: 54.33145459099393\n",
      "Max reward for a batch so far: 1118.0\n",
      "check 1006.0\n",
      "Training loss:-0.02163056470453739\n",
      "Cross Entropy:[0.7650257  0.3591706  0.80052507 ... 0.19057626 1.4354845  0.15535502]\n",
      "VE Training loss:0.9433929324150085\n",
      "check (1073,) ()\n",
      "epoch 38\n",
      "====================================\n",
      "Epoch:  38 / 500\n",
      "------------\n",
      "Number of training episodes: 10\n",
      "Total reward:1073.0\n",
      "Mean Reward of that batch 107.3\n",
      "Average Reward of all training: 55.72536368070461\n",
      "Max reward for a batch so far: 1118.0\n",
      "check 1073.0\n",
      "Training loss:-0.005532439798116684\n",
      "Cross Entropy:[0.62489706 0.44223747 0.6696638  ... 0.7268891  1.0529742  0.26676875]\n",
      "VE Training loss:1.1276075839996338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1169,) ()\n",
      "epoch 39\n",
      "====================================\n",
      "Epoch:  39 / 500\n",
      "------------\n",
      "Number of training episodes: 11\n",
      "Total reward:1169.0\n",
      "Mean Reward of that batch 106.27272727272727\n",
      "Average Reward of all training: 57.021449926653915\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1169.0\n",
      "Training loss:-0.02011018991470337\n",
      "Cross Entropy:[0.69413507 1.1257586  0.2180802  ... 0.11261839 1.9308714  0.0835974 ]\n",
      "VE Training loss:1.083898901939392\n",
      "check (1046,) ()\n",
      "epoch 40\n",
      "====================================\n",
      "Epoch:  40 / 500\n",
      "------------\n",
      "Number of training episodes: 9\n",
      "Total reward:1046.0\n",
      "Mean Reward of that batch 116.22222222222223\n",
      "Average Reward of all training: 58.501469234043135\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1046.0\n",
      "Training loss:-0.030573656782507896\n",
      "Cross Entropy:[0.7390274 1.1778281 0.2090649 ... 0.4119439 0.6231837 0.9098507]\n",
      "VE Training loss:1.0924530029296875\n",
      "Model saved\n",
      "check (1146,) ()\n",
      "epoch 41\n",
      "====================================\n",
      "Epoch:  41 / 500\n",
      "------------\n",
      "Number of training episodes: 9\n",
      "Total reward:1146.0\n",
      "Mean Reward of that batch 127.33333333333333\n",
      "Average Reward of all training: 60.180295187684365\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1146.0\n",
      "Training loss:-0.0054768468253314495\n",
      "Cross Entropy:[0.5871659  0.9445877  1.3669884  ... 0.10950552 0.17457797 0.27459547]\n",
      "VE Training loss:1.1552326679229736\n",
      "check (1036,) ()\n",
      "epoch 42\n",
      "====================================\n",
      "Epoch:  42 / 500\n",
      "------------\n",
      "Number of training episodes: 8\n",
      "Total reward:1036.0\n",
      "Mean Reward of that batch 129.5\n",
      "Average Reward of all training: 61.83076434988235\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1036.0\n",
      "Training loss:-0.022683987393975258\n",
      "Cross Entropy:[0.66423273 1.0636315  0.24620569 ... 0.05560311 0.07814761 0.1100081 ]\n",
      "VE Training loss:0.9866487979888916\n",
      "check (1011,) ()\n",
      "epoch 43\n",
      "====================================\n",
      "Epoch:  43 / 500\n",
      "------------\n",
      "Number of training episodes: 8\n",
      "Total reward:1011.0\n",
      "Mean Reward of that batch 126.375\n",
      "Average Reward of all training: 63.3317930859316\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1011.0\n",
      "Training loss:-0.0027964843902736902\n",
      "Cross Entropy:[0.6639511  0.4001267  0.6976816  ... 0.7974106  0.34858155 0.9589267 ]\n",
      "VE Training loss:0.9600288271903992\n",
      "check (1033,) ()\n",
      "epoch 44\n",
      "====================================\n",
      "Epoch:  44 / 500\n",
      "------------\n",
      "Number of training episodes: 9\n",
      "Total reward:1033.0\n",
      "Mean Reward of that batch 114.77777777777777\n",
      "Average Reward of all training: 64.50102001074629\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1033.0\n",
      "Training loss:-0.006502772681415081\n",
      "Cross Entropy:[0.80252624 1.2650746  0.18251906 ... 1.2684847  1.8431938  0.08989125]\n",
      "VE Training loss:1.13882577419281\n",
      "check (1113,) ()\n",
      "epoch 45\n",
      "====================================\n",
      "Epoch:  45 / 500\n",
      "------------\n",
      "Number of training episodes: 7\n",
      "Total reward:1113.0\n",
      "Mean Reward of that batch 159.0\n",
      "Average Reward of all training: 66.60099734384082\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1113.0\n",
      "Training loss:-0.0043539865873754025\n",
      "Cross Entropy:[0.63527834 0.4229416  0.6133924  ... 0.15856098 0.28374213 0.9365941 ]\n",
      "VE Training loss:1.2110662460327148\n",
      "check (1083,) ()\n",
      "epoch 46\n",
      "====================================\n",
      "Epoch:  46 / 500\n",
      "------------\n",
      "Number of training episodes: 9\n",
      "Total reward:1083.0\n",
      "Mean Reward of that batch 120.33333333333333\n",
      "Average Reward of all training: 67.76909160448196\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1083.0\n",
      "Training loss:-0.005437598098069429\n",
      "Cross Entropy:[0.59794366 0.4310555  0.7887637  ... 0.24167426 1.1495488  0.17010884]\n",
      "VE Training loss:1.1573494672775269\n",
      "check (1110,) ()\n",
      "epoch 47\n",
      "====================================\n",
      "Epoch:  47 / 500\n",
      "------------\n",
      "Number of training episodes: 8\n",
      "Total reward:1110.0\n",
      "Mean Reward of that batch 138.75\n",
      "Average Reward of all training: 69.27932369800362\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1110.0\n",
      "Training loss:-0.010532478801906109\n",
      "Cross Entropy:[0.66224253 1.1585258  0.19337535 ... 0.321889   0.910225   0.22473244]\n",
      "VE Training loss:1.0204704999923706\n",
      "check (1065,) ()\n",
      "epoch 48\n",
      "====================================\n",
      "Epoch:  48 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1065.0\n",
      "Mean Reward of that batch 177.5\n",
      "Average Reward of all training: 71.53392112096186\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1065.0\n",
      "Training loss:-0.005666552111506462\n",
      "Cross Entropy:[0.745142   0.32019424 0.5991502  ... 0.33364877 0.81054527 0.28305596]\n",
      "VE Training loss:1.1294586658477783\n",
      "check (1160,) ()\n",
      "epoch 49\n",
      "====================================\n",
      "Epoch:  49 / 500\n",
      "------------\n",
      "Number of training episodes: 8\n",
      "Total reward:1160.0\n",
      "Mean Reward of that batch 145.0\n",
      "Average Reward of all training: 73.03322885318714\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1160.0\n",
      "Training loss:-0.009497536346316338\n",
      "Cross Entropy:[0.7285914  0.32757765 0.76478297 ... 0.3408701  0.59118575 0.48575664]\n",
      "VE Training loss:1.0239089727401733\n",
      "check (1027,) ()\n",
      "epoch 50\n",
      "====================================\n",
      "Epoch:  50 / 500\n",
      "------------\n",
      "Number of training episodes: 9\n",
      "Total reward:1027.0\n",
      "Mean Reward of that batch 114.11111111111111\n",
      "Average Reward of all training: 73.85478649834562\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1027.0\n",
      "Training loss:-0.007301230449229479\n",
      "Cross Entropy:[0.55539846 1.0184906  0.22731604 ... 0.5928342  0.43813708 0.7164634 ]\n",
      "VE Training loss:1.0867979526519775\n",
      "Model saved\n",
      "check (1020,) ()\n",
      "epoch 51\n",
      "====================================\n",
      "Epoch:  51 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1020.0\n",
      "Mean Reward of that batch 170.0\n",
      "Average Reward of all training: 75.73998676308395\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1020.0\n",
      "Training loss:-0.014405467547476292\n",
      "Cross Entropy:[0.63138735 0.3705108  0.73053914 ... 0.51426375 0.50676227 0.85211754]\n",
      "VE Training loss:1.0827652215957642\n",
      "check (1077,) ()\n",
      "epoch 52\n",
      "====================================\n",
      "Epoch:  52 / 500\n",
      "------------\n",
      "Number of training episodes: 7\n",
      "Total reward:1077.0\n",
      "Mean Reward of that batch 153.85714285714286\n",
      "Average Reward of all training: 77.24223976489276\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1077.0\n",
      "Training loss:-0.013290904462337494\n",
      "Cross Entropy:[0.57102454 0.4212555  0.54942894 ... 0.22125521 1.0189166  0.1909364 ]\n",
      "VE Training loss:0.9812272191047668\n",
      "check (1118,) ()\n",
      "epoch 53\n",
      "====================================\n",
      "Epoch:  53 / 500\n",
      "------------\n",
      "Number of training episodes: 7\n",
      "Total reward:1118.0\n",
      "Mean Reward of that batch 159.71428571428572\n",
      "Average Reward of all training: 78.79831610356055\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1118.0\n",
      "Training loss:-0.009359902702271938\n",
      "Cross Entropy:[0.70703644 1.2668221  1.9345534  ... 0.13940029 0.22262576 0.34826988]\n",
      "VE Training loss:1.0220715999603271\n",
      "check (1114,) ()\n",
      "epoch 54\n",
      "====================================\n",
      "Epoch:  54 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1114.0\n",
      "Mean Reward of that batch 185.66666666666666\n",
      "Average Reward of all training: 80.77735963250697\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1114.0\n",
      "Training loss:0.00349722383543849\n",
      "Cross Entropy:[0.60405064 0.36131656 0.6457151  ... 0.33623725 0.7523904  0.335291  ]\n",
      "VE Training loss:0.977652907371521\n",
      "check (1043,) ()\n",
      "epoch 55\n",
      "====================================\n",
      "Epoch:  55 / 500\n",
      "------------\n",
      "Number of training episodes: 7\n",
      "Total reward:1043.0\n",
      "Mean Reward of that batch 149.0\n",
      "Average Reward of all training: 82.01777127555229\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1043.0\n",
      "Training loss:0.0009668886195868254\n",
      "Cross Entropy:[0.710613   0.314007   0.6967418  ... 0.17812997 0.3540274  0.65412253]\n",
      "VE Training loss:1.06256103515625\n",
      "check (1081,) ()\n",
      "epoch 56\n",
      "====================================\n",
      "Epoch:  56 / 500\n",
      "------------\n",
      "Number of training episodes: 7\n",
      "Total reward:1081.0\n",
      "Mean Reward of that batch 154.42857142857142\n",
      "Average Reward of all training: 83.31082127828479\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1081.0\n",
      "Training loss:-0.002749557374045253\n",
      "Cross Entropy:[0.6760332  1.2769861  0.14477777 ... 2.2328322  0.04802041 0.07701373]\n",
      "VE Training loss:0.9899427890777588\n",
      "check (1071,) ()\n",
      "epoch 57\n",
      "====================================\n",
      "Epoch:  57 / 500\n",
      "------------\n",
      "Number of training episodes: 7\n",
      "Total reward:1071.0\n",
      "Mean Reward of that batch 153.0\n",
      "Average Reward of all training: 84.5334384488412\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1071.0\n",
      "Training loss:0.003952528350055218\n",
      "Cross Entropy:[0.7599937  0.27580792 0.77971697 ... 0.4994836  0.830498   0.33091468]\n",
      "VE Training loss:0.9968122839927673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1014,) ()\n",
      "epoch 58\n",
      "====================================\n",
      "Epoch:  58 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1014.0\n",
      "Mean Reward of that batch 169.0\n",
      "Average Reward of all training: 85.9897584755853\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1014.0\n",
      "Training loss:-0.013449572958052158\n",
      "Cross Entropy:[0.74079406 1.4036249  0.11906946 ... 0.74270093 0.28399792 0.5489378 ]\n",
      "VE Training loss:1.0092222690582275\n",
      "check (1128,) ()\n",
      "epoch 59\n",
      "====================================\n",
      "Epoch:  59 / 500\n",
      "------------\n",
      "Number of training episodes: 8\n",
      "Total reward:1128.0\n",
      "Mean Reward of that batch 141.0\n",
      "Average Reward of all training: 86.9221354505754\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1128.0\n",
      "Training loss:-0.023851288482546806\n",
      "Cross Entropy:[0.76364595 0.25603956 0.5633168  ... 0.10807535 0.19080973 1.2718754 ]\n",
      "VE Training loss:1.007515549659729\n",
      "check (1143,) ()\n",
      "epoch 60\n",
      "====================================\n",
      "Epoch:  60 / 500\n",
      "------------\n",
      "Number of training episodes: 7\n",
      "Total reward:1143.0\n",
      "Mean Reward of that batch 163.28571428571428\n",
      "Average Reward of all training: 88.19486176449438\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1143.0\n",
      "Training loss:0.004159221425652504\n",
      "Cross Entropy:[0.74536407 1.4279135  0.11509345 ... 0.56283927 0.8950048  0.3144602 ]\n",
      "VE Training loss:1.041462779045105\n",
      "Model saved\n",
      "check (1025,) ()\n",
      "epoch 61\n",
      "====================================\n",
      "Epoch:  61 / 500\n",
      "------------\n",
      "Number of training episodes: 7\n",
      "Total reward:1025.0\n",
      "Mean Reward of that batch 146.42857142857142\n",
      "Average Reward of all training: 89.149512742594\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1025.0\n",
      "Training loss:-0.01366033311933279\n",
      "Cross Entropy:[0.6299744  1.2526866  0.14168432 ... 0.01251263 0.02292114 0.04294133]\n",
      "VE Training loss:1.0631299018859863\n",
      "check (1143,) ()\n",
      "epoch 62\n",
      "====================================\n",
      "Epoch:  62 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1143.0\n",
      "Mean Reward of that batch 190.5\n",
      "Average Reward of all training: 90.78419802093926\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1143.0\n",
      "Training loss:-0.004243862349539995\n",
      "Cross Entropy:[0.70471776 0.2822255  0.66183496 ... 0.19717331 1.0928247  0.1636039 ]\n",
      "VE Training loss:1.1406244039535522\n",
      "check (1112,) ()\n",
      "epoch 63\n",
      "====================================\n",
      "Epoch:  63 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1112.0\n",
      "Mean Reward of that batch 185.33333333333334\n",
      "Average Reward of all training: 92.28497794653282\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1112.0\n",
      "Training loss:-0.0075647057965397835\n",
      "Cross Entropy:[0.6356802  1.3416425  0.11743774 ... 0.9789705  0.19711156 1.1050658 ]\n",
      "VE Training loss:1.0047144889831543\n",
      "check (1038,) ()\n",
      "epoch 64\n",
      "====================================\n",
      "Epoch:  64 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1038.0\n",
      "Mean Reward of that batch 173.0\n",
      "Average Reward of all training: 93.54615016611822\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1038.0\n",
      "Training loss:0.004330210387706757\n",
      "Cross Entropy:[0.58201486 0.35051277 0.84316844 ... 0.9516307  0.18567528 0.36293036]\n",
      "VE Training loss:0.9688904285430908\n",
      "check (1023,) ()\n",
      "epoch 65\n",
      "====================================\n",
      "Epoch:  65 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1023.0\n",
      "Mean Reward of that batch 170.5\n",
      "Average Reward of all training: 94.73005554817794\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1023.0\n",
      "Training loss:-0.011463888920843601\n",
      "Cross Entropy:[0.6655743  1.4108796  0.10344524 ... 0.18530156 0.35179338 0.6227449 ]\n",
      "VE Training loss:0.9816026091575623\n",
      "check (1025,) ()\n",
      "epoch 66\n",
      "====================================\n",
      "Epoch:  66 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1025.0\n",
      "Mean Reward of that batch 170.83333333333334\n",
      "Average Reward of all training: 95.88313551461968\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1025.0\n",
      "Training loss:0.0038402725476771593\n",
      "Cross Entropy:[0.74658734 1.5388279  0.08967079 ... 0.12790701 1.4804379  0.07943603]\n",
      "VE Training loss:0.9673258662223816\n",
      "check (1059,) ()\n",
      "epoch 67\n",
      "====================================\n",
      "Epoch:  67 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1059.0\n",
      "Mean Reward of that batch 176.5\n",
      "Average Reward of all training: 97.08637229798357\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1059.0\n",
      "Training loss:0.005331504158675671\n",
      "Cross Entropy:[0.60865355 0.3347747  0.84229046 ... 0.940099   0.2193656  1.0954742 ]\n",
      "VE Training loss:1.0127651691436768\n",
      "check (1168,) ()\n",
      "epoch 68\n",
      "====================================\n",
      "Epoch:  68 / 500\n",
      "------------\n",
      "Number of training episodes: 7\n",
      "Total reward:1168.0\n",
      "Mean Reward of that batch 166.85714285714286\n",
      "Average Reward of all training: 98.11241304150063\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1168.0\n",
      "Training loss:-0.0164339542388916\n",
      "Cross Entropy:[0.57206434 0.3464837  0.8600602  ... 2.0827868  0.06046506 0.11892891]\n",
      "VE Training loss:0.9196459650993347\n",
      "check (1060,) ()\n",
      "epoch 69\n",
      "====================================\n",
      "Epoch:  69 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1060.0\n",
      "Mean Reward of that batch 176.66666666666666\n",
      "Average Reward of all training: 99.25088048534361\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1060.0\n",
      "Training loss:-0.0002784944954328239\n",
      "Cross Entropy:[0.5985903  1.2692277  0.12834296 ... 0.47229314 0.9486267  0.21714146]\n",
      "VE Training loss:0.9979167580604553\n",
      "check (1130,) ()\n",
      "epoch 70\n",
      "====================================\n",
      "Epoch:  70 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1130.0\n",
      "Mean Reward of that batch 188.33333333333334\n",
      "Average Reward of all training: 100.52348695460061\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1130.0\n",
      "Training loss:0.002797280438244343\n",
      "Cross Entropy:[0.75239277 0.23418637 0.57924783 ... 1.1571344  0.14731441 0.27084985]\n",
      "VE Training loss:1.0215564966201782\n",
      "Model saved\n",
      "check (1163,) ()\n",
      "epoch 71\n",
      "====================================\n",
      "Epoch:  71 / 500\n",
      "------------\n",
      "Number of training episodes: 7\n",
      "Total reward:1163.0\n",
      "Mean Reward of that batch 166.14285714285714\n",
      "Average Reward of all training: 101.44770343612534\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1163.0\n",
      "Training loss:-0.009974503889679909\n",
      "Cross Entropy:[0.65252984 0.28706706 0.65878284 ... 0.30281192 0.7243252  0.27799657]\n",
      "VE Training loss:0.931410014629364\n",
      "check (1163,) ()\n",
      "epoch 72\n",
      "====================================\n",
      "Epoch:  72 / 500\n",
      "------------\n",
      "Number of training episodes: 7\n",
      "Total reward:1163.0\n",
      "Mean Reward of that batch 166.14285714285714\n",
      "Average Reward of all training: 102.34624723760773\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1163.0\n",
      "Training loss:-0.011374264024198055\n",
      "Cross Entropy:[0.58692193 0.3264798  0.5777562  ... 0.61378944 0.31894702 0.8027915 ]\n",
      "VE Training loss:0.9966132044792175\n",
      "check (1057,) ()\n",
      "epoch 73\n",
      "====================================\n",
      "Epoch:  73 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1057.0\n",
      "Mean Reward of that batch 176.16666666666666\n",
      "Average Reward of all training: 103.3574858599236\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1057.0\n",
      "Training loss:-0.01855241321027279\n",
      "Cross Entropy:[0.76281786 1.5114805  0.09565617 ... 0.36963284 0.5533317  0.3539351 ]\n",
      "VE Training loss:1.0159690380096436\n",
      "check (1031,) ()\n",
      "epoch 74\n",
      "====================================\n",
      "Epoch:  74 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1031.0\n",
      "Mean Reward of that batch 171.83333333333334\n",
      "Average Reward of all training: 104.28283515010482\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1031.0\n",
      "Training loss:-0.001116590341553092\n",
      "Cross Entropy:[0.71859616 0.2553813  0.7168604  ... 0.34093744 0.8110118  0.22900896]\n",
      "VE Training loss:0.9924230575561523\n",
      "check (1117,) ()\n",
      "epoch 75\n",
      "====================================\n",
      "Epoch:  75 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1117.0\n",
      "Mean Reward of that batch 186.16666666666666\n",
      "Average Reward of all training: 105.37461957032565\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1117.0\n",
      "Training loss:-0.006108785048127174\n",
      "Cross Entropy:[0.75999016 0.24718885 0.72297657 ... 1.0094476  0.17593518 0.49587983]\n",
      "VE Training loss:1.0265250205993652\n",
      "check (1049,) ()\n",
      "epoch 76\n",
      "====================================\n",
      "Epoch:  76 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1049.0\n",
      "Mean Reward of that batch 174.83333333333334\n",
      "Average Reward of all training: 106.28855001457573\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1049.0\n",
      "Training loss:0.018138587474822998\n",
      "Cross Entropy:[0.6700395  1.419821   0.10309744 ... 0.40368947 0.6846514  0.26891991]\n",
      "VE Training loss:0.9518433213233948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1072,) ()\n",
      "epoch 77\n",
      "====================================\n",
      "Epoch:  77 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1072.0\n",
      "Mean Reward of that batch 178.66666666666666\n",
      "Average Reward of all training: 107.228525555512\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1072.0\n",
      "Training loss:-0.0005435610073618591\n",
      "Cross Entropy:[0.7378892 1.5360596 0.087049  ... 0.5872363 0.298734  0.6922266]\n",
      "VE Training loss:0.9872849583625793\n",
      "check (1155,) ()\n",
      "epoch 78\n",
      "====================================\n",
      "Epoch:  78 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1155.0\n",
      "Mean Reward of that batch 192.5\n",
      "Average Reward of all training: 108.32174958685158\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1155.0\n",
      "Training loss:-0.00393707025796175\n",
      "Cross Entropy:[0.75123614 1.4909959  0.09979896 ... 0.05738474 0.1218955  0.2663347 ]\n",
      "VE Training loss:1.0344771146774292\n",
      "check (1142,) ()\n",
      "epoch 79\n",
      "====================================\n",
      "Epoch:  79 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1142.0\n",
      "Mean Reward of that batch 190.33333333333334\n",
      "Average Reward of all training: 109.35987090009819\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1142.0\n",
      "Training loss:-0.0026106140576303005\n",
      "Cross Entropy:[0.58821744 1.273507   2.1463096  ... 0.06010142 0.14236806 0.33083856]\n",
      "VE Training loss:0.9811200499534607\n",
      "check (1160,) ()\n",
      "epoch 80\n",
      "====================================\n",
      "Epoch:  80 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1160.0\n",
      "Mean Reward of that batch 193.33333333333334\n",
      "Average Reward of all training: 110.40953918051363\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1160.0\n",
      "Training loss:-0.016766449436545372\n",
      "Cross Entropy:[0.78333735 0.22864862 0.78487766 ... 0.08232148 1.6045872  0.09408365]\n",
      "VE Training loss:0.981803297996521\n",
      "Model saved\n",
      "check (1104,) ()\n",
      "epoch 81\n",
      "====================================\n",
      "Epoch:  81 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1104.0\n",
      "Mean Reward of that batch 184.0\n",
      "Average Reward of all training: 111.31806338816162\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1104.0\n",
      "Training loss:-0.014151230454444885\n",
      "Cross Entropy:[0.6980743  0.27705643 0.62994885 ... 0.62508225 0.34262878 0.5962568 ]\n",
      "VE Training loss:0.9491322636604309\n",
      "check (1045,) ()\n",
      "epoch 82\n",
      "====================================\n",
      "Epoch:  82 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1045.0\n",
      "Mean Reward of that batch 174.16666666666666\n",
      "Average Reward of all training: 112.0845097696068\n",
      "Max reward for a batch so far: 1169.0\n",
      "check 1045.0\n",
      "Training loss:-0.002409266075119376\n",
      "Cross Entropy:[0.7506597  1.5116905  0.09364422 ... 0.5702968  1.1866454  0.13862516]\n",
      "VE Training loss:1.0333545207977295\n",
      "check (1200,) ()\n",
      "epoch 83\n",
      "====================================\n",
      "Epoch:  83 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 113.14373254346695\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.007261594291776419\n",
      "Cross Entropy:[0.69744664 0.2553824  0.7117693  ... 0.32769758 0.9225038  0.15764764]\n",
      "VE Training loss:1.004406452178955\n",
      "check (1106,) ()\n",
      "epoch 84\n",
      "====================================\n",
      "Epoch:  84 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1106.0\n",
      "Mean Reward of that batch 184.33333333333334\n",
      "Average Reward of all training: 113.99122779096537\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1106.0\n",
      "Training loss:0.004738017451018095\n",
      "Cross Entropy:[0.6028706  1.3644673  0.10433801 ... 0.35112038 0.63375896 0.43721256]\n",
      "VE Training loss:0.995442807674408\n",
      "check (1167,) ()\n",
      "epoch 85\n",
      "====================================\n",
      "Epoch:  85 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1167.0\n",
      "Mean Reward of that batch 194.5\n",
      "Average Reward of all training: 114.93838981695401\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1167.0\n",
      "Training loss:0.007112334482371807\n",
      "Cross Entropy:[0.86293566 0.18905891 0.49989164 ... 0.2600101  0.94195247 0.1638804 ]\n",
      "VE Training loss:1.000612497329712\n",
      "check (1056,) ()\n",
      "epoch 86\n",
      "====================================\n",
      "Epoch:  86 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1056.0\n",
      "Mean Reward of that batch 176.0\n",
      "Average Reward of all training: 115.64840854001268\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1056.0\n",
      "Training loss:0.007985766045749187\n",
      "Cross Entropy:[0.66122055 0.28463942 0.79208124 ... 0.44329464 0.9282285  0.22333275]\n",
      "VE Training loss:0.9811314940452576\n",
      "check (1089,) ()\n",
      "epoch 87\n",
      "====================================\n",
      "Epoch:  87 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1089.0\n",
      "Mean Reward of that batch 181.5\n",
      "Average Reward of all training: 116.40532338438035\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1089.0\n",
      "Training loss:-0.006196309346705675\n",
      "Cross Entropy:[0.57755    1.3639913  2.3542542  ... 0.50858617 0.97188133 1.5923116 ]\n",
      "VE Training loss:0.9314019083976746\n",
      "check (1145,) ()\n",
      "epoch 88\n",
      "====================================\n",
      "Epoch:  88 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1145.0\n",
      "Mean Reward of that batch 190.83333333333334\n",
      "Average Reward of all training: 117.25109622470936\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1145.0\n",
      "Training loss:-0.005273373331874609\n",
      "Cross Entropy:[0.6393515  0.28005987 0.64809096 ... 0.0376127  0.08026335 0.17071635]\n",
      "VE Training loss:0.9366192817687988\n",
      "check (1033,) ()\n",
      "epoch 89\n",
      "====================================\n",
      "Epoch:  89 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1033.0\n",
      "Mean Reward of that batch 172.16666666666666\n",
      "Average Reward of all training: 117.86812510607966\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1033.0\n",
      "Training loss:-0.0007918165065348148\n",
      "Cross Entropy:[0.53750765 1.2489691  0.12133203 ... 0.393354   0.6174331  0.3037719 ]\n",
      "VE Training loss:0.902131199836731\n",
      "check (1105,) ()\n",
      "epoch 90\n",
      "====================================\n",
      "Epoch:  90 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1105.0\n",
      "Mean Reward of that batch 184.16666666666666\n",
      "Average Reward of all training: 118.60477556786395\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1105.0\n",
      "Training loss:-0.0030455368105322123\n",
      "Cross Entropy:[0.8697643  0.18668446 0.5102881  ... 0.0849985  1.5785732  0.07525969]\n",
      "VE Training loss:0.9515712261199951\n",
      "Model saved\n",
      "check (1095,) ()\n",
      "epoch 91\n",
      "====================================\n",
      "Epoch:  91 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1095.0\n",
      "Mean Reward of that batch 182.5\n",
      "Average Reward of all training: 119.30692089129401\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1095.0\n",
      "Training loss:0.011301625519990921\n",
      "Cross Entropy:[0.6023385  0.28977606 0.6247206  ... 0.1082201  0.25846058 0.5706562 ]\n",
      "VE Training loss:0.979354202747345\n",
      "check (1153,) ()\n",
      "epoch 92\n",
      "====================================\n",
      "Epoch:  92 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1153.0\n",
      "Mean Reward of that batch 192.16666666666666\n",
      "Average Reward of all training: 120.09887464972198\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1153.0\n",
      "Training loss:-0.0033479840494692326\n",
      "Cross Entropy:[0.70663166 0.24033633 0.66290176 ... 0.7271981  0.26891336 0.6137907 ]\n",
      "VE Training loss:0.9352675080299377\n",
      "check (1026,) ()\n",
      "epoch 93\n",
      "====================================\n",
      "Epoch:  93 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1026.0\n",
      "Mean Reward of that batch 171.0\n",
      "Average Reward of all training: 120.64619857821958\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1026.0\n",
      "Training loss:-0.014465101063251495\n",
      "Cross Entropy:[0.71819854 0.2273489  0.60320616 ... 0.60662097 0.33276996 0.626349  ]\n",
      "VE Training loss:0.9904389977455139\n",
      "check (1087,) ()\n",
      "epoch 94\n",
      "====================================\n",
      "Epoch:  94 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1087.0\n",
      "Mean Reward of that batch 181.16666666666666\n",
      "Average Reward of all training: 121.29003334511795\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1087.0\n",
      "Training loss:0.008333071134984493\n",
      "Cross Entropy:[0.60661376 0.30060887 0.83364916 ... 0.28586987 0.6010827  0.31263757]\n",
      "VE Training loss:0.9835023283958435\n",
      "check (1145,) ()\n",
      "epoch 95\n",
      "====================================\n",
      "Epoch:  95 / 500\n",
      "------------\n",
      "Number of training episodes: 7\n",
      "Total reward:1145.0\n",
      "Mean Reward of that batch 163.57142857142858\n",
      "Average Reward of all training: 121.73510066328966\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1145.0\n",
      "Training loss:-0.006902989465743303\n",
      "Cross Entropy:[0.9824399  0.15192997 0.41220105 ... 0.83366704 0.20544337 0.5033058 ]\n",
      "VE Training loss:0.9142172932624817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1200,) ()\n",
      "epoch 96\n",
      "====================================\n",
      "Epoch:  96 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 122.55036003138041\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.004714453127235174\n",
      "Cross Entropy:[0.8007611  0.19712652 0.89312476 ... 0.9375659  0.16374955 1.138126  ]\n",
      "VE Training loss:0.9822216033935547\n",
      "check (1008,) ()\n",
      "epoch 97\n",
      "====================================\n",
      "Epoch:  97 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1008.0\n",
      "Mean Reward of that batch 168.0\n",
      "Average Reward of all training: 123.01891302074762\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1008.0\n",
      "Training loss:-0.010838034562766552\n",
      "Cross Entropy:[0.6284267  0.285923   0.7959713  ... 0.7400355  0.24963526 0.6480371 ]\n",
      "VE Training loss:0.9047507047653198\n",
      "check (1150,) ()\n",
      "epoch 98\n",
      "====================================\n",
      "Epoch:  98 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1150.0\n",
      "Mean Reward of that batch 191.66666666666666\n",
      "Average Reward of all training: 123.71940030284883\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1150.0\n",
      "Training loss:0.0014941307017579675\n",
      "Cross Entropy:[0.44543177 0.4129103  0.41654408 ... 2.2599633  0.03758755 0.09450001]\n",
      "VE Training loss:0.9455475211143494\n",
      "check (1028,) ()\n",
      "epoch 99\n",
      "====================================\n",
      "Epoch:  99 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1028.0\n",
      "Mean Reward of that batch 171.33333333333334\n",
      "Average Reward of all training: 124.20034912133858\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1028.0\n",
      "Training loss:-0.022320669144392014\n",
      "Cross Entropy:[0.62546194 0.2687714  0.7179291  ... 0.45112485 0.45413148 0.98001003]\n",
      "VE Training loss:0.9709537029266357\n",
      "check (1066,) ()\n",
      "epoch 100\n",
      "====================================\n",
      "Epoch:  100 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1066.0\n",
      "Mean Reward of that batch 177.66666666666666\n",
      "Average Reward of all training: 124.73501229679185\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1066.0\n",
      "Training loss:-0.015589209273457527\n",
      "Cross Entropy:[0.69054633 0.25840738 0.7448603  ... 0.2733478  0.8625709  0.16492444]\n",
      "VE Training loss:0.9716513156890869\n",
      "Model saved\n",
      "check (1189,) ()\n",
      "epoch 101\n",
      "====================================\n",
      "Epoch:  101 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1189.0\n",
      "Mean Reward of that batch 198.16666666666666\n",
      "Average Reward of all training: 125.46205837966188\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1189.0\n",
      "Training loss:0.001893474254757166\n",
      "Cross Entropy:[0.5882102  0.30559063 0.55526793 ... 0.4430819  0.42569593 0.48655045]\n",
      "VE Training loss:0.9590240120887756\n",
      "check (1108,) ()\n",
      "epoch 102\n",
      "====================================\n",
      "Epoch:  102 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1108.0\n",
      "Mean Reward of that batch 184.66666666666666\n",
      "Average Reward of all training: 126.04249571580898\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1108.0\n",
      "Training loss:-0.004992175381630659\n",
      "Cross Entropy:[0.77764386 0.19968715 0.53947043 ... 1.2973517  0.09922916 0.2097796 ]\n",
      "VE Training loss:0.9918813109397888\n",
      "check (1193,) ()\n",
      "epoch 103\n",
      "====================================\n",
      "Epoch:  103 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1193.0\n",
      "Mean Reward of that batch 198.83333333333334\n",
      "Average Reward of all training: 126.74920287714419\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1193.0\n",
      "Training loss:-0.020268909633159637\n",
      "Cross Entropy:[0.6654161  0.26500425 0.77032256 ... 0.26262626 0.7375349  1.6708256 ]\n",
      "VE Training loss:0.9849216938018799\n",
      "check (1200,) ()\n",
      "epoch 104\n",
      "====================================\n",
      "Epoch:  104 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 127.45353746486397\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0031570387072861195\n",
      "Cross Entropy:[0.7883613  1.6369433  0.07476732 ... 1.3105526  0.09845552 0.26670247]\n",
      "VE Training loss:0.9619885087013245\n",
      "check (1167,) ()\n",
      "epoch 105\n",
      "====================================\n",
      "Epoch:  105 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1167.0\n",
      "Mean Reward of that batch 194.5\n",
      "Average Reward of all training: 128.09207520329383\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1167.0\n",
      "Training loss:-0.002372231800109148\n",
      "Cross Entropy:[0.5754328  1.3568703  0.10076861 ... 0.3584113  0.98789716 0.15409282]\n",
      "VE Training loss:0.9976106882095337\n",
      "check (1136,) ()\n",
      "epoch 106\n",
      "====================================\n",
      "Epoch:  106 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1136.0\n",
      "Mean Reward of that batch 189.33333333333334\n",
      "Average Reward of all training: 128.66982292150175\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1136.0\n",
      "Training loss:-0.00011322028149152175\n",
      "Cross Entropy:[0.8135918  1.7307609  0.06330153 ... 0.17861278 0.54113865 0.3174751 ]\n",
      "VE Training loss:0.991560161113739\n",
      "check (1178,) ()\n",
      "epoch 107\n",
      "====================================\n",
      "Epoch:  107 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1178.0\n",
      "Mean Reward of that batch 196.33333333333334\n",
      "Average Reward of all training: 129.30219217768712\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1178.0\n",
      "Training loss:-0.0006464648758992553\n",
      "Cross Entropy:[0.63828206 0.26837444 0.7654477  ... 1.1384815  0.18118986 0.49192894]\n",
      "VE Training loss:1.002020239830017\n",
      "check (1181,) ()\n",
      "epoch 108\n",
      "====================================\n",
      "Epoch:  108 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1181.0\n",
      "Mean Reward of that batch 196.83333333333334\n",
      "Average Reward of all training: 129.92748052172087\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1181.0\n",
      "Training loss:0.00665969168767333\n",
      "Cross Entropy:[0.7213673  0.22523916 0.6427802  ... 0.25165805 0.86513686 0.16935897]\n",
      "VE Training loss:1.0384224653244019\n",
      "check (1200,) ()\n",
      "epoch 109\n",
      "====================================\n",
      "Epoch:  109 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 130.57034767289775\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0021846136078238487\n",
      "Cross Entropy:[0.6045899  0.28991416 0.83768106 ... 0.6912581  1.4342512  0.09413639]\n",
      "VE Training loss:0.9904378056526184\n",
      "check (1054,) ()\n",
      "epoch 110\n",
      "====================================\n",
      "Epoch:  110 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1054.0\n",
      "Mean Reward of that batch 175.66666666666666\n",
      "Average Reward of all training: 130.98031420920472\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1054.0\n",
      "Training loss:-0.005932950880378485\n",
      "Cross Entropy:[0.63575697 0.25645372 0.7260781  ... 0.2826203  0.59472555 1.136615  ]\n",
      "VE Training loss:1.0414735078811646\n",
      "Model saved\n",
      "check (1194,) ()\n",
      "epoch 111\n",
      "====================================\n",
      "Epoch:  111 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1194.0\n",
      "Mean Reward of that batch 199.0\n",
      "Average Reward of all training: 131.59310417128395\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1194.0\n",
      "Training loss:0.003802793798968196\n",
      "Cross Entropy:[0.58009714 0.30787435 0.5216454  ... 0.38109896 0.48413607 1.1041392 ]\n",
      "VE Training loss:1.0115281343460083\n",
      "check (1120,) ()\n",
      "epoch 112\n",
      "====================================\n",
      "Epoch:  112 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1120.0\n",
      "Mean Reward of that batch 186.66666666666666\n",
      "Average Reward of all training: 132.0848324078499\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1120.0\n",
      "Training loss:-0.013431092724204063\n",
      "Cross Entropy:[0.7242468  0.22638279 0.71858394 ... 0.32043308 0.65537983 0.37201855]\n",
      "VE Training loss:1.0197356939315796\n",
      "check (1168,) ()\n",
      "epoch 113\n",
      "====================================\n",
      "Epoch:  113 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1168.0\n",
      "Mean Reward of that batch 194.66666666666666\n",
      "Average Reward of all training: 132.6386539499633\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1168.0\n",
      "Training loss:-0.0012130035320296884\n",
      "Cross Entropy:[0.62243986 0.25357392 0.7051213  ... 0.12117723 1.4497632  2.724695  ]\n",
      "VE Training loss:0.9998432397842407\n",
      "check (1045,) ()\n",
      "epoch 114\n",
      "====================================\n",
      "Epoch:  114 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1045.0\n",
      "Mean Reward of that batch 174.16666666666666\n",
      "Average Reward of all training: 133.00293476326772\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1045.0\n",
      "Training loss:-0.01350382249802351\n",
      "Cross Entropy:[0.57694733 0.30369166 0.5299914  ... 0.16208924 1.1888163  0.08993755]\n",
      "VE Training loss:1.0084251165390015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1109,) ()\n",
      "epoch 115\n",
      "====================================\n",
      "Epoch:  115 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1109.0\n",
      "Mean Reward of that batch 184.83333333333334\n",
      "Average Reward of all training: 133.45363388126827\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1109.0\n",
      "Training loss:1.7762937204679474e-05\n",
      "Cross Entropy:[0.7106867  0.22408581 0.6464306  ... 0.15821901 1.2752908  0.09046236]\n",
      "VE Training loss:0.9598700404167175\n",
      "check (1160,) ()\n",
      "epoch 116\n",
      "====================================\n",
      "Epoch:  116 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1160.0\n",
      "Mean Reward of that batch 193.33333333333334\n",
      "Average Reward of all training: 133.96983818688955\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1160.0\n",
      "Training loss:-0.001948269084095955\n",
      "Cross Entropy:[0.6735107  0.24549735 0.65495044 ... 0.11383701 1.479311   2.7974143 ]\n",
      "VE Training loss:0.9562765955924988\n",
      "check (1131,) ()\n",
      "epoch 117\n",
      "====================================\n",
      "Epoch:  117 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1131.0\n",
      "Mean Reward of that batch 188.5\n",
      "Average Reward of all training: 134.43590794597594\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1131.0\n",
      "Training loss:0.0010308283381164074\n",
      "Cross Entropy:[0.643933   1.5328108  0.07598088 ... 0.4843189  0.43595606 0.9344994 ]\n",
      "VE Training loss:0.9520359635353088\n",
      "check (1141,) ()\n",
      "epoch 118\n",
      "====================================\n",
      "Epoch:  118 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1141.0\n",
      "Mean Reward of that batch 190.16666666666666\n",
      "Average Reward of all training: 134.90820251140553\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1141.0\n",
      "Training loss:-0.010074838064610958\n",
      "Cross Entropy:[0.77314764 0.19153199 0.5532862  ... 0.86305124 1.8513479  0.0469575 ]\n",
      "VE Training loss:0.9730433225631714\n",
      "check (1200,) ()\n",
      "epoch 119\n",
      "====================================\n",
      "Epoch:  119 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 135.45519240626766\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.002239604713395238\n",
      "Cross Entropy:[0.8078245  0.17995936 0.8911743  ... 2.627287   0.02650235 0.08669216]\n",
      "VE Training loss:0.9658286571502686\n",
      "check (1200,) ()\n",
      "epoch 120\n",
      "====================================\n",
      "Epoch:  120 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 135.99306580288211\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.008313467726111412\n",
      "Cross Entropy:[0.70965594 0.2086108  0.6008607  ... 1.5364497  0.08306382 0.25010538]\n",
      "VE Training loss:0.9702020287513733\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 121\n",
      "====================================\n",
      "Epoch:  121 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 136.52204873013102\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0027124807238578796\n",
      "Cross Entropy:[0.759038   0.19419117 0.56069857 ... 1.5710692  0.07051052 1.6262734 ]\n",
      "VE Training loss:0.996324896812439\n",
      "check (1200,) ()\n",
      "epoch 122\n",
      "====================================\n",
      "Epoch:  122 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 137.04235980611355\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.011119291186332703\n",
      "Cross Entropy:[0.5590382  0.29951182 0.8766593  ... 1.5189601  0.07959522 0.24855967]\n",
      "VE Training loss:1.011699914932251\n",
      "check (1187,) ()\n",
      "epoch 123\n",
      "====================================\n",
      "Epoch:  123 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1187.0\n",
      "Mean Reward of that batch 197.83333333333334\n",
      "Average Reward of all training: 137.53659536324543\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1187.0\n",
      "Training loss:0.0014443754917010665\n",
      "Cross Entropy:[0.66429824 0.23111153 0.72647536 ... 1.0425625  0.16588953 1.211399  ]\n",
      "VE Training loss:0.9557037949562073\n",
      "check (1186,) ()\n",
      "epoch 124\n",
      "====================================\n",
      "Epoch:  124 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1186.0\n",
      "Mean Reward of that batch 197.66666666666666\n",
      "Average Reward of all training: 138.02151529311172\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1186.0\n",
      "Training loss:0.0032971238251775503\n",
      "Cross Entropy:[0.6230657  0.25686184 0.6530892  ... 0.17841165 0.42571282 0.5266738 ]\n",
      "VE Training loss:0.9848337173461914\n",
      "check (1103,) ()\n",
      "epoch 125\n",
      "====================================\n",
      "Epoch:  125 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1103.0\n",
      "Mean Reward of that batch 183.83333333333334\n",
      "Average Reward of all training: 138.3880098374335\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1103.0\n",
      "Training loss:0.005114771891385317\n",
      "Cross Entropy:[0.69786125 0.24169494 0.6606733  ... 0.49191266 0.35798115 0.45427272]\n",
      "VE Training loss:0.9928173422813416\n",
      "check (1200,) ()\n",
      "epoch 126\n",
      "====================================\n",
      "Epoch:  126 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 138.87699388634275\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.01433645561337471\n",
      "Cross Entropy:[0.56327987 0.30475608 0.54043895 ... 0.923923   0.16794805 0.47020277]\n",
      "VE Training loss:0.9940309524536133\n",
      "check (1196,) ()\n",
      "epoch 127\n",
      "====================================\n",
      "Epoch:  127 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1196.0\n",
      "Mean Reward of that batch 199.33333333333334\n",
      "Average Reward of all training: 139.35302805521667\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1196.0\n",
      "Training loss:-0.0014840624062344432\n",
      "Cross Entropy:[0.56647414 0.28211653 0.7594379  ... 0.38938543 0.5740075  1.3156204 ]\n",
      "VE Training loss:0.9630568623542786\n",
      "check (1200,) ()\n",
      "epoch 128\n",
      "====================================\n",
      "Epoch:  128 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 139.8268325235353\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.005011184141039848\n",
      "Cross Entropy:[0.5657041  0.31328142 0.8935225  ... 0.12964436 0.40347296 0.43429238]\n",
      "VE Training loss:0.9900243282318115\n",
      "check (1152,) ()\n",
      "epoch 129\n",
      "====================================\n",
      "Epoch:  129 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1152.0\n",
      "Mean Reward of that batch 192.0\n",
      "Average Reward of all training: 140.23127568226758\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1152.0\n",
      "Training loss:0.0034062406048178673\n",
      "Cross Entropy:[0.6547034  0.23612428 0.639875   ... 0.77488714 0.22502823 0.8194337 ]\n",
      "VE Training loss:0.9537816643714905\n",
      "check (1193,) ()\n",
      "epoch 130\n",
      "====================================\n",
      "Epoch:  130 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1193.0\n",
      "Mean Reward of that batch 198.83333333333334\n",
      "Average Reward of all training: 140.68206074112194\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1193.0\n",
      "Training loss:-0.0005252818809822202\n",
      "Cross Entropy:[0.5592053  0.31568205 0.521518   ... 0.51298255 0.970875   0.2228471 ]\n",
      "VE Training loss:0.9720194935798645\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 131\n",
      "====================================\n",
      "Epoch:  131 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 141.13486943775462\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0019729998894035816\n",
      "Cross Entropy:[0.8415902  0.17818394 0.50335217 ... 0.09849614 0.25462428 0.76209295]\n",
      "VE Training loss:0.9792031645774841\n",
      "check (1187,) ()\n",
      "epoch 132\n",
      "====================================\n",
      "Epoch:  132 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1187.0\n",
      "Mean Reward of that batch 197.83333333333334\n",
      "Average Reward of all training: 141.56440325514535\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1187.0\n",
      "Training loss:-0.007049435283988714\n",
      "Cross Entropy:[0.5878881  0.27275208 0.73716646 ... 1.4334478  0.10004783 1.4132909 ]\n",
      "VE Training loss:0.9533101916313171\n",
      "check (1200,) ()\n",
      "epoch 133\n",
      "====================================\n",
      "Epoch:  133 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 142.0037686442044\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0017344971420243382\n",
      "Cross Entropy:[0.60032165 0.28938475 0.5714404  ... 0.42244315 0.5501992  0.39812514]\n",
      "VE Training loss:0.9748570919036865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1132,) ()\n",
      "epoch 134\n",
      "====================================\n",
      "Epoch:  134 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1132.0\n",
      "Mean Reward of that batch 188.66666666666666\n",
      "Average Reward of all training: 142.3519992264616\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1132.0\n",
      "Training loss:-0.025910843163728714\n",
      "Cross Entropy:[0.82326186 0.19652134 0.5859445  ... 0.05998826 0.15297028 1.1246465 ]\n",
      "VE Training loss:0.9691994786262512\n",
      "check (1174,) ()\n",
      "epoch 135\n",
      "====================================\n",
      "Epoch:  135 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1174.0\n",
      "Mean Reward of that batch 195.66666666666666\n",
      "Average Reward of all training: 142.7469226889816\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1174.0\n",
      "Training loss:0.009621607139706612\n",
      "Cross Entropy:[0.6775753  0.24802558 0.7352384  ... 0.12078979 1.1937082  0.12394488]\n",
      "VE Training loss:0.9659517407417297\n",
      "check (1200,) ()\n",
      "epoch 136\n",
      "====================================\n",
      "Epoch:  136 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 143.16790119862145\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0033062989823520184\n",
      "Cross Entropy:[0.5608385  0.31191638 0.5270637  ... 1.469422   2.5134695  0.02755162]\n",
      "VE Training loss:0.9507846832275391\n",
      "check (1167,) ()\n",
      "epoch 137\n",
      "====================================\n",
      "Epoch:  137 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1167.0\n",
      "Mean Reward of that batch 194.5\n",
      "Average Reward of all training: 143.54258805118627\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1167.0\n",
      "Training loss:-0.0007446696981787682\n",
      "Cross Entropy:[0.877908   1.8081067  0.05832013 ... 0.44945133 0.46683723 0.4089105 ]\n",
      "VE Training loss:0.9548038840293884\n",
      "check (1200,) ()\n",
      "epoch 138\n",
      "====================================\n",
      "Epoch:  138 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 143.95169973197477\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.017128972336649895\n",
      "Cross Entropy:[0.5537737  1.3466835  0.09849732 ... 0.41310355 0.3706409  0.49895316]\n",
      "VE Training loss:0.9624750018119812\n",
      "check (1073,) ()\n",
      "epoch 139\n",
      "====================================\n",
      "Epoch:  139 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1073.0\n",
      "Mean Reward of that batch 178.83333333333334\n",
      "Average Reward of all training: 144.20264673630112\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1073.0\n",
      "Training loss:-0.008907097391784191\n",
      "Cross Entropy:[0.61052483 0.27934772 0.58723694 ... 0.26208827 0.6735258  1.3203416 ]\n",
      "VE Training loss:0.942783772945404\n",
      "check (1182,) ()\n",
      "epoch 140\n",
      "====================================\n",
      "Epoch:  140 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1182.0\n",
      "Mean Reward of that batch 197.0\n",
      "Average Reward of all training: 144.57977068818468\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1182.0\n",
      "Training loss:-0.008107680827379227\n",
      "Cross Entropy:[0.8507386  0.17598785 0.50754887 ... 1.1944528  0.12129285 1.2873563 ]\n",
      "VE Training loss:0.9510896801948547\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 141\n",
      "====================================\n",
      "Epoch:  141 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 144.97282195989968\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0022300623822957277\n",
      "Cross Entropy:[0.872542   0.16564906 0.4701563  ... 0.91140074 0.17459038 1.021748  ]\n",
      "VE Training loss:0.9547638893127441\n",
      "check (1148,) ()\n",
      "epoch 142\n",
      "====================================\n",
      "Epoch:  142 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1148.0\n",
      "Mean Reward of that batch 191.33333333333334\n",
      "Average Reward of all training: 145.29930443436047\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1148.0\n",
      "Training loss:-0.0010673858923837543\n",
      "Cross Entropy:[0.73040736 0.21347275 0.7699299  ... 0.6825355  1.5932133  0.06367867]\n",
      "VE Training loss:0.9492420554161072\n",
      "check (1137,) ()\n",
      "epoch 143\n",
      "====================================\n",
      "Epoch:  143 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1137.0\n",
      "Mean Reward of that batch 189.5\n",
      "Average Reward of all training: 145.60840020754677\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1137.0\n",
      "Training loss:-0.011738775297999382\n",
      "Cross Entropy:[0.73308074 0.2289679  0.69901437 ... 0.17834273 0.98726654 0.14368305]\n",
      "VE Training loss:0.9578738212585449\n",
      "check (1120,) ()\n",
      "epoch 144\n",
      "====================================\n",
      "Epoch:  144 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1120.0\n",
      "Mean Reward of that batch 186.66666666666666\n",
      "Average Reward of all training: 145.8935270579573\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1120.0\n",
      "Training loss:0.005401664413511753\n",
      "Cross Entropy:[0.67249566 1.5585829  0.07349093 ... 0.62880254 0.34495187 0.50325906]\n",
      "VE Training loss:0.9395725727081299\n",
      "check (1186,) ()\n",
      "epoch 145\n",
      "====================================\n",
      "Epoch:  145 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1186.0\n",
      "Mean Reward of that batch 197.66666666666666\n",
      "Average Reward of all training: 146.25058319318978\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1186.0\n",
      "Training loss:-0.0025702673010528088\n",
      "Cross Entropy:[0.9314889  0.16229267 0.93452793 ... 1.6922909  0.05749144 0.11467251]\n",
      "VE Training loss:0.9832068085670471\n",
      "check (1185,) ()\n",
      "epoch 146\n",
      "====================================\n",
      "Epoch:  146 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1185.0\n",
      "Mean Reward of that batch 197.5\n",
      "Average Reward of all training: 146.60160659597616\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1185.0\n",
      "Training loss:-0.0206932220607996\n",
      "Cross Entropy:[0.74091566 0.21361718 0.6283783  ... 0.17283066 1.053705   0.13570182]\n",
      "VE Training loss:0.9389623999595642\n",
      "check (1185,) ()\n",
      "epoch 147\n",
      "====================================\n",
      "Epoch:  147 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1185.0\n",
      "Mean Reward of that batch 197.5\n",
      "Average Reward of all training: 146.9478541701532\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1185.0\n",
      "Training loss:0.004261799156665802\n",
      "Cross Entropy:[0.63882023 0.27157724 0.82510984 ... 0.09516302 1.3479931  0.09281833]\n",
      "VE Training loss:0.9980568885803223\n",
      "check (1171,) ()\n",
      "epoch 148\n",
      "====================================\n",
      "Epoch:  148 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1171.0\n",
      "Mean Reward of that batch 195.16666666666666\n",
      "Average Reward of all training: 147.27365695729176\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1171.0\n",
      "Training loss:-0.0061821043491363525\n",
      "Cross Entropy:[0.92165244 0.15460503 0.45536768 ... 1.4379387  0.09714315 0.29718867]\n",
      "VE Training loss:0.9555952548980713\n",
      "check (1200,) ()\n",
      "epoch 149\n",
      "====================================\n",
      "Epoch:  149 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 147.6275250314039\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.003469482995569706\n",
      "Cross Entropy:[0.8978504  0.17549068 0.8428447  ... 0.68643916 0.25481996 0.7108804 ]\n",
      "VE Training loss:0.9882055521011353\n",
      "check (1200,) ()\n",
      "epoch 150\n",
      "====================================\n",
      "Epoch:  150 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 147.9766748645279\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0017649508081376553\n",
      "Cross Entropy:[0.76017433 1.6938369  0.06274111 ... 0.55659974 0.27810317 0.68152064]\n",
      "VE Training loss:0.9462288618087769\n",
      "Model saved\n",
      "check (1165,) ()\n",
      "epoch 151\n",
      "====================================\n",
      "Epoch:  151 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1165.0\n",
      "Mean Reward of that batch 194.16666666666666\n",
      "Average Reward of all training: 148.28256884997253\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1165.0\n",
      "Training loss:0.006486833561211824\n",
      "Cross Entropy:[0.6136707  0.2777099  0.84718883 ... 1.2172608  0.1276462  0.44374222]\n",
      "VE Training loss:0.9827175736427307\n",
      "check (1180,) ()\n",
      "epoch 152\n",
      "====================================\n",
      "Epoch:  152 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1180.0\n",
      "Mean Reward of that batch 196.66666666666666\n",
      "Average Reward of all training: 148.6008852829771\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1180.0\n",
      "Training loss:0.004970957525074482\n",
      "Cross Entropy:[0.9831171  0.14275704 0.43213308 ... 0.16690949 0.4719192  0.3994239 ]\n",
      "VE Training loss:0.9455204606056213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1174,) ()\n",
      "epoch 153\n",
      "====================================\n",
      "Epoch:  153 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1174.0\n",
      "Mean Reward of that batch 195.66666666666666\n",
      "Average Reward of all training: 148.908504769145\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1174.0\n",
      "Training loss:-0.02022477053105831\n",
      "Cross Entropy:[0.77927756 0.19377455 0.8176161  ... 0.53197896 0.29168996 0.6494969 ]\n",
      "VE Training loss:0.9571539163589478\n",
      "check (1179,) ()\n",
      "epoch 154\n",
      "====================================\n",
      "Epoch:  154 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1179.0\n",
      "Mean Reward of that batch 196.5\n",
      "Average Reward of all training: 149.21754045246226\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1179.0\n",
      "Training loss:-0.006249654106795788\n",
      "Cross Entropy:[0.5663532  0.2997517  0.5212433  ... 0.40311927 0.99748117 1.9214089 ]\n",
      "VE Training loss:0.9487678408622742\n",
      "check (1154,) ()\n",
      "epoch 155\n",
      "====================================\n",
      "Epoch:  155 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1154.0\n",
      "Mean Reward of that batch 192.33333333333334\n",
      "Average Reward of all training: 149.49570685814527\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1154.0\n",
      "Training loss:0.00582392793148756\n",
      "Cross Entropy:[0.6104044  0.27788046 0.5437474  ... 0.14795701 0.44414583 0.3867022 ]\n",
      "VE Training loss:0.991614818572998\n",
      "check (1180,) ()\n",
      "epoch 156\n",
      "====================================\n",
      "Epoch:  156 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1180.0\n",
      "Mean Reward of that batch 196.66666666666666\n",
      "Average Reward of all training: 149.79808480563582\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1180.0\n",
      "Training loss:0.016444174572825432\n",
      "Cross Entropy:[0.6038873  0.25863576 0.7514828  ... 0.12463339 0.4145425  0.39139712]\n",
      "VE Training loss:0.9249834418296814\n",
      "check (1181,) ()\n",
      "epoch 157\n",
      "====================================\n",
      "Epoch:  157 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1181.0\n",
      "Mean Reward of that batch 196.83333333333334\n",
      "Average Reward of all training: 150.0976723758759\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1181.0\n",
      "Training loss:0.006194036453962326\n",
      "Cross Entropy:[0.785896   1.7923725  0.05275238 ... 0.759712   1.9302502  0.03567917]\n",
      "VE Training loss:0.9791291952133179\n",
      "check (1142,) ()\n",
      "epoch 158\n",
      "====================================\n",
      "Epoch:  158 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1142.0\n",
      "Mean Reward of that batch 190.33333333333334\n",
      "Average Reward of all training: 150.35232845788514\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1142.0\n",
      "Training loss:0.0005295848823152483\n",
      "Cross Entropy:[1.0451149  0.12516376 0.3773491  ... 0.16571803 1.2948375  0.09898505]\n",
      "VE Training loss:0.971088171005249\n",
      "check (1200,) ()\n",
      "epoch 159\n",
      "====================================\n",
      "Epoch:  159 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 150.6645779644393\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.00476854620501399\n",
      "Cross Entropy:[0.6406948  0.2592504  0.7984139  ... 0.20676896 0.8670012  0.19418833]\n",
      "VE Training loss:0.9988753199577332\n",
      "check (1154,) ()\n",
      "epoch 160\n",
      "====================================\n",
      "Epoch:  160 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1154.0\n",
      "Mean Reward of that batch 192.33333333333334\n",
      "Average Reward of all training: 150.9250076854949\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1154.0\n",
      "Training loss:0.00459983479231596\n",
      "Cross Entropy:[0.8406495  0.16569926 0.49445218 ... 0.09335858 0.2719291  0.7447198 ]\n",
      "VE Training loss:0.955204963684082\n",
      "Model saved\n",
      "check (1181,) ()\n",
      "epoch 161\n",
      "====================================\n",
      "Epoch:  161 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1181.0\n",
      "Mean Reward of that batch 196.83333333333334\n",
      "Average Reward of all training: 151.21015256529515\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1181.0\n",
      "Training loss:0.0011844602413475513\n",
      "Cross Entropy:[0.80931526 0.196265   0.64280754 ... 1.411953   0.08986814 0.2768634 ]\n",
      "VE Training loss:0.92597895860672\n",
      "check (1167,) ()\n",
      "epoch 162\n",
      "====================================\n",
      "Epoch:  162 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1167.0\n",
      "Mean Reward of that batch 194.5\n",
      "Average Reward of all training: 151.4773738457563\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1167.0\n",
      "Training loss:0.00972905382514\n",
      "Cross Entropy:[0.9913652  0.132765   0.40731505 ... 0.49496508 0.4131821  0.3890171 ]\n",
      "VE Training loss:0.9432946443557739\n",
      "check (1184,) ()\n",
      "epoch 163\n",
      "====================================\n",
      "Epoch:  163 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1184.0\n",
      "Mean Reward of that batch 197.33333333333334\n",
      "Average Reward of all training: 151.75869875058805\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1184.0\n",
      "Training loss:-0.006128976587206125\n",
      "Cross Entropy:[0.82038236 0.18601057 0.6018005  ... 0.0940638  0.27649936 0.76945746]\n",
      "VE Training loss:0.9418383240699768\n",
      "check (1139,) ()\n",
      "epoch 164\n",
      "====================================\n",
      "Epoch:  164 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1139.0\n",
      "Mean Reward of that batch 189.83333333333334\n",
      "Average Reward of all training: 151.99086115658042\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1139.0\n",
      "Training loss:-0.0012464305618777871\n",
      "Cross Entropy:[0.8557162  0.16802847 0.5260831  ... 0.3755684  0.5142738  0.35285234]\n",
      "VE Training loss:0.9485718607902527\n",
      "check (1146,) ()\n",
      "epoch 165\n",
      "====================================\n",
      "Epoch:  165 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1146.0\n",
      "Mean Reward of that batch 191.0\n",
      "Average Reward of all training: 152.22728017987387\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1146.0\n",
      "Training loss:0.009682479314506054\n",
      "Cross Entropy:[0.851019   0.17552948 0.5815383  ... 0.31573996 0.5105951  0.3908559 ]\n",
      "VE Training loss:0.944756031036377\n",
      "check (1200,) ()\n",
      "epoch 166\n",
      "====================================\n",
      "Epoch:  166 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 152.51506764866983\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.011986525729298592\n",
      "Cross Entropy:[0.4785658  0.32850668 0.9615692  ... 1.7647374  0.06026732 0.18849769]\n",
      "VE Training loss:0.945388674736023\n",
      "check (1095,) ()\n",
      "epoch 167\n",
      "====================================\n",
      "Epoch:  167 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1095.0\n",
      "Mean Reward of that batch 182.5\n",
      "Average Reward of all training: 152.69461814179155\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1095.0\n",
      "Training loss:1.5938118735903117e-07\n",
      "Cross Entropy:[0.72431904 0.19779405 0.7899102  ... 0.9865118  0.18114331 0.814107  ]\n",
      "VE Training loss:0.9698185324668884\n",
      "check (1136,) ()\n",
      "epoch 168\n",
      "====================================\n",
      "Epoch:  168 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1136.0\n",
      "Mean Reward of that batch 189.33333333333334\n",
      "Average Reward of all training: 152.91270573221738\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1136.0\n",
      "Training loss:0.006008793134242296\n",
      "Cross Entropy:[0.60601157 0.26079768 0.5854716  ... 0.18275125 0.5057938  0.36279362]\n",
      "VE Training loss:0.9841830730438232\n",
      "check (1163,) ()\n",
      "epoch 169\n",
      "====================================\n",
      "Epoch:  169 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1163.0\n",
      "Mean Reward of that batch 193.83333333333334\n",
      "Average Reward of all training: 153.15483962334824\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1163.0\n",
      "Training loss:0.005554540548473597\n",
      "Cross Entropy:[0.73737866 0.19178618 0.58403546 ... 0.563582   0.27198693 0.7907102 ]\n",
      "VE Training loss:0.9520485997200012\n",
      "check (1077,) ()\n",
      "epoch 170\n",
      "====================================\n",
      "Epoch:  170 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1077.0\n",
      "Mean Reward of that batch 179.5\n",
      "Average Reward of all training: 153.3098111549756\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1077.0\n",
      "Training loss:0.012514257803559303\n",
      "Cross Entropy:[0.68317264 0.21819013 0.6748696  ... 0.16120501 0.41398224 0.5049531 ]\n",
      "VE Training loss:0.9549795389175415\n",
      "Model saved\n",
      "check (1180,) ()\n",
      "epoch 171\n",
      "====================================\n",
      "Epoch:  171 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1180.0\n",
      "Mean Reward of that batch 196.66666666666666\n",
      "Average Reward of all training: 153.56336001761707\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1180.0\n",
      "Training loss:-0.011150502599775791\n",
      "Cross Entropy:[0.7990837  0.18143259 0.81647384 ... 0.40668032 1.0446858  0.1374123 ]\n",
      "VE Training loss:0.9457611441612244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1076,) ()\n",
      "epoch 172\n",
      "====================================\n",
      "Epoch:  172 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1076.0\n",
      "Mean Reward of that batch 179.33333333333334\n",
      "Average Reward of all training: 153.71318544387123\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1076.0\n",
      "Training loss:-0.003270538290962577\n",
      "Cross Entropy:[0.5203339  0.29378664 0.86797976 ... 0.14518872 1.0525761  0.14655045]\n",
      "VE Training loss:0.9186097979545593\n",
      "check (1062,) ()\n",
      "epoch 173\n",
      "====================================\n",
      "Epoch:  173 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1062.0\n",
      "Mean Reward of that batch 177.0\n",
      "Average Reward of all training: 153.84779130835753\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1062.0\n",
      "Training loss:0.012011063285171986\n",
      "Cross Entropy:[0.7459697  0.20223421 0.6503798  ... 0.41314057 0.89540243 0.2167224 ]\n",
      "VE Training loss:0.940028727054596\n",
      "check (1121,) ()\n",
      "epoch 174\n",
      "====================================\n",
      "Epoch:  174 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1121.0\n",
      "Mean Reward of that batch 186.83333333333334\n",
      "Average Reward of all training: 154.0373633889608\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1121.0\n",
      "Training loss:-0.0037003320176154375\n",
      "Cross Entropy:[0.5184302  0.29600406 0.8716945  ... 0.6227835  0.27625352 0.6928582 ]\n",
      "VE Training loss:0.9322399497032166\n",
      "check (1098,) ()\n",
      "epoch 175\n",
      "====================================\n",
      "Epoch:  175 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1098.0\n",
      "Mean Reward of that batch 183.0\n",
      "Average Reward of all training: 154.20286416959533\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1098.0\n",
      "Training loss:-0.006824959069490433\n",
      "Cross Entropy:[0.7175355  1.6972251  0.05824726 ... 0.21176255 0.62474555 0.26711205]\n",
      "VE Training loss:0.9438345432281494\n",
      "check (1126,) ()\n",
      "epoch 176\n",
      "====================================\n",
      "Epoch:  176 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1126.0\n",
      "Mean Reward of that batch 187.66666666666666\n",
      "Average Reward of all training: 154.39299941105597\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1126.0\n",
      "Training loss:-0.004570750053972006\n",
      "Cross Entropy:[0.6226576  0.25650916 0.5823429  ... 0.43065485 0.47580028 0.27914748]\n",
      "VE Training loss:0.9412763714790344\n",
      "check (1158,) ()\n",
      "epoch 177\n",
      "====================================\n",
      "Epoch:  177 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1158.0\n",
      "Mean Reward of that batch 193.0\n",
      "Average Reward of all training: 154.61111805845113\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1158.0\n",
      "Training loss:-0.005097608547657728\n",
      "Cross Entropy:[0.5771324  0.27995887 0.5402467  ... 0.04397144 0.12803003 0.377622  ]\n",
      "VE Training loss:0.9454651474952698\n",
      "check (1063,) ()\n",
      "epoch 178\n",
      "====================================\n",
      "Epoch:  178 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1063.0\n",
      "Mean Reward of that batch 177.16666666666666\n",
      "Average Reward of all training: 154.73783462366583\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1063.0\n",
      "Training loss:-0.013658912852406502\n",
      "Cross Entropy:[0.8212112  0.17222865 0.8777709  ... 0.6539139  1.4071256  0.09414973]\n",
      "VE Training loss:0.9478708505630493\n",
      "check (1015,) ()\n",
      "epoch 179\n",
      "====================================\n",
      "Epoch:  179 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1015.0\n",
      "Mean Reward of that batch 169.16666666666666\n",
      "Average Reward of all training: 154.81844262390607\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1015.0\n",
      "Training loss:0.001540960161946714\n",
      "Cross Entropy:[0.5649587 0.2620469 0.610502  ... 0.161692  1.0526451 2.098102 ]\n",
      "VE Training loss:0.910698413848877\n",
      "check (1133,) ()\n",
      "epoch 180\n",
      "====================================\n",
      "Epoch:  180 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1133.0\n",
      "Mean Reward of that batch 188.83333333333334\n",
      "Average Reward of all training: 155.00741423895843\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1133.0\n",
      "Training loss:-0.011455690488219261\n",
      "Cross Entropy:[0.6733969  0.21027268 0.62993544 ... 0.05739195 0.14016725 0.33188674]\n",
      "VE Training loss:0.8861184120178223\n",
      "Model saved\n",
      "check (1120,) ()\n",
      "epoch 181\n",
      "====================================\n",
      "Epoch:  181 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1120.0\n",
      "Mean Reward of that batch 186.66666666666666\n",
      "Average Reward of all training: 155.1823272357966\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1120.0\n",
      "Training loss:0.0013766748597845435\n",
      "Cross Entropy:[0.7313811  0.1895419  0.822999   ... 0.5068484  0.29863787 0.59705216]\n",
      "VE Training loss:0.9479616284370422\n",
      "check (1137,) ()\n",
      "epoch 182\n",
      "====================================\n",
      "Epoch:  182 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1137.0\n",
      "Mean Reward of that batch 189.5\n",
      "Average Reward of all training: 155.37088587735818\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1137.0\n",
      "Training loss:0.0026668214704841375\n",
      "Cross Entropy:[0.6850111  0.22076693 0.687779   ... 1.0543569  0.12467991 0.35794905]\n",
      "VE Training loss:0.9410431385040283\n",
      "check (1119,) ()\n",
      "epoch 183\n",
      "====================================\n",
      "Epoch:  183 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1119.0\n",
      "Mean Reward of that batch 186.5\n",
      "Average Reward of all training: 155.54099032611578\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1119.0\n",
      "Training loss:0.0038278321735560894\n",
      "Cross Entropy:[0.5944931  0.26492396 0.5685115  ... 0.71774226 0.2557723  0.55480975]\n",
      "VE Training loss:0.9356322884559631\n",
      "check (1183,) ()\n",
      "epoch 184\n",
      "====================================\n",
      "Epoch:  184 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1183.0\n",
      "Mean Reward of that batch 197.16666666666666\n",
      "Average Reward of all training: 155.7672168279666\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1183.0\n",
      "Training loss:-0.0014937828527763486\n",
      "Cross Entropy:[0.68800586 1.5840975  0.07235675 ... 0.09589849 0.2480826  0.5860516 ]\n",
      "VE Training loss:0.964886486530304\n",
      "check (1153,) ()\n",
      "epoch 185\n",
      "====================================\n",
      "Epoch:  185 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1153.0\n",
      "Mean Reward of that batch 192.16666666666666\n",
      "Average Reward of all training: 155.96397061087848\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1153.0\n",
      "Training loss:0.0014959948603063822\n",
      "Cross Entropy:[0.6855359  0.21565643 0.7079641  ... 0.1705874  0.5378108  0.31209046]\n",
      "VE Training loss:0.9980007410049438\n",
      "check (1169,) ()\n",
      "epoch 186\n",
      "====================================\n",
      "Epoch:  186 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1169.0\n",
      "Mean Reward of that batch 194.83333333333334\n",
      "Average Reward of all training: 156.17294567927877\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1169.0\n",
      "Training loss:-0.02508806250989437\n",
      "Cross Entropy:[0.7063607  0.2252657  0.7550547  ... 0.50017464 1.199048   2.1693118 ]\n",
      "VE Training loss:0.9546878933906555\n",
      "check (1072,) ()\n",
      "epoch 187\n",
      "====================================\n",
      "Epoch:  187 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1072.0\n",
      "Mean Reward of that batch 178.66666666666666\n",
      "Average Reward of all training: 156.2932329572862\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1072.0\n",
      "Training loss:-0.0044230506755411625\n",
      "Cross Entropy:[0.69652396 0.2000331  0.78289986 ... 0.2866254  0.84817827 1.8813398 ]\n",
      "VE Training loss:0.9402586817741394\n",
      "check (1200,) ()\n",
      "epoch 188\n",
      "====================================\n",
      "Epoch:  188 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 156.52571576070488\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.012385734356939793\n",
      "Cross Entropy:[0.7524383  0.19754088 0.6485825  ... 0.35293266 0.4663672  0.37928468]\n",
      "VE Training loss:0.9603920578956604\n",
      "check (1107,) ()\n",
      "epoch 189\n",
      "====================================\n",
      "Epoch:  189 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1107.0\n",
      "Mean Reward of that batch 184.5\n",
      "Average Reward of all training: 156.6737278466271\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1107.0\n",
      "Training loss:-0.024025917053222656\n",
      "Cross Entropy:[0.6001334  1.5599822  0.06460205 ... 0.7446507  0.20806645 0.77638686]\n",
      "VE Training loss:0.9344662427902222\n",
      "check (1063,) ()\n",
      "epoch 190\n",
      "====================================\n",
      "Epoch:  190 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1063.0\n",
      "Mean Reward of that batch 177.16666666666666\n",
      "Average Reward of all training: 156.78158541936415\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1063.0\n",
      "Training loss:-0.004724789876490831\n",
      "Cross Entropy:[0.7341957  0.19123615 0.79298735 ... 0.36241138 0.57523924 0.23286729]\n",
      "VE Training loss:1.004843831062317\n",
      "Model saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1144,) ()\n",
      "epoch 191\n",
      "====================================\n",
      "Epoch:  191 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1144.0\n",
      "Mean Reward of that batch 190.66666666666666\n",
      "Average Reward of all training: 156.95899422170604\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1144.0\n",
      "Training loss:-0.0028214554768055677\n",
      "Cross Entropy:[0.61974615 0.24710989 0.59379023 ... 1.0579131  0.12846322 1.2210128 ]\n",
      "VE Training loss:0.9285492897033691\n",
      "check (1181,) ()\n",
      "epoch 192\n",
      "====================================\n",
      "Epoch:  192 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1181.0\n",
      "Mean Reward of that batch 196.83333333333334\n",
      "Average Reward of all training: 157.16667307124575\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1181.0\n",
      "Training loss:0.0007132324972189963\n",
      "Cross Entropy:[0.5885524  0.25078857 0.6090099  ... 0.2614068  0.63253623 0.2718841 ]\n",
      "VE Training loss:0.981525719165802\n",
      "check (1168,) ()\n",
      "epoch 193\n",
      "====================================\n",
      "Epoch:  193 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1168.0\n",
      "Mean Reward of that batch 194.66666666666666\n",
      "Average Reward of all training: 157.36097355619611\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1168.0\n",
      "Training loss:0.0010169744491577148\n",
      "Cross Entropy:[0.64821273 0.23665883 0.7810315  ... 0.01056565 0.02319536 0.05246047]\n",
      "VE Training loss:0.9618682861328125\n",
      "check (1178,) ()\n",
      "epoch 194\n",
      "====================================\n",
      "Epoch:  194 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1178.0\n",
      "Mean Reward of that batch 196.33333333333334\n",
      "Average Reward of all training: 157.5618620086556\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1178.0\n",
      "Training loss:-0.002773631364107132\n",
      "Cross Entropy:[0.5972605  1.519497   0.07159954 ... 0.09746092 1.6472473  0.05951561]\n",
      "VE Training loss:0.9778414368629456\n",
      "check (1095,) ()\n",
      "epoch 195\n",
      "====================================\n",
      "Epoch:  195 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1095.0\n",
      "Mean Reward of that batch 182.5\n",
      "Average Reward of all training: 157.68974989579067\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1095.0\n",
      "Training loss:0.006665833294391632\n",
      "Cross Entropy:[0.5748482  0.24879237 0.75122106 ... 0.24138232 0.6845291  1.5759015 ]\n",
      "VE Training loss:0.9755637645721436\n",
      "check (1165,) ()\n",
      "epoch 196\n",
      "====================================\n",
      "Epoch:  196 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1165.0\n",
      "Mean Reward of that batch 194.16666666666666\n",
      "Average Reward of all training: 157.87585661400945\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1165.0\n",
      "Training loss:0.002391222631558776\n",
      "Cross Entropy:[0.8603753  0.15638557 0.9304546  ... 0.5342437  0.31082156 0.54510814]\n",
      "VE Training loss:0.9677411913871765\n",
      "check (1200,) ()\n",
      "epoch 197\n",
      "====================================\n",
      "Epoch:  197 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 158.0896847530246\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0005429188604466617\n",
      "Cross Entropy:[0.7994434  0.16764435 0.5132125  ... 0.51832205 1.0656165  1.8419857 ]\n",
      "VE Training loss:0.9884445071220398\n",
      "check (1200,) ()\n",
      "epoch 198\n",
      "====================================\n",
      "Epoch:  198 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 158.30135301184774\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.006189870648086071\n",
      "Cross Entropy:[0.6655221  0.23840225 0.6081026  ... 0.9491034  0.15034899 0.9868534 ]\n",
      "VE Training loss:0.9861453175544739\n",
      "check (1200,) ()\n",
      "epoch 199\n",
      "====================================\n",
      "Epoch:  199 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 158.51089395148668\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.004416137468069792\n",
      "Cross Entropy:[0.6877392  0.22027193 0.68182534 ... 0.1351593  0.35955307 0.5767154 ]\n",
      "VE Training loss:0.9807917475700378\n",
      "check (1200,) ()\n",
      "epoch 200\n",
      "====================================\n",
      "Epoch:  200 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 158.71833948172926\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.010090087540447712\n",
      "Cross Entropy:[0.53102446 1.3372927  0.09473263 ... 0.4831258  1.0695626  0.16213304]\n",
      "VE Training loss:0.9929684400558472\n",
      "Model saved\n",
      "check (1156,) ()\n",
      "epoch 201\n",
      "====================================\n",
      "Epoch:  201 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1156.0\n",
      "Mean Reward of that batch 192.66666666666666\n",
      "Average Reward of all training: 158.88723663190308\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1156.0\n",
      "Training loss:-0.0024348022416234016\n",
      "Cross Entropy:[0.6262835  0.23529881 0.6912017  ... 2.0920262  0.0500416  0.11436751]\n",
      "VE Training loss:0.964293360710144\n",
      "check (1200,) ()\n",
      "epoch 202\n",
      "====================================\n",
      "Epoch:  202 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 159.0907651634283\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.005304017104208469\n",
      "Cross Entropy:[0.84306747 1.8457679  0.05234972 ... 0.59144634 0.31804624 0.7880324 ]\n",
      "VE Training loss:1.0167245864868164\n",
      "check (1200,) ()\n",
      "epoch 203\n",
      "====================================\n",
      "Epoch:  203 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 159.2922884877464\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0013176961801946163\n",
      "Cross Entropy:[0.5891738  0.28282994 0.5333656  ... 0.33579615 0.4205212  0.5157058 ]\n",
      "VE Training loss:0.9973219037055969\n",
      "check (1124,) ()\n",
      "epoch 204\n",
      "====================================\n",
      "Epoch:  204 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1124.0\n",
      "Mean Reward of that batch 187.33333333333334\n",
      "Average Reward of all training: 159.42974458993064\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1124.0\n",
      "Training loss:0.0003578858159016818\n",
      "Cross Entropy:[0.6451065  0.2431336  0.75221    ... 0.95253164 0.13022734 0.37468848]\n",
      "VE Training loss:0.9814150333404541\n",
      "check (1200,) ()\n",
      "epoch 205\n",
      "====================================\n",
      "Epoch:  205 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 159.62764827485782\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.005802782252430916\n",
      "Cross Entropy:[0.80693763 0.18623304 0.59177023 ... 0.06930911 0.22987515 0.70424277]\n",
      "VE Training loss:0.999325156211853\n",
      "check (1200,) ()\n",
      "epoch 206\n",
      "====================================\n",
      "Epoch:  206 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 159.8236305647857\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0027711004950106144\n",
      "Cross Entropy:[0.61934525 0.24986678 0.7468341  ... 1.4571782  0.08113847 0.26609138]\n",
      "VE Training loss:0.9971995949745178\n",
      "check (1160,) ()\n",
      "epoch 207\n",
      "====================================\n",
      "Epoch:  207 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1160.0\n",
      "Mean Reward of that batch 193.33333333333334\n",
      "Average Reward of all training: 159.98551318685597\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1160.0\n",
      "Training loss:-0.0035302401520311832\n",
      "Cross Entropy:[0.9431333  0.14174536 0.4162526  ... 0.30898657 0.82784784 0.20394026]\n",
      "VE Training loss:0.9972538352012634\n",
      "check (1189,) ()\n",
      "epoch 208\n",
      "====================================\n",
      "Epoch:  208 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1189.0\n",
      "Mean Reward of that batch 198.16666666666666\n",
      "Average Reward of all training: 160.16907642473967\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1189.0\n",
      "Training loss:0.0030818278901278973\n",
      "Cross Entropy:[0.6132959  0.2640725  0.59841764 ... 1.823301   0.06476184 0.20599107]\n",
      "VE Training loss:1.0110089778900146\n",
      "check (1181,) ()\n",
      "epoch 209\n",
      "====================================\n",
      "Epoch:  209 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1181.0\n",
      "Mean Reward of that batch 196.83333333333334\n",
      "Average Reward of all training: 160.34450349128795\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1181.0\n",
      "Training loss:0.010822426527738571\n",
      "Cross Entropy:[0.77739656 0.1987135  0.61679643 ... 1.0785261  0.09884112 0.22785209]\n",
      "VE Training loss:1.016862392425537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1184,) ()\n",
      "epoch 210\n",
      "====================================\n",
      "Epoch:  210 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1184.0\n",
      "Mean Reward of that batch 197.33333333333334\n",
      "Average Reward of all training: 160.52064077625008\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1184.0\n",
      "Training loss:-0.004868335090577602\n",
      "Cross Entropy:[0.75998676 1.6496638  0.06916384 ... 1.1146573  0.09892675 1.6232429 ]\n",
      "VE Training loss:0.9711143970489502\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 211\n",
      "====================================\n",
      "Epoch:  211 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 160.70774674413516\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.00675421254709363\n",
      "Cross Entropy:[0.8561737  0.1691455  0.919852   ... 0.41483006 0.38186666 0.43766946]\n",
      "VE Training loss:1.0215065479278564\n",
      "check (1200,) ()\n",
      "epoch 212\n",
      "====================================\n",
      "Epoch:  212 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 160.89308756137982\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.006804824806749821\n",
      "Cross Entropy:[0.5021051  0.31845668 0.8856565  ... 0.50830406 0.32892638 0.52751315]\n",
      "VE Training loss:1.0002299547195435\n",
      "check (1097,) ()\n",
      "epoch 213\n",
      "====================================\n",
      "Epoch:  213 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1097.0\n",
      "Mean Reward of that batch 182.83333333333334\n",
      "Average Reward of all training: 160.9960934100744\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1097.0\n",
      "Training loss:0.002985082333907485\n",
      "Cross Entropy:[0.7559368  0.1987349  0.8216802  ... 0.2907545  0.811512   0.20024563]\n",
      "VE Training loss:0.9956973791122437\n",
      "check (1171,) ()\n",
      "epoch 214\n",
      "====================================\n",
      "Epoch:  214 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1171.0\n",
      "Mean Reward of that batch 195.16666666666666\n",
      "Average Reward of all training: 161.15576898603982\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1171.0\n",
      "Training loss:0.007437844295054674\n",
      "Cross Entropy:[1.0046294  0.13387758 0.38729504 ... 0.51071966 0.30741712 0.6586492 ]\n",
      "VE Training loss:1.0032944679260254\n",
      "check (1200,) ()\n",
      "epoch 215\n",
      "====================================\n",
      "Epoch:  215 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 161.3364398279652\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0064313714392483234\n",
      "Cross Entropy:[0.92416596 0.17109798 0.85795665 ... 1.4551585  0.0856802  0.2696595 ]\n",
      "VE Training loss:1.0115253925323486\n",
      "check (1115,) ()\n",
      "epoch 216\n",
      "====================================\n",
      "Epoch:  216 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1115.0\n",
      "Mean Reward of that batch 185.83333333333334\n",
      "Average Reward of all training: 161.44985137197156\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1115.0\n",
      "Training loss:0.002458424074575305\n",
      "Cross Entropy:[0.7651614  0.19339019 0.8734044  ... 0.09790137 0.26636404 0.7329754 ]\n",
      "VE Training loss:0.9702674150466919\n",
      "check (1200,) ()\n",
      "epoch 217\n",
      "====================================\n",
      "Epoch:  217 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 161.62750182647858\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.018830830231308937\n",
      "Cross Entropy:[0.92270106 0.15671849 0.9984955  ... 0.09448764 0.27359265 0.73847103]\n",
      "VE Training loss:0.9918124675750732\n",
      "check (1200,) ()\n",
      "epoch 218\n",
      "====================================\n",
      "Epoch:  218 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 161.80352246030208\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.007177668623626232\n",
      "Cross Entropy:[0.6451286  0.2581178  0.61956394 ... 0.04949139 0.14672288 0.42512807]\n",
      "VE Training loss:0.9840447902679443\n",
      "check (1074,) ()\n",
      "epoch 219\n",
      "====================================\n",
      "Epoch:  219 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1074.0\n",
      "Mean Reward of that batch 179.0\n",
      "Average Reward of all training: 161.88204518879385\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1074.0\n",
      "Training loss:0.00888805277645588\n",
      "Cross Entropy:[0.7336143  0.20252047 0.56682837 ... 0.26412743 0.8599012  0.13833848]\n",
      "VE Training loss:0.9384963512420654\n",
      "check (1200,) ()\n",
      "epoch 220\n",
      "====================================\n",
      "Epoch:  220 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 162.05530861975387\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.000382417842047289\n",
      "Cross Entropy:[0.9156593  0.15426467 0.45193747 ... 0.8112196  1.8136113  0.05055728]\n",
      "VE Training loss:0.9793060421943665\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 221\n",
      "====================================\n",
      "Epoch:  221 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 162.2270040558636\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.011684004217386246\n",
      "Cross Entropy:[0.69148266 0.24557577 0.77159595 ... 0.09325194 0.27314967 0.62698716]\n",
      "VE Training loss:0.9557350873947144\n",
      "check (1127,) ()\n",
      "epoch 222\n",
      "====================================\n",
      "Epoch:  222 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1127.0\n",
      "Mean Reward of that batch 187.83333333333334\n",
      "Average Reward of all training: 162.34234788143777\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1127.0\n",
      "Training loss:0.0005233302363194525\n",
      "Cross Entropy:[0.5069888  0.3087456  0.55436647 ... 0.3725466  0.41741025 1.0039644 ]\n",
      "VE Training loss:0.9792080521583557\n",
      "check (1178,) ()\n",
      "epoch 223\n",
      "====================================\n",
      "Epoch:  223 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1178.0\n",
      "Mean Reward of that batch 196.33333333333334\n",
      "Average Reward of all training: 162.49477382516824\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1178.0\n",
      "Training loss:-0.0013178904773667455\n",
      "Cross Entropy:[0.47377858 1.2123071  0.11488667 ... 0.13490622 0.39939454 0.44299394]\n",
      "VE Training loss:0.966928243637085\n",
      "check (1179,) ()\n",
      "epoch 224\n",
      "====================================\n",
      "Epoch:  224 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1179.0\n",
      "Mean Reward of that batch 196.5\n",
      "Average Reward of all training: 162.6465828705916\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1179.0\n",
      "Training loss:-0.0007735754479654133\n",
      "Cross Entropy:[0.58561987 0.2502397  0.6826683  ... 0.72835076 0.22679736 0.7414457 ]\n",
      "VE Training loss:0.9266115427017212\n",
      "check (1116,) ()\n",
      "epoch 225\n",
      "====================================\n",
      "Epoch:  225 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1116.0\n",
      "Mean Reward of that batch 186.0\n",
      "Average Reward of all training: 162.75037583561118\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1116.0\n",
      "Training loss:0.020947061479091644\n",
      "Cross Entropy:[0.6657946  0.25080135 0.58711237 ... 0.2077991  0.7149711  0.2278535 ]\n",
      "VE Training loss:0.938814640045166\n",
      "check (1173,) ()\n",
      "epoch 226\n",
      "====================================\n",
      "Epoch:  226 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1173.0\n",
      "Mean Reward of that batch 195.5\n",
      "Average Reward of all training: 162.89528567704653\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1173.0\n",
      "Training loss:-0.010306784883141518\n",
      "Cross Entropy:[0.47430003 1.2295952  0.11016624 ... 0.01216822 0.02526661 0.05405448]\n",
      "VE Training loss:0.95549076795578\n",
      "check (1200,) ()\n",
      "epoch 227\n",
      "====================================\n",
      "Epoch:  227 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 163.0587425683371\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.003298493567854166\n",
      "Cross Entropy:[0.9490812  0.15349464 0.9296988  ... 0.49520645 0.29635468 0.75180936]\n",
      "VE Training loss:0.9646478295326233\n",
      "check (1127,) ()\n",
      "epoch 228\n",
      "====================================\n",
      "Epoch:  228 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1127.0\n",
      "Mean Reward of that batch 187.83333333333334\n",
      "Average Reward of all training: 163.16740305414848\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1127.0\n",
      "Training loss:0.0008457127842120826\n",
      "Cross Entropy:[0.3950473  0.4307941  1.2076058  ... 0.14757405 0.44324696 0.40379375]\n",
      "VE Training loss:0.9446429014205933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1180,) ()\n",
      "epoch 229\n",
      "====================================\n",
      "Epoch:  229 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1180.0\n",
      "Mean Reward of that batch 196.66666666666666\n",
      "Average Reward of all training: 163.3136880480896\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1180.0\n",
      "Training loss:0.012485774233937263\n",
      "Cross Entropy:[0.6910589  0.21544002 0.7490305  ... 0.3742536  0.41583183 0.56020075]\n",
      "VE Training loss:0.9530219435691833\n",
      "check (1164,) ()\n",
      "epoch 230\n",
      "====================================\n",
      "Epoch:  230 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1164.0\n",
      "Mean Reward of that batch 194.0\n",
      "Average Reward of all training: 163.44710679570662\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1164.0\n",
      "Training loss:-0.008193887770175934\n",
      "Cross Entropy:[0.89338183 0.15326467 0.45949057 ... 0.18489926 0.62229747 0.2353302 ]\n",
      "VE Training loss:0.9432901740074158\n",
      "Model saved\n",
      "check (1175,) ()\n",
      "epoch 231\n",
      "====================================\n",
      "Epoch:  231 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1175.0\n",
      "Mean Reward of that batch 195.83333333333334\n",
      "Average Reward of all training: 163.58730691058813\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1175.0\n",
      "Training loss:-0.0022914723958820105\n",
      "Cross Entropy:[0.6287858  0.24060617 0.702082   ... 0.66333926 0.2595655  0.689753  ]\n",
      "VE Training loss:0.950764000415802\n",
      "check (1200,) ()\n",
      "epoch 232\n",
      "====================================\n",
      "Epoch:  232 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 163.74425817390454\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.010907202959060669\n",
      "Cross Entropy:[0.7558697  0.192055   0.82924736 ... 0.6504463  1.5587137  0.0641991 ]\n",
      "VE Training loss:0.9478546380996704\n",
      "check (1200,) ()\n",
      "epoch 233\n",
      "====================================\n",
      "Epoch:  233 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 163.89986221607663\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.006904483772814274\n",
      "Cross Entropy:[0.80793345 0.18117893 0.5685294  ... 0.46820307 0.33436927 0.5645781 ]\n",
      "VE Training loss:0.9498321413993835\n",
      "check (1200,) ()\n",
      "epoch 234\n",
      "====================================\n",
      "Epoch:  234 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 164.05413630917033\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.001985270995646715\n",
      "Cross Entropy:[0.67474645 1.6140648  0.06622103 ... 1.0133574  0.14169519 0.45051172]\n",
      "VE Training loss:0.9331489205360413\n",
      "check (1110,) ()\n",
      "epoch 235\n",
      "====================================\n",
      "Epoch:  235 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1110.0\n",
      "Mean Reward of that batch 185.0\n",
      "Average Reward of all training: 164.14326764402492\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1110.0\n",
      "Training loss:0.006887617986649275\n",
      "Cross Entropy:[0.6468591  0.22001386 0.7548804  ... 0.30716646 0.5187645  0.3314432 ]\n",
      "VE Training loss:0.9435641169548035\n",
      "check (1151,) ()\n",
      "epoch 236\n",
      "====================================\n",
      "Epoch:  236 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1151.0\n",
      "Mean Reward of that batch 191.83333333333334\n",
      "Average Reward of all training: 164.260598430844\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1151.0\n",
      "Training loss:0.00920812040567398\n",
      "Cross Entropy:[0.94304    0.1557241  0.52867407 ... 0.2542048  0.5103268  1.1542643 ]\n",
      "VE Training loss:0.9356162548065186\n",
      "check (1200,) ()\n",
      "epoch 237\n",
      "====================================\n",
      "Epoch:  237 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 164.41139759358305\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.008441462181508541\n",
      "Cross Entropy:[0.741786   0.19961305 0.62336046 ... 1.0138025  0.12870882 1.0883464 ]\n",
      "VE Training loss:0.9755326509475708\n",
      "check (1163,) ()\n",
      "epoch 238\n",
      "====================================\n",
      "Epoch:  238 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1163.0\n",
      "Mean Reward of that batch 193.83333333333334\n",
      "Average Reward of all training: 164.5350191723215\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1163.0\n",
      "Training loss:-0.01756184548139572\n",
      "Cross Entropy:[0.59074396 0.24648336 0.7277564  ... 1.0643741  2.120231   0.03758192]\n",
      "VE Training loss:0.9532908201217651\n",
      "check (1200,) ()\n",
      "epoch 239\n",
      "====================================\n",
      "Epoch:  239 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 164.6834082134415\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0014974975492805243\n",
      "Cross Entropy:[0.5821647  0.24560198 0.712742   ... 0.27773264 0.7809988  0.20267838]\n",
      "VE Training loss:0.9356278777122498\n",
      "check (1200,) ()\n",
      "epoch 240\n",
      "====================================\n",
      "Epoch:  240 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 164.83056067921882\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.009964149445295334\n",
      "Cross Entropy:[0.46979478 1.3014843  0.09135614 ... 0.71646357 0.23337074 0.6372232 ]\n",
      "VE Training loss:0.9836854934692383\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 241\n",
      "====================================\n",
      "Epoch:  241 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 164.97649196270754\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.001201509265229106\n",
      "Cross Entropy:[0.6025435  0.27075064 0.888546   ... 0.74083126 0.19582112 0.58729553]\n",
      "VE Training loss:0.9378211498260498\n",
      "check (1071,) ()\n",
      "epoch 242\n",
      "====================================\n",
      "Epoch:  242 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1071.0\n",
      "Mean Reward of that batch 178.5\n",
      "Average Reward of all training: 165.03237422732445\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1071.0\n",
      "Training loss:-0.013866438530385494\n",
      "Cross Entropy:[0.7260382  1.7314383  0.05565216 ... 0.54309064 1.4263577  2.5917459 ]\n",
      "VE Training loss:0.991308867931366\n",
      "check (1200,) ()\n",
      "epoch 243\n",
      "====================================\n",
      "Epoch:  243 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 165.17627392186222\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.002786720637232065\n",
      "Cross Entropy:[0.6340364  0.24317792 0.60260737 ... 0.16641544 0.47210824 0.37893197]\n",
      "VE Training loss:0.9722062945365906\n",
      "check (1200,) ()\n",
      "epoch 244\n",
      "====================================\n",
      "Epoch:  244 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 165.31899411070705\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0028826880734413862\n",
      "Cross Entropy:[0.8372876  0.17365289 0.8035011  ... 1.4959129  0.07374931 1.4629626 ]\n",
      "VE Training loss:0.9741281270980835\n",
      "check (1200,) ()\n",
      "epoch 245\n",
      "====================================\n",
      "Epoch:  245 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 165.4605492367858\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.003351453924551606\n",
      "Cross Entropy:[0.7334741  1.6748735  2.8022888  ... 0.7601971  0.17967443 0.84233606]\n",
      "VE Training loss:0.9941600561141968\n",
      "check (1200,) ()\n",
      "epoch 246\n",
      "====================================\n",
      "Epoch:  246 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 165.60095350818096\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.002922812243923545\n",
      "Cross Entropy:[0.7108087  0.19164073 0.7849743  ... 1.5226299  0.06810781 0.2342324 ]\n",
      "VE Training loss:0.9749245047569275\n",
      "check (1198,) ()\n",
      "epoch 247\n",
      "====================================\n",
      "Epoch:  247 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1198.0\n",
      "Mean Reward of that batch 199.66666666666666\n",
      "Average Reward of all training: 165.7388713752194\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1198.0\n",
      "Training loss:-0.018735427409410477\n",
      "Cross Entropy:[0.8024976  0.17612222 0.590757   ... 0.07879952 0.25995606 0.59334236]\n",
      "VE Training loss:0.999729573726654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1200,) ()\n",
      "epoch 248\n",
      "====================================\n",
      "Epoch:  248 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 165.8770210874161\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.019271310418844223\n",
      "Cross Entropy:[0.70500195 0.20107557 0.7260556  ... 1.6246624  0.05459621 0.16785735]\n",
      "VE Training loss:0.9972675442695618\n",
      "check (1200,) ()\n",
      "epoch 249\n",
      "====================================\n",
      "Epoch:  249 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 166.0140611633702\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.005751046352088451\n",
      "Cross Entropy:[0.46848983 0.3357164  1.0608981  ... 0.46601075 0.34209624 1.0095166 ]\n",
      "VE Training loss:1.012027621269226\n",
      "check (1200,) ()\n",
      "epoch 250\n",
      "====================================\n",
      "Epoch:  250 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 166.15000491871672\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0005015619681216776\n",
      "Cross Entropy:[0.60136014 1.5632954  0.06408965 ... 0.2689887  0.52313626 0.33578354]\n",
      "VE Training loss:0.9995720386505127\n",
      "Model saved\n",
      "check (1169,) ()\n",
      "epoch 251\n",
      "====================================\n",
      "Epoch:  251 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1169.0\n",
      "Mean Reward of that batch 194.83333333333334\n",
      "Average Reward of all training: 166.2642811275399\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1169.0\n",
      "Training loss:-0.012081735767424107\n",
      "Cross Entropy:[0.6792685  1.7045357  0.05376058 ... 1.0792952  0.1163116  1.0740061 ]\n",
      "VE Training loss:1.0025230646133423\n",
      "check (1197,) ()\n",
      "epoch 252\n",
      "====================================\n",
      "Epoch:  252 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1197.0\n",
      "Mean Reward of that batch 199.5\n",
      "Average Reward of all training: 166.39616890084332\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1197.0\n",
      "Training loss:-0.006942068692296743\n",
      "Cross Entropy:[0.85698736 0.15506749 0.5357962  ... 0.8231589  0.16744836 0.8748454 ]\n",
      "VE Training loss:1.0288703441619873\n",
      "check (1200,) ()\n",
      "epoch 253\n",
      "====================================\n",
      "Epoch:  253 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 166.52899036763841\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.004454352892935276\n",
      "Cross Entropy:[0.7315059  0.18855989 0.74507284 ... 1.2754811  0.09981558 0.3680367 ]\n",
      "VE Training loss:1.000254511833191\n",
      "check (1200,) ()\n",
      "epoch 254\n",
      "====================================\n",
      "Epoch:  254 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 166.66076599611227\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0053686280734837055\n",
      "Cross Entropy:[0.74827236 0.19274302 0.69126666 ... 0.27413374 0.7101023  0.17499365]\n",
      "VE Training loss:1.0269966125488281\n",
      "check (1125,) ()\n",
      "epoch 255\n",
      "====================================\n",
      "Epoch:  255 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1125.0\n",
      "Mean Reward of that batch 187.5\n",
      "Average Reward of all training: 166.74248848240202\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1125.0\n",
      "Training loss:0.008338847197592258\n",
      "Cross Entropy:[0.6638362  0.21261449 0.6404712  ... 0.27254698 0.5338174  0.27336103]\n",
      "VE Training loss:0.9843549132347107\n",
      "check (1200,) ()\n",
      "epoch 256\n",
      "====================================\n",
      "Epoch:  256 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 166.87240063676765\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.009597858414053917\n",
      "Cross Entropy:[0.75108457 0.18401016 0.71587574 ... 0.46607223 0.3264256  0.4266155 ]\n",
      "VE Training loss:0.9979495406150818\n",
      "check (1180,) ()\n",
      "epoch 257\n",
      "====================================\n",
      "Epoch:  257 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1180.0\n",
      "Mean Reward of that batch 196.66666666666666\n",
      "Average Reward of all training: 166.98833163299292\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1180.0\n",
      "Training loss:-0.014496726915240288\n",
      "Cross Entropy:[0.81675476 1.9466374  0.04034142 ... 0.37824243 0.35756788 0.4628417 ]\n",
      "VE Training loss:1.0072828531265259\n",
      "check (1200,) ()\n",
      "epoch 258\n",
      "====================================\n",
      "Epoch:  258 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 167.11628383596582\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0012603199575096369\n",
      "Cross Entropy:[0.8864274  0.13347188 0.4564938  ... 0.23428257 0.54141444 0.31494692]\n",
      "VE Training loss:1.0312718152999878\n",
      "check (1200,) ()\n",
      "epoch 259\n",
      "====================================\n",
      "Epoch:  259 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 167.2432479910393\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.016111109405755997\n",
      "Cross Entropy:[0.59466136 0.22318132 0.7564517  ... 0.06558867 0.20184256 0.78407395]\n",
      "VE Training loss:0.9846486449241638\n",
      "check (1200,) ()\n",
      "epoch 260\n",
      "====================================\n",
      "Epoch:  260 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 167.3692354987661\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.003385957097634673\n",
      "Cross Entropy:[0.4873281  0.30275556 1.0590429  ... 0.7285619  0.20803702 0.6122308 ]\n",
      "VE Training loss:1.0397719144821167\n",
      "Model saved\n",
      "check (1149,) ()\n",
      "epoch 261\n",
      "====================================\n",
      "Epoch:  261 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1149.0\n",
      "Mean Reward of that batch 191.5\n",
      "Average Reward of all training: 167.46169053516928\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1149.0\n",
      "Training loss:0.01452761422842741\n",
      "Cross Entropy:[0.5962414  0.22113787 0.7744699  ... 0.7753485  0.23195925 0.57576084]\n",
      "VE Training loss:1.0310382843017578\n",
      "check (1151,) ()\n",
      "epoch 262\n",
      "====================================\n",
      "Epoch:  262 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1151.0\n",
      "Mean Reward of that batch 191.83333333333334\n",
      "Average Reward of all training: 167.55471207256687\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1151.0\n",
      "Training loss:-0.00243670167401433\n",
      "Cross Entropy:[0.6800529  0.18539208 0.7483496  ... 0.05839919 0.17982522 0.50689155]\n",
      "VE Training loss:1.013279676437378\n",
      "check (1200,) ()\n",
      "epoch 263\n",
      "====================================\n",
      "Epoch:  263 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 167.6780781863594\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.01294122263789177\n",
      "Cross Entropy:[0.6698704  0.181466   0.782337   ... 0.7380796  0.14881648 0.43896845]\n",
      "VE Training loss:1.0098048448562622\n",
      "check (1200,) ()\n",
      "epoch 264\n",
      "====================================\n",
      "Epoch:  264 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 167.80050970838076\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.008883425034582615\n",
      "Cross Entropy:[0.5041475  0.29257417 0.43418986 ... 0.22999516 0.83987385 0.13351287]\n",
      "VE Training loss:1.0028914213180542\n",
      "check (1190,) ()\n",
      "epoch 265\n",
      "====================================\n",
      "Epoch:  265 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1190.0\n",
      "Mean Reward of that batch 198.33333333333334\n",
      "Average Reward of all training: 167.91572791073907\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1190.0\n",
      "Training loss:-0.01840272545814514\n",
      "Cross Entropy:[0.7301729  0.17491868 0.6341872  ... 2.2352877  0.02054389 0.0535519 ]\n",
      "VE Training loss:0.9898749589920044\n",
      "check (1200,) ()\n",
      "epoch 266\n",
      "====================================\n",
      "Epoch:  266 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 168.03634547498442\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.004190314561128616\n",
      "Cross Entropy:[0.74061143 0.18122974 0.69763047 ... 1.3752176  0.07084941 0.24651094]\n",
      "VE Training loss:0.9876644611358643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1200,) ()\n",
      "epoch 267\n",
      "====================================\n",
      "Epoch:  267 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 168.15605953687586\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.00366887915879488\n",
      "Cross Entropy:[0.6950828  0.2007609  0.62130463 ... 0.07284842 0.28240633 0.52356076]\n",
      "VE Training loss:0.9842110276222229\n",
      "check (1200,) ()\n",
      "epoch 268\n",
      "====================================\n",
      "Epoch:  268 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 168.27488021024573\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0022394387051463127\n",
      "Cross Entropy:[0.7708742  0.15891778 0.83327043 ... 0.2583155  0.50551367 0.31176725]\n",
      "VE Training loss:1.0349596738815308\n",
      "check (1200,) ()\n",
      "epoch 269\n",
      "====================================\n",
      "Epoch:  269 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 168.39281745853478\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.001775971264578402\n",
      "Cross Entropy:[0.56518435 1.5429798  0.06305835 ... 0.07757107 0.23763084 0.69654447]\n",
      "VE Training loss:1.025596022605896\n",
      "check (1107,) ()\n",
      "epoch 270\n",
      "====================================\n",
      "Epoch:  270 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1107.0\n",
      "Mean Reward of that batch 184.5\n",
      "Average Reward of all training: 168.45247369016982\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1107.0\n",
      "Training loss:-0.010830666869878769\n",
      "Cross Entropy:[0.5546142  0.26262975 0.48337445 ... 0.17666984 0.7055223  0.178808  ]\n",
      "VE Training loss:1.009291648864746\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 271\n",
      "====================================\n",
      "Epoch:  271 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 168.56888522636848\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0011625726474449039\n",
      "Cross Entropy:[0.92244434 0.12479281 0.45179012 ... 0.13735701 0.4392295  0.36915272]\n",
      "VE Training loss:1.0151110887527466\n",
      "check (1197,) ()\n",
      "epoch 272\n",
      "====================================\n",
      "Epoch:  272 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1197.0\n",
      "Mean Reward of that batch 199.5\n",
      "Average Reward of all training: 168.68260256009506\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1197.0\n",
      "Training loss:0.007781748194247484\n",
      "Cross Entropy:[0.6318622  0.19982529 0.7010209  ... 0.07187337 0.21580218 0.60082424]\n",
      "VE Training loss:1.021193265914917\n",
      "check (1143,) ()\n",
      "epoch 273\n",
      "====================================\n",
      "Epoch:  273 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1143.0\n",
      "Mean Reward of that batch 190.5\n",
      "Average Reward of all training: 168.76251976683463\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1143.0\n",
      "Training loss:-0.015543436631560326\n",
      "Cross Entropy:[0.7099122  0.1728795  0.6013856  ... 0.31825283 0.86859477 0.17180549]\n",
      "VE Training loss:1.0243724584579468\n",
      "check (1115,) ()\n",
      "epoch 274\n",
      "====================================\n",
      "Epoch:  274 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1115.0\n",
      "Mean Reward of that batch 185.83333333333334\n",
      "Average Reward of all training: 168.82482200612841\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1115.0\n",
      "Training loss:-0.000884003471583128\n",
      "Cross Entropy:[0.7662147  0.1498742  0.8923877  ... 0.16781723 1.1418161  0.08698431]\n",
      "VE Training loss:1.0137913227081299\n",
      "check (1200,) ()\n",
      "epoch 275\n",
      "====================================\n",
      "Epoch:  275 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 168.93818628974248\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0035326986107975245\n",
      "Cross Entropy:[0.7104312  0.17287128 0.7730578  ... 0.8718935  0.13435481 0.4603505 ]\n",
      "VE Training loss:1.009884238243103\n",
      "check (1184,) ()\n",
      "epoch 276\n",
      "====================================\n",
      "Epoch:  276 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1184.0\n",
      "Mean Reward of that batch 197.33333333333334\n",
      "Average Reward of all training: 169.04106725729173\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1184.0\n",
      "Training loss:0.007076622452586889\n",
      "Cross Entropy:[0.586485   0.23929234 0.5313598  ... 2.3256166  0.01915399 0.04755946]\n",
      "VE Training loss:0.9923155307769775\n",
      "check (1200,) ()\n",
      "epoch 277\n",
      "====================================\n",
      "Epoch:  277 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 169.1528323574459\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.008593975566327572\n",
      "Cross Entropy:[0.6776915  0.17849901 0.76007843 ... 0.5321593  0.2492258  0.5566759 ]\n",
      "VE Training loss:1.0160995721817017\n",
      "check (1200,) ()\n",
      "epoch 278\n",
      "====================================\n",
      "Epoch:  278 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 169.26379339213136\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0019181915558874607\n",
      "Cross Entropy:[0.5354127  0.25097528 0.53505635 ... 0.29350457 0.9082585  0.13915865]\n",
      "VE Training loss:0.9822908639907837\n",
      "check (1200,) ()\n",
      "epoch 279\n",
      "====================================\n",
      "Epoch:  279 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 169.37395900721333\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0003852748777717352\n",
      "Cross Entropy:[0.7045117  1.8495792  0.04083008 ... 0.48809028 1.4638213  0.0583334 ]\n",
      "VE Training loss:0.9938045144081116\n",
      "check (1200,) ()\n",
      "epoch 280\n",
      "====================================\n",
      "Epoch:  280 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 169.48333772504472\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.00045364259858615696\n",
      "Cross Entropy:[0.68998265 0.19325538 0.67740834 ... 0.3190982  1.028841   2.243621  ]\n",
      "VE Training loss:0.9896093606948853\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 281\n",
      "====================================\n",
      "Epoch:  281 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 169.59193794666376\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0064739990048110485\n",
      "Cross Entropy:[0.73666644 0.18614556 0.68995285 ... 0.14822213 0.9875746  0.12415433]\n",
      "VE Training loss:0.9876881837844849\n",
      "check (1200,) ()\n",
      "epoch 282\n",
      "====================================\n",
      "Epoch:  282 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 169.6997679539451\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.00420547416433692\n",
      "Cross Entropy:[0.7541642  0.18427889 0.69962645 ... 0.23912689 0.64159495 0.27647197]\n",
      "VE Training loss:0.9850453734397888\n",
      "check (1200,) ()\n",
      "epoch 283\n",
      "====================================\n",
      "Epoch:  283 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 169.806835911705\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0026479267980903387\n",
      "Cross Entropy:[0.6580927  0.20877141 0.6505621  ... 0.3882881  0.36940745 0.41783485]\n",
      "VE Training loss:1.0048837661743164\n",
      "check (1200,) ()\n",
      "epoch 284\n",
      "====================================\n",
      "Epoch:  284 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 169.91314986976238\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0021490741055458784\n",
      "Cross Entropy:[0.72831845 1.7935972  0.04763835 ... 0.2634237  0.6105726  1.6737405 ]\n",
      "VE Training loss:0.9787531495094299\n",
      "check (1200,) ()\n",
      "epoch 285\n",
      "====================================\n",
      "Epoch:  285 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 170.0187177649562\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.010610876604914665\n",
      "Cross Entropy:[0.873611   0.14772885 0.88325346 ... 0.80686194 1.7673212  0.05694611]\n",
      "VE Training loss:1.000814437866211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1200,) ()\n",
      "epoch 286\n",
      "====================================\n",
      "Epoch:  286 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 170.1235474231207\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.003728570183739066\n",
      "Cross Entropy:[0.65642726 0.20016816 0.67206156 ... 0.23446363 0.8311931  1.9739044 ]\n",
      "VE Training loss:0.9813210964202881\n",
      "check (1096,) ()\n",
      "epoch 287\n",
      "====================================\n",
      "Epoch:  287 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1096.0\n",
      "Mean Reward of that batch 182.66666666666666\n",
      "Average Reward of all training: 170.16725167135604\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1096.0\n",
      "Training loss:0.016981318593025208\n",
      "Cross Entropy:[0.6016152  0.22819307 0.61461985 ... 0.79723704 0.18074073 0.8055097 ]\n",
      "VE Training loss:0.9863051772117615\n",
      "check (1200,) ()\n",
      "epoch 288\n",
      "====================================\n",
      "Epoch:  288 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 170.2708376030527\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.004794748965650797\n",
      "Cross Entropy:[0.55046606 0.2673377  0.5104986  ... 0.17768854 0.706953   1.8729167 ]\n",
      "VE Training loss:1.0059057474136353\n",
      "check (1200,) ()\n",
      "epoch 289\n",
      "====================================\n",
      "Epoch:  289 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 170.3737066770906\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.012100091204047203\n",
      "Cross Entropy:[0.6850497  0.19462283 0.66392004 ... 0.27348727 0.8293249  0.17478296]\n",
      "VE Training loss:0.9815753102302551\n",
      "check (1200,) ()\n",
      "epoch 290\n",
      "====================================\n",
      "Epoch:  290 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 170.47586630923857\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0034935411531478167\n",
      "Cross Entropy:[0.5939228  0.23628326 0.781125   ... 0.46186453 0.36430874 0.3637931 ]\n",
      "VE Training loss:0.9872757196426392\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 291\n",
      "====================================\n",
      "Epoch:  291 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 170.57732381333054\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0011916160583496094\n",
      "Cross Entropy:[0.8192356  0.17478387 0.7430005  ... 0.2743533  0.75259566 1.6655667 ]\n",
      "VE Training loss:0.9840079545974731\n",
      "check (1200,) ()\n",
      "epoch 292\n",
      "====================================\n",
      "Epoch:  292 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 170.6780864030109\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.007087430916726589\n",
      "Cross Entropy:[0.8629873  0.14714229 0.4851822  ... 0.92117107 2.0940979  0.03342732]\n",
      "VE Training loss:0.9813894629478455\n",
      "check (1200,) ()\n",
      "epoch 293\n",
      "====================================\n",
      "Epoch:  293 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 170.7781611934443\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.00978910457342863\n",
      "Cross Entropy:[0.9815682  0.12095374 0.3857663  ... 0.22615986 0.70392275 0.18870322]\n",
      "VE Training loss:0.9855916500091553\n",
      "check (1200,) ()\n",
      "epoch 294\n",
      "====================================\n",
      "Epoch:  294 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 170.8775552029904\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.005746414884924889\n",
      "Cross Entropy:[0.7558409  0.18171394 0.8103458  ... 1.1121278  0.13319373 0.46118075]\n",
      "VE Training loss:0.972744882106781\n",
      "check (1200,) ()\n",
      "epoch 295\n",
      "====================================\n",
      "Epoch:  295 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 170.9762753548447\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.011041533201932907\n",
      "Cross Entropy:[0.7065017  0.19700055 0.61692935 ... 0.17783743 0.7631752  0.21198167]\n",
      "VE Training loss:0.9928067922592163\n",
      "check (1200,) ()\n",
      "epoch 296\n",
      "====================================\n",
      "Epoch:  296 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 171.0743284786459\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0015742334071546793\n",
      "Cross Entropy:[0.68267655 0.21910614 0.7144873  ... 1.8980054  0.04091728 0.11861397]\n",
      "VE Training loss:0.9819287061691284\n",
      "check (1200,) ()\n",
      "epoch 297\n",
      "====================================\n",
      "Epoch:  297 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 171.17172131205112\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.00111220998223871\n",
      "Cross Entropy:[0.7466592  0.19064148 0.7900015  ... 0.13756762 0.4076227  0.46528876]\n",
      "VE Training loss:0.9760650396347046\n",
      "check (1200,) ()\n",
      "epoch 298\n",
      "====================================\n",
      "Epoch:  298 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 171.26846050227914\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.010664896108210087\n",
      "Cross Entropy:[0.8822277  0.15408207 0.48135912 ... 0.54233015 0.33785793 1.0480253 ]\n",
      "VE Training loss:0.9789326190948486\n",
      "check (1200,) ()\n",
      "epoch 299\n",
      "====================================\n",
      "Epoch:  299 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 171.3645526076227\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.005122826900333166\n",
      "Cross Entropy:[0.5623059  0.28941616 0.88518727 ... 0.28442937 0.58282065 0.27920744]\n",
      "VE Training loss:0.9824725389480591\n",
      "check (1137,) ()\n",
      "epoch 300\n",
      "====================================\n",
      "Epoch:  300 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1137.0\n",
      "Mean Reward of that batch 189.5\n",
      "Average Reward of all training: 171.4250040989306\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1137.0\n",
      "Training loss:-0.01589260622859001\n",
      "Cross Entropy:[0.6162797  0.27167663 0.56440973 ... 0.41806984 0.42801324 0.39710107]\n",
      "VE Training loss:0.987138032913208\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 301\n",
      "====================================\n",
      "Epoch:  301 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 171.51993764013017\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0008002169779501855\n",
      "Cross Entropy:[0.6681471  0.23269197 0.7037981  ... 0.23756993 0.63925946 0.26612735]\n",
      "VE Training loss:0.9674825072288513\n",
      "check (1200,) ()\n",
      "epoch 302\n",
      "====================================\n",
      "Epoch:  302 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 171.6142424823814\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.006419776938855648\n",
      "Cross Entropy:[0.6764167  0.23450483 0.6680293  ... 1.4456586  0.08112868 0.26771247]\n",
      "VE Training loss:0.9497179985046387\n",
      "check (1200,) ()\n",
      "epoch 303\n",
      "====================================\n",
      "Epoch:  303 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 171.70792485042634\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0039029160980135202\n",
      "Cross Entropy:[0.61578906 1.4290104  0.09003714 ... 0.71872437 0.24226218 0.7479062 ]\n",
      "VE Training loss:0.9531549215316772\n",
      "check (1200,) ()\n",
      "epoch 304\n",
      "====================================\n",
      "Epoch:  304 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 171.80099088710259\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0019923010841012\n",
      "Cross Entropy:[0.5945129  0.27638248 0.7944076  ... 0.15690681 0.4901501  0.3286634 ]\n",
      "VE Training loss:0.9637401103973389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1200,) ()\n",
      "epoch 305\n",
      "====================================\n",
      "Epoch:  305 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 171.89344665468585\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0026046722196042538\n",
      "Cross Entropy:[0.69062    0.22401169 0.7670766  ... 0.11596071 0.3954147  0.38719264]\n",
      "VE Training loss:0.9749867916107178\n",
      "check (1200,) ()\n",
      "epoch 306\n",
      "====================================\n",
      "Epoch:  306 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 171.98529813620647\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.005780340638011694\n",
      "Cross Entropy:[0.5235062  0.3406339  0.4818951  ... 1.4476554  0.07237981 0.14388272]\n",
      "VE Training loss:1.0001882314682007\n",
      "check (1200,) ()\n",
      "epoch 307\n",
      "====================================\n",
      "Epoch:  307 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 172.07655123674002\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.004630756564438343\n",
      "Cross Entropy:[0.8646244  1.7911744  0.05881044 ... 1.7427089  0.05451719 0.14245357]\n",
      "VE Training loss:1.0023266077041626\n",
      "check (1200,) ()\n",
      "epoch 308\n",
      "====================================\n",
      "Epoch:  308 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 172.16721178467267\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0004397833254188299\n",
      "Cross Entropy:[0.5865527  0.28251258 0.63618237 ... 0.10434585 0.23924786 0.86947745]\n",
      "VE Training loss:0.9548911452293396\n",
      "check (1200,) ()\n",
      "epoch 309\n",
      "====================================\n",
      "Epoch:  309 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 172.25728553294235\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.006490676663815975\n",
      "Cross Entropy:[0.76205355 0.23278236 0.6984702  ... 0.25984445 0.6092878  0.35489446]\n",
      "VE Training loss:0.9931541681289673\n",
      "check (1153,) ()\n",
      "epoch 310\n",
      "====================================\n",
      "Epoch:  310 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1153.0\n",
      "Mean Reward of that batch 192.16666666666666\n",
      "Average Reward of all training: 172.32150934305113\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1153.0\n",
      "Training loss:0.004015185404568911\n",
      "Cross Entropy:[0.8467619  1.7542422  2.8106234  ... 0.3181759  0.76825666 0.24517061]\n",
      "VE Training loss:0.986430287361145\n",
      "Model saved\n",
      "check (1151,) ()\n",
      "epoch 311\n",
      "====================================\n",
      "Epoch:  311 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1151.0\n",
      "Mean Reward of that batch 191.83333333333334\n",
      "Average Reward of all training: 172.3842483269427\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1151.0\n",
      "Training loss:-0.01971847005188465\n",
      "Cross Entropy:[0.8086165  0.19227691 0.49146312 ... 1.1539346  0.13867974 0.40812406]\n",
      "VE Training loss:0.9689500331878662\n",
      "check (1200,) ()\n",
      "epoch 312\n",
      "====================================\n",
      "Epoch:  312 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 172.47276035153584\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0005640538292936981\n",
      "Cross Entropy:[0.8431957  0.18336205 0.46761775 ... 0.35676885 0.52504736 1.1808327 ]\n",
      "VE Training loss:0.9918542504310608\n",
      "check (1200,) ()\n",
      "epoch 313\n",
      "====================================\n",
      "Epoch:  313 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 172.56070680408683\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.012652414850890636\n",
      "Cross Entropy:[0.6519185  0.26251826 0.67396635 ... 0.09309001 0.2544641  0.65327746]\n",
      "VE Training loss:0.9750857353210449\n",
      "check (1146,) ()\n",
      "epoch 314\n",
      "====================================\n",
      "Epoch:  314 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1146.0\n",
      "Mean Reward of that batch 191.0\n",
      "Average Reward of all training: 172.61943066776809\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1146.0\n",
      "Training loss:-0.03505207225680351\n",
      "Cross Entropy:[1.0131849  0.14838575 0.3822257  ... 0.11615124 0.34649426 0.53715533]\n",
      "VE Training loss:0.9528024792671204\n",
      "check (1200,) ()\n",
      "epoch 315\n",
      "====================================\n",
      "Epoch:  315 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 172.70635311009264\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0042501226998865604\n",
      "Cross Entropy:[0.5339578  0.36862418 0.9541192  ... 0.56194264 0.4167955  0.44267377]\n",
      "VE Training loss:0.9626319408416748\n",
      "check (1082,) ()\n",
      "epoch 316\n",
      "====================================\n",
      "Epoch:  316 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1082.0\n",
      "Mean Reward of that batch 180.33333333333334\n",
      "Average Reward of all training: 172.73048912345735\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1082.0\n",
      "Training loss:-0.005925741046667099\n",
      "Cross Entropy:[0.63594854 0.28713846 0.6508838  ... 0.57070774 0.3637185  0.9793089 ]\n",
      "VE Training loss:0.9785873889923096\n",
      "check (1200,) ()\n",
      "epoch 317\n",
      "====================================\n",
      "Epoch:  317 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 172.8165128170742\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.012643832713365555\n",
      "Cross Entropy:[0.76895833 0.2336284  0.6200229  ... 0.5689734  0.36252245 0.48184738]\n",
      "VE Training loss:0.9487982988357544\n",
      "check (1200,) ()\n",
      "epoch 318\n",
      "====================================\n",
      "Epoch:  318 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 172.90199548117144\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0050988150760531425\n",
      "Cross Entropy:[0.7853896  0.22320755 0.8270142  ... 0.12106717 0.33461356 0.571589  ]\n",
      "VE Training loss:0.9599675536155701\n",
      "check (1139,) ()\n",
      "epoch 319\n",
      "====================================\n",
      "Epoch:  319 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1139.0\n",
      "Mean Reward of that batch 189.83333333333334\n",
      "Average Reward of all training: 172.95507177537883\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1139.0\n",
      "Training loss:-0.013327171094715595\n",
      "Cross Entropy:[0.666682   0.26382795 0.65949243 ... 2.4662943  0.02597283 0.04424531]\n",
      "VE Training loss:0.9903581738471985\n",
      "check (1160,) ()\n",
      "epoch 320\n",
      "====================================\n",
      "Epoch:  320 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1160.0\n",
      "Mean Reward of that batch 193.33333333333334\n",
      "Average Reward of all training: 173.01875384274746\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1160.0\n",
      "Training loss:-0.0044333310797810555\n",
      "Cross Entropy:[0.967547   0.16426495 1.0762686  ... 0.68133485 0.3580429  0.49162596]\n",
      "VE Training loss:0.9756096005439758\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 321\n",
      "====================================\n",
      "Epoch:  321 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 173.10280756909404\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.01056328509002924\n",
      "Cross Entropy:[0.7363254  0.2511438  0.673334   ... 0.6486305  0.29024568 0.75097674]\n",
      "VE Training loss:0.9836446046829224\n",
      "check (1089,) ()\n",
      "epoch 322\n",
      "====================================\n",
      "Epoch:  322 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1089.0\n",
      "Mean Reward of that batch 181.5\n",
      "Average Reward of all training: 173.12888580645708\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1089.0\n",
      "Training loss:-0.00251125474460423\n",
      "Cross Entropy:[0.6347951  0.29856446 0.7701005  ... 0.49405178 0.37573117 0.8738462 ]\n",
      "VE Training loss:0.9635979533195496\n",
      "check (1200,) ()\n",
      "epoch 323\n",
      "====================================\n",
      "Epoch:  323 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 173.21207811046187\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.004336514510214329\n",
      "Cross Entropy:[0.8342862  0.2004635  0.5084653  ... 0.4807537  0.35515884 0.67479974]\n",
      "VE Training loss:0.9540364742279053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1125,) ()\n",
      "epoch 324\n",
      "====================================\n",
      "Epoch:  324 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1125.0\n",
      "Mean Reward of that batch 187.5\n",
      "Average Reward of all training: 173.2561766348123\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1125.0\n",
      "Training loss:0.012615768238902092\n",
      "Cross Entropy:[0.77538145 0.22079103 0.560025   ... 0.32002345 0.65218335 0.2535715 ]\n",
      "VE Training loss:0.9510459899902344\n",
      "check (1200,) ()\n",
      "epoch 325\n",
      "====================================\n",
      "Epoch:  325 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 173.3384653220898\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.008348347619175911\n",
      "Cross Entropy:[0.8052609  0.22890534 0.61498713 ... 1.4309262  0.10751367 0.3254831 ]\n",
      "VE Training loss:0.9893689751625061\n",
      "check (1200,) ()\n",
      "epoch 326\n",
      "====================================\n",
      "Epoch:  326 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 173.42024917079505\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0013067699037492275\n",
      "Cross Entropy:[0.8415443  0.20075534 0.50643945 ... 0.20194641 0.44081762 0.91528213]\n",
      "VE Training loss:0.9749711155891418\n",
      "check (1131,) ()\n",
      "epoch 327\n",
      "====================================\n",
      "Epoch:  327 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1131.0\n",
      "Mean Reward of that batch 188.5\n",
      "Average Reward of all training: 173.46636461675592\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1131.0\n",
      "Training loss:-0.013065208680927753\n",
      "Cross Entropy:[0.5111163  0.37358677 0.53198004 ... 0.23776206 0.58005244 1.2098992 ]\n",
      "VE Training loss:0.950434148311615\n",
      "check (1146,) ()\n",
      "epoch 328\n",
      "====================================\n",
      "Epoch:  328 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1146.0\n",
      "Mean Reward of that batch 191.0\n",
      "Average Reward of all training: 173.51982082219263\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1146.0\n",
      "Training loss:-0.008680622093379498\n",
      "Cross Entropy:[0.49621478 0.38664752 0.51862884 ... 0.01885608 0.03231657 0.05812186]\n",
      "VE Training loss:0.9838324785232544\n",
      "check (1144,) ()\n",
      "epoch 329\n",
      "====================================\n",
      "Epoch:  329 / 500\n",
      "------------\n",
      "Number of training episodes: 7\n",
      "Total reward:1144.0\n",
      "Mean Reward of that batch 163.42857142857142\n",
      "Average Reward of all training: 173.48914833163448\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1144.0\n",
      "Training loss:-0.016277886927127838\n",
      "Cross Entropy:[0.76285076 0.23311828 0.8429569  ... 0.3629584  0.5763094  0.33988786]\n",
      "VE Training loss:0.9438400864601135\n",
      "check (1200,) ()\n",
      "epoch 330\n",
      "====================================\n",
      "Epoch:  330 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 173.56948424578104\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.007922436110675335\n",
      "Cross Entropy:[0.5629986  1.2396882  0.12820014 ... 0.08794364 0.21030901 0.49721977]\n",
      "VE Training loss:0.9470111131668091\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 331\n",
      "====================================\n",
      "Epoch:  331 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 173.6493347465491\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.004267103038728237\n",
      "Cross Entropy:[0.76857233 0.25974903 0.6829221  ... 1.0648624  0.19266967 0.5401225 ]\n",
      "VE Training loss:0.9234991669654846\n",
      "check (1200,) ()\n",
      "epoch 332\n",
      "====================================\n",
      "Epoch:  332 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 173.72870422020407\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0061841742135584354\n",
      "Cross Entropy:[0.91792035 0.18752435 0.98070014 ... 0.1342556  0.35312754 0.84904087]\n",
      "VE Training loss:0.9481961131095886\n",
      "check (1162,) ()\n",
      "epoch 333\n",
      "====================================\n",
      "Epoch:  333 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1162.0\n",
      "Mean Reward of that batch 193.66666666666666\n",
      "Average Reward of all training: 173.78857798130457\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1162.0\n",
      "Training loss:0.004882467910647392\n",
      "Cross Entropy:[0.601058   0.32888967 0.59125847 ... 0.31691793 0.62923294 1.1302906 ]\n",
      "VE Training loss:0.9462167024612427\n",
      "check (1200,) ()\n",
      "epoch 334\n",
      "====================================\n",
      "Epoch:  334 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 173.86705529273777\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.001333517604507506\n",
      "Cross Entropy:[0.88066125 0.20893984 0.557034   ... 0.10146625 0.25975105 0.7721176 ]\n",
      "VE Training loss:0.9225006103515625\n",
      "check (1200,) ()\n",
      "epoch 335\n",
      "====================================\n",
      "Epoch:  335 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 173.94506408290871\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.004154341295361519\n",
      "Cross Entropy:[0.53245836 0.36915267 0.89292794 ... 0.46195564 0.52635795 0.31152272]\n",
      "VE Training loss:0.9425150752067566\n",
      "check (1153,) ()\n",
      "epoch 336\n",
      "====================================\n",
      "Epoch:  336 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1153.0\n",
      "Mean Reward of that batch 192.16666666666666\n",
      "Average Reward of all training: 173.99929504297944\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1153.0\n",
      "Training loss:-0.010484753176569939\n",
      "Cross Entropy:[1.1948565  0.12780073 1.2714782  ... 0.6518197  1.3822051  0.0971729 ]\n",
      "VE Training loss:0.9385912418365479\n",
      "check (1200,) ()\n",
      "epoch 337\n",
      "====================================\n",
      "Epoch:  337 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 174.0764484701516\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.006255807355046272\n",
      "Cross Entropy:[0.91752434 0.18835275 0.9484992  ... 0.21303345 0.8687409  0.21779174]\n",
      "VE Training loss:0.9377173781394958\n",
      "check (1200,) ()\n",
      "epoch 338\n",
      "====================================\n",
      "Epoch:  338 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 174.1531453681689\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.002699833596125245\n",
      "Cross Entropy:[0.55141085 0.35257855 0.8422052  ... 0.3976881  0.47022855 0.46571207]\n",
      "VE Training loss:0.9614099264144897\n",
      "check (1194,) ()\n",
      "epoch 339\n",
      "====================================\n",
      "Epoch:  339 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1194.0\n",
      "Mean Reward of that batch 199.0\n",
      "Average Reward of all training: 174.226439924605\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1194.0\n",
      "Training loss:-0.0041320533491671085\n",
      "Cross Entropy:[0.40618804 0.9150423  0.21182206 ... 0.57976633 0.34328282 0.5795914 ]\n",
      "VE Training loss:0.9545812606811523\n",
      "check (1200,) ()\n",
      "epoch 340\n",
      "====================================\n",
      "Epoch:  340 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 174.30224451306202\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.006607750430703163\n",
      "Cross Entropy:[0.76831186 0.23132996 0.572888   ... 0.15407574 0.3953741  0.51504856]\n",
      "VE Training loss:0.9062749147415161\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 341\n",
      "====================================\n",
      "Epoch:  341 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 174.37760449982724\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.006939942017197609\n",
      "Cross Entropy:[0.51932645 1.1953944  2.1002162  ... 0.15070556 0.39330846 0.5162501 ]\n",
      "VE Training loss:0.965319812297821\n",
      "check (1200,) ()\n",
      "epoch 342\n",
      "====================================\n",
      "Epoch:  342 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 174.45252378491548\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.005435558967292309\n",
      "Cross Entropy:[0.78758395 0.21386199 0.9226459  ... 1.0843143  0.1812736  0.49637735]\n",
      "VE Training loss:0.9637628197669983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1200,) ()\n",
      "epoch 343\n",
      "====================================\n",
      "Epoch:  343 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 174.52700622286034\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.004519718699157238\n",
      "Cross Entropy:[0.68989533 1.5087637  2.5040486  ... 0.6732346  1.2966617  0.12594414]\n",
      "VE Training loss:0.9203617572784424\n",
      "check (1200,) ()\n",
      "epoch 344\n",
      "====================================\n",
      "Epoch:  344 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 174.60105562337526\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.004547474440187216\n",
      "Cross Entropy:[0.5218447 1.1454923 1.9799428 ... 0.3078931 0.7481061 0.2525732]\n",
      "VE Training loss:0.9420560598373413\n",
      "check (1200,) ()\n",
      "epoch 345\n",
      "====================================\n",
      "Epoch:  345 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 174.67467575200317\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.007981308735907078\n",
      "Cross Entropy:[0.73669004 1.4910598  0.09302788 ... 0.6010917  0.33529153 0.6191123 ]\n",
      "VE Training loss:0.9256885647773743\n",
      "check (1200,) ()\n",
      "epoch 346\n",
      "====================================\n",
      "Epoch:  346 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 174.7478703307546\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.004414808005094528\n",
      "Cross Entropy:[0.42219472 0.4506767  0.4342518  ... 0.3723719  0.7373375  0.31797504]\n",
      "VE Training loss:0.927732527256012\n",
      "check (1176,) ()\n",
      "epoch 347\n",
      "====================================\n",
      "Epoch:  347 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1176.0\n",
      "Mean Reward of that batch 196.0\n",
      "Average Reward of all training: 174.80911566121353\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1176.0\n",
      "Training loss:0.004159265663474798\n",
      "Cross Entropy:[1.0985528  0.14679052 0.3936035  ... 0.2282446  0.80085516 1.4175934 ]\n",
      "VE Training loss:0.970890998840332\n",
      "check (1200,) ()\n",
      "epoch 348\n",
      "====================================\n",
      "Epoch:  348 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 174.8815032598882\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0050757694989442825\n",
      "Cross Entropy:[0.4474435  1.0126731  0.17878029 ... 0.1519057  0.34298837 0.70754397]\n",
      "VE Training loss:0.963914692401886\n",
      "check (1200,) ()\n",
      "epoch 349\n",
      "====================================\n",
      "Epoch:  349 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 174.95347602991717\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0014017232460901141\n",
      "Cross Entropy:[0.28185812 0.7001885  1.4005069  ... 0.04809574 0.12832807 1.2705669 ]\n",
      "VE Training loss:0.9768442511558533\n",
      "check (1199,) ()\n",
      "epoch 350\n",
      "====================================\n",
      "Epoch:  350 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1199.0\n",
      "Mean Reward of that batch 199.83333333333334\n",
      "Average Reward of all training: 175.02456133649835\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1199.0\n",
      "Training loss:-0.005292964167892933\n",
      "Cross Entropy:[0.86685836 0.21493699 0.80932146 ... 0.1659476  1.0738168  2.0473309 ]\n",
      "VE Training loss:1.0014857053756714\n",
      "Model saved\n",
      "check (1172,) ()\n",
      "epoch 351\n",
      "====================================\n",
      "Epoch:  351 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1172.0\n",
      "Mean Reward of that batch 195.33333333333334\n",
      "Average Reward of all training: 175.08242108577707\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1172.0\n",
      "Training loss:0.000971543078776449\n",
      "Cross Entropy:[0.76638424 0.22022066 0.8961146  ... 0.13348126 0.31928533 0.70288086]\n",
      "VE Training loss:0.942030131816864\n",
      "check (1171,) ()\n",
      "epoch 352\n",
      "====================================\n",
      "Epoch:  352 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1171.0\n",
      "Mean Reward of that batch 195.16666666666666\n",
      "Average Reward of all training: 175.13947860163188\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1171.0\n",
      "Training loss:0.007435276638716459\n",
      "Cross Entropy:[0.6218723  0.28453657 0.7091712  ... 0.88331103 0.18494444 0.46294838]\n",
      "VE Training loss:0.9814696907997131\n",
      "check (1200,) ()\n",
      "epoch 353\n",
      "====================================\n",
      "Epoch:  353 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 175.20990500785953\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.016325684264302254\n",
      "Cross Entropy:[0.34888098 0.84350854 0.22356804 ... 0.56116295 0.38455227 0.4578393 ]\n",
      "VE Training loss:0.9432465434074402\n",
      "check (1194,) ()\n",
      "epoch 354\n",
      "====================================\n",
      "Epoch:  354 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1194.0\n",
      "Mean Reward of that batch 199.0\n",
      "Average Reward of all training: 175.27710866602942\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1194.0\n",
      "Training loss:-0.0009811343625187874\n",
      "Cross Entropy:[0.51011324 0.36539862 0.5521134  ... 0.40408388 0.75561184 0.32713926]\n",
      "VE Training loss:0.9766753911972046\n",
      "check (1200,) ()\n",
      "epoch 355\n",
      "====================================\n",
      "Epoch:  355 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 175.34675061344907\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.013202973641455173\n",
      "Cross Entropy:[0.49261656 0.38166377 0.8816246  ... 0.4733584  1.070943   0.14932874]\n",
      "VE Training loss:0.9445600509643555\n",
      "check (1200,) ()\n",
      "epoch 356\n",
      "====================================\n",
      "Epoch:  356 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 175.41600131397308\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0006293408223427832\n",
      "Cross Entropy:[0.5654303  0.36202127 0.8950107  ... 0.09355303 0.23442101 0.827744  ]\n",
      "VE Training loss:0.9669396877288818\n",
      "check (1200,) ()\n",
      "epoch 357\n",
      "====================================\n",
      "Epoch:  357 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 175.48486405539052\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.00845699105411768\n",
      "Cross Entropy:[0.89272326 1.7021314  0.07335901 ... 0.21558869 0.5400826  0.37117827]\n",
      "VE Training loss:0.9589760303497314\n",
      "check (1200,) ()\n",
      "epoch 358\n",
      "====================================\n",
      "Epoch:  358 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 175.55334208875536\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0012259181821718812\n",
      "Cross Entropy:[1.0551777  1.9149076  0.05767207 ... 0.10475678 0.21238537 1.0353259 ]\n",
      "VE Training loss:0.9487703442573547\n",
      "check (1200,) ()\n",
      "epoch 359\n",
      "====================================\n",
      "Epoch:  359 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 175.62143862889812\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0112741868942976\n",
      "Cross Entropy:[0.4761137  1.0792599  0.15988006 ... 1.0972615  0.13538244 1.336032  ]\n",
      "VE Training loss:0.9466426372528076\n",
      "check (1200,) ()\n",
      "epoch 360\n",
      "====================================\n",
      "Epoch:  360 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 175.68915685492894\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.004775173030793667\n",
      "Cross Entropy:[0.5867537  1.2527001  0.12982707 ... 1.1816514  0.13861178 0.35317144]\n",
      "VE Training loss:0.9184172749519348\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 361\n",
      "====================================\n",
      "Epoch:  361 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 175.75649991073246\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.01164211705327034\n",
      "Cross Entropy:[0.8642293  0.22731413 0.5460268  ... 0.25746363 0.5446166  0.41767925]\n",
      "VE Training loss:0.9939243793487549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1200,) ()\n",
      "epoch 362\n",
      "====================================\n",
      "Epoch:  362 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 175.8234709054542\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.001080106943845749\n",
      "Cross Entropy:[0.5590471  0.35754752 0.64915955 ... 1.5876428  0.08489294 0.14147447]\n",
      "VE Training loss:0.9496296048164368\n",
      "check (1200,) ()\n",
      "epoch 363\n",
      "====================================\n",
      "Epoch:  363 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 175.8900729139791\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.007535476703196764\n",
      "Cross Entropy:[0.68845856 0.3078575  0.68847287 ... 2.1005445  0.04597885 0.0690725 ]\n",
      "VE Training loss:0.9432631134986877\n",
      "check (1200,) ()\n",
      "epoch 364\n",
      "====================================\n",
      "Epoch:  364 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 175.95630897740224\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.005718231201171875\n",
      "Cross Entropy:[0.850091   0.227154   0.48326313 ... 0.25605312 0.83012664 0.2579743 ]\n",
      "VE Training loss:0.965110182762146\n",
      "check (1200,) ()\n",
      "epoch 365\n",
      "====================================\n",
      "Epoch:  365 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 176.02218210349156\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.006645099725574255\n",
      "Cross Entropy:[0.42289504 0.8972208  0.23284103 ... 0.18936338 0.36060435 0.7051755 ]\n",
      "VE Training loss:0.9668782353401184\n",
      "check (1200,) ()\n",
      "epoch 366\n",
      "====================================\n",
      "Epoch:  366 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 176.08769526714323\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.006257606204599142\n",
      "Cross Entropy:[0.7445932  0.30235857 0.6589579  ... 0.2976249  0.86979234 0.2008118 ]\n",
      "VE Training loss:1.0287925004959106\n",
      "check (1200,) ()\n",
      "epoch 367\n",
      "====================================\n",
      "Epoch:  367 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 176.15285141082947\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0002793931926134974\n",
      "Cross Entropy:[0.59719026 0.38311216 0.7873727  ... 0.17552638 1.2725877  0.12234496]\n",
      "VE Training loss:0.9755902886390686\n",
      "check (1071,) ()\n",
      "epoch 368\n",
      "====================================\n",
      "Epoch:  368 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1071.0\n",
      "Mean Reward of that batch 178.5\n",
      "Average Reward of all training: 176.1592295319957\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1071.0\n",
      "Training loss:0.0013558381469920278\n",
      "Cross Entropy:[0.59026253 0.41074452 0.8597224  ... 0.27058622 0.5334617  0.9590045 ]\n",
      "VE Training loss:0.957286536693573\n",
      "check (1167,) ()\n",
      "epoch 369\n",
      "====================================\n",
      "Epoch:  369 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1167.0\n",
      "Mean Reward of that batch 194.5\n",
      "Average Reward of all training: 176.20893351700386\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1167.0\n",
      "Training loss:0.0070890942588448524\n",
      "Cross Entropy:[0.47092664 0.9201131  0.24374217 ... 0.10183196 1.9650882  0.04995247]\n",
      "VE Training loss:0.9503691792488098\n",
      "check (1069,) ()\n",
      "epoch 370\n",
      "====================================\n",
      "Epoch:  370 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1069.0\n",
      "Mean Reward of that batch 178.16666666666666\n",
      "Average Reward of all training: 176.21422468767864\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1069.0\n",
      "Training loss:0.002695197705179453\n",
      "Cross Entropy:[0.42677283 0.8405607  0.27640933 ... 0.51626694 0.5074721  0.43783957]\n",
      "VE Training loss:0.9715636968612671\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 371\n",
      "====================================\n",
      "Epoch:  371 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 176.27833728959862\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.005344822071492672\n",
      "Cross Entropy:[0.6974971  0.31259334 0.6147462  ... 1.2111441  0.16855909 0.38286132]\n",
      "VE Training loss:0.976335883140564\n",
      "check (1199,) ()\n",
      "epoch 372\n",
      "====================================\n",
      "Epoch:  372 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1199.0\n",
      "Mean Reward of that batch 199.83333333333334\n",
      "Average Reward of all training: 176.34165717143662\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1199.0\n",
      "Training loss:-0.020382089540362358\n",
      "Cross Entropy:[0.9178568  0.24363257 0.8986292  ... 1.7411861  0.08399086 0.16180679]\n",
      "VE Training loss:0.9387180805206299\n",
      "check (1131,) ()\n",
      "epoch 373\n",
      "====================================\n",
      "Epoch:  373 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1131.0\n",
      "Mean Reward of that batch 188.5\n",
      "Average Reward of all training: 176.3742532648108\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1131.0\n",
      "Training loss:-0.0023681563325226307\n",
      "Cross Entropy:[0.40421486 0.6161917  0.3631258  ... 0.829106   0.24277234 0.4471429 ]\n",
      "VE Training loss:0.9568831920623779\n",
      "check (1200,) ()\n",
      "epoch 374\n",
      "====================================\n",
      "Epoch:  374 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 176.4374237106268\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.01045827567577362\n",
      "Cross Entropy:[0.5810807  1.0851984  0.19602594 ... 0.06559782 0.11990144 1.6129493 ]\n",
      "VE Training loss:0.9512467384338379\n",
      "check (1111,) ()\n",
      "epoch 375\n",
      "====================================\n",
      "Epoch:  375 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1111.0\n",
      "Mean Reward of that batch 185.16666666666666\n",
      "Average Reward of all training: 176.4607016918429\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1111.0\n",
      "Training loss:-0.010226535610854626\n",
      "Cross Entropy:[0.60415465 0.35818154 0.6821859  ... 0.11100461 0.19783865 1.1742091 ]\n",
      "VE Training loss:0.965330958366394\n",
      "check (1200,) ()\n",
      "epoch 376\n",
      "====================================\n",
      "Epoch:  376 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 176.5233062086199\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.015524027869105339\n",
      "Cross Entropy:[0.78655267 1.431715   0.11930542 ... 0.23487674 0.522601   0.4423424 ]\n",
      "VE Training loss:0.9713418483734131\n",
      "check (1200,) ()\n",
      "epoch 377\n",
      "====================================\n",
      "Epoch:  377 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 176.58557860594453\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.004872471559792757\n",
      "Cross Entropy:[0.55806214 0.4070433  0.58866787 ... 0.7621312  0.23969962 0.40749484]\n",
      "VE Training loss:0.9555995464324951\n",
      "check (1200,) ()\n",
      "epoch 378\n",
      "====================================\n",
      "Epoch:  378 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 176.64752151968543\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.004063854925334454\n",
      "Cross Entropy:[0.48319066 0.50710636 0.44560984 ... 0.595392   0.39147297 0.77669525]\n",
      "VE Training loss:0.9585015773773193\n",
      "check (1145,) ()\n",
      "epoch 379\n",
      "====================================\n",
      "Epoch:  379 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1145.0\n",
      "Mean Reward of that batch 190.83333333333334\n",
      "Average Reward of all training: 176.68495110230717\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1145.0\n",
      "Training loss:0.005006303545087576\n",
      "Cross Entropy:[0.48382604 0.9311098  0.24339326 ... 0.50325155 0.958364   0.22087505]\n",
      "VE Training loss:0.9689565896987915\n",
      "check (1200,) ()\n",
      "epoch 380\n",
      "====================================\n",
      "Epoch:  380 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 176.7463064941432\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.006781075615435839\n",
      "Cross Entropy:[0.80208945 1.4374287  0.1163307  ... 0.4905784  0.5735466  1.122299  ]\n",
      "VE Training loss:1.0410608053207397\n",
      "Model saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1200,) ()\n",
      "epoch 381\n",
      "====================================\n",
      "Epoch:  381 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 176.80733981043156\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.011698850430548191\n",
      "Cross Entropy:[0.59472847 1.1016847  0.19257784 ... 0.19203444 0.3978257  0.6180343 ]\n",
      "VE Training loss:0.9443707466125488\n",
      "check (1128,) ()\n",
      "epoch 382\n",
      "====================================\n",
      "Epoch:  382 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1128.0\n",
      "Mean Reward of that batch 188.0\n",
      "Average Reward of all training: 176.83663996799586\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1128.0\n",
      "Training loss:0.009476106613874435\n",
      "Cross Entropy:[0.4070606  0.5967596  1.1024557  ... 0.28862214 0.9518056  0.24187551]\n",
      "VE Training loss:0.9454041719436646\n",
      "check (1099,) ()\n",
      "epoch 383\n",
      "====================================\n",
      "Epoch:  383 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1099.0\n",
      "Mean Reward of that batch 183.16666666666666\n",
      "Average Reward of all training: 176.8531674528488\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1099.0\n",
      "Training loss:-0.0009840878192335367\n",
      "Cross Entropy:[0.6433707  0.35300133 0.73185956 ... 0.46963692 0.52046645 0.48088962]\n",
      "VE Training loss:0.9541409015655518\n",
      "check (1200,) ()\n",
      "epoch 384\n",
      "====================================\n",
      "Epoch:  384 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 176.91344566260702\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0014431615127250552\n",
      "Cross Entropy:[0.601162   1.0739963  1.6672654  ... 0.77277124 1.2532264  0.15992476]\n",
      "VE Training loss:0.9581979513168335\n",
      "check (1200,) ()\n",
      "epoch 385\n",
      "====================================\n",
      "Epoch:  385 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 176.97341073880804\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0007484658272005618\n",
      "Cross Entropy:[0.83183175 0.25948164 0.9559267  ... 1.3962239  0.12129201 0.24780695]\n",
      "VE Training loss:0.9750065207481384\n",
      "check (1163,) ()\n",
      "epoch 386\n",
      "====================================\n",
      "Epoch:  386 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1163.0\n",
      "Mean Reward of that batch 193.83333333333334\n",
      "Average Reward of all training: 177.0170892947524\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1163.0\n",
      "Training loss:-0.00099264457821846\n",
      "Cross Entropy:[0.44947004 0.822165   0.3073092  ... 0.21742493 1.0698763  1.7587957 ]\n",
      "VE Training loss:0.9667197465896606\n",
      "check (1200,) ()\n",
      "epoch 387\n",
      "====================================\n",
      "Epoch:  387 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 177.0764766609158\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.005643331911414862\n",
      "Cross Entropy:[0.5854121  0.39648917 0.7130674  ... 0.7106576  0.34791917 0.6942561 ]\n",
      "VE Training loss:1.0185132026672363\n",
      "check (1026,) ()\n",
      "epoch 388\n",
      "====================================\n",
      "Epoch:  388 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1026.0\n",
      "Mean Reward of that batch 171.0\n",
      "Average Reward of all training: 177.06081563859385\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1026.0\n",
      "Training loss:-0.001594402128830552\n",
      "Cross Entropy:[0.5303003  0.9884542  0.22893453 ... 0.04791599 0.06408898 0.08579487]\n",
      "VE Training loss:0.9683895111083984\n",
      "check (1164,) ()\n",
      "epoch 389\n",
      "====================================\n",
      "Epoch:  389 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1164.0\n",
      "Mean Reward of that batch 194.0\n",
      "Average Reward of all training: 177.10436109967716\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1164.0\n",
      "Training loss:0.002387689659371972\n",
      "Cross Entropy:[0.74549496 1.2888011  0.15675487 ... 0.26771092 0.9814356  0.22402455]\n",
      "VE Training loss:1.0336792469024658\n",
      "check (1200,) ()\n",
      "epoch 390\n",
      "====================================\n",
      "Epoch:  390 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 177.16306786608826\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.008917968720197678\n",
      "Cross Entropy:[0.6687864  1.2193495  1.9225429  ... 0.77906156 0.28073794 0.5075968 ]\n",
      "VE Training loss:1.0250800848007202\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 391\n",
      "====================================\n",
      "Epoch:  391 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 177.22147434213406\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0011291229166090488\n",
      "Cross Entropy:[0.87141114 0.26425558 0.49620244 ... 0.3953414  0.6500811  1.1514887 ]\n",
      "VE Training loss:1.000206470489502\n",
      "check (1149,) ()\n",
      "epoch 392\n",
      "====================================\n",
      "Epoch:  392 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1149.0\n",
      "Mean Reward of that batch 191.5\n",
      "Average Reward of all training: 177.25789915248578\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1149.0\n",
      "Training loss:-0.002785965334624052\n",
      "Cross Entropy:[0.64147794 0.4150752  0.80248487 ... 0.45502746 0.599304   0.4106237 ]\n",
      "VE Training loss:0.9728230237960815\n",
      "check (1046,) ()\n",
      "epoch 393\n",
      "====================================\n",
      "Epoch:  393 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1046.0\n",
      "Mean Reward of that batch 174.33333333333334\n",
      "Average Reward of all training: 177.25045750918005\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1046.0\n",
      "Training loss:0.008036485873162746\n",
      "Cross Entropy:[0.44560146 0.59643    1.0709409  ... 0.8928778  0.26100174 0.9170487 ]\n",
      "VE Training loss:0.9638420343399048\n",
      "check (1100,) ()\n",
      "epoch 394\n",
      "====================================\n",
      "Epoch:  394 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1100.0\n",
      "Mean Reward of that batch 183.33333333333334\n",
      "Average Reward of all training: 177.26589628030735\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1100.0\n",
      "Training loss:0.0030133125837892294\n",
      "Cross Entropy:[0.74664015 1.2296213  1.7991692  ... 0.72665673 0.3285751  0.5310893 ]\n",
      "VE Training loss:0.9528427124023438\n",
      "check (1161,) ()\n",
      "epoch 395\n",
      "====================================\n",
      "Epoch:  395 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1161.0\n",
      "Mean Reward of that batch 193.5\n",
      "Average Reward of all training: 177.30699527706605\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1161.0\n",
      "Training loss:-0.004527446813881397\n",
      "Cross Entropy:[0.6307689  0.42051965 0.6282771  ... 0.28654984 0.43879867 0.67029524]\n",
      "VE Training loss:0.979529857635498\n",
      "check (1020,) ()\n",
      "epoch 396\n",
      "====================================\n",
      "Epoch:  396 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1020.0\n",
      "Mean Reward of that batch 170.0\n",
      "Average Reward of all training: 177.28854326879065\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1020.0\n",
      "Training loss:-0.005113224033266306\n",
      "Cross Entropy:[0.49815446 0.5635114  0.45704335 ... 2.3019195  0.04461465 0.05647923]\n",
      "VE Training loss:0.9798167943954468\n",
      "check (1081,) ()\n",
      "epoch 397\n",
      "====================================\n",
      "Epoch:  397 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1081.0\n",
      "Mean Reward of that batch 180.16666666666666\n",
      "Average Reward of all training: 177.2957929498936\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1081.0\n",
      "Training loss:-0.012076184153556824\n",
      "Cross Entropy:[0.96880484 1.5582547  0.11616673 ... 1.762557   2.5480723  0.03526854]\n",
      "VE Training loss:0.9876484274864197\n",
      "check (1098,) ()\n",
      "epoch 398\n",
      "====================================\n",
      "Epoch:  398 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1098.0\n",
      "Mean Reward of that batch 183.0\n",
      "Average Reward of all training: 177.31012512841147\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1098.0\n",
      "Training loss:-0.004120307508856058\n",
      "Cross Entropy:[0.7237608  1.2090786  0.18358104 ... 0.2964483  0.51564634 0.5405698 ]\n",
      "VE Training loss:0.9613850712776184\n",
      "check (1085,) ()\n",
      "epoch 399\n",
      "====================================\n",
      "Epoch:  399 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1085.0\n",
      "Mean Reward of that batch 180.83333333333334\n",
      "Average Reward of all training: 177.31895522416315\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1085.0\n",
      "Training loss:0.004510160535573959\n",
      "Cross Entropy:[0.81919867 0.29135364 0.5230629  ... 0.16910432 0.29866078 0.8955145 ]\n",
      "VE Training loss:1.0136361122131348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1200,) ()\n",
      "epoch 400\n",
      "====================================\n",
      "Epoch:  400 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 177.37565783610273\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0006319443346001208\n",
      "Cross Entropy:[0.6516976  0.3935681  0.67606634 ... 0.32965788 0.59045005 0.46019083]\n",
      "VE Training loss:0.9561377763748169\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 401\n",
      "====================================\n",
      "Epoch:  401 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 177.43207764199772\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.007370618171989918\n",
      "Cross Entropy:[0.7526746  1.2633947  0.16261615 ... 0.40139747 0.77589333 0.29973596]\n",
      "VE Training loss:1.0148626565933228\n",
      "check (1200,) ()\n",
      "epoch 402\n",
      "====================================\n",
      "Epoch:  402 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 177.48821675234103\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.003313415916636586\n",
      "Cross Entropy:[0.53975886 0.95000994 0.2510354  ... 0.6432708  0.4685736  0.50957656]\n",
      "VE Training loss:1.034851312637329\n",
      "check (1184,) ()\n",
      "epoch 403\n",
      "====================================\n",
      "Epoch:  403 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1184.0\n",
      "Mean Reward of that batch 197.33333333333334\n",
      "Average Reward of all training: 177.53746021780253\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1184.0\n",
      "Training loss:-0.01407981850206852\n",
      "Cross Entropy:[0.5593064  0.48343575 0.522766   ... 1.4494219  0.16005418 0.3230746 ]\n",
      "VE Training loss:0.9982182383537292\n",
      "check (1200,) ()\n",
      "epoch 404\n",
      "====================================\n",
      "Epoch:  404 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 177.59306056379808\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.014916757121682167\n",
      "Cross Entropy:[0.93246704 1.5404743  0.1151419  ... 0.18657576 1.3450239  1.9851414 ]\n",
      "VE Training loss:1.0472522974014282\n",
      "check (1077,) ()\n",
      "epoch 405\n",
      "====================================\n",
      "Epoch:  405 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1077.0\n",
      "Mean Reward of that batch 179.5\n",
      "Average Reward of all training: 177.59776905623315\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1077.0\n",
      "Training loss:0.0026313140988349915\n",
      "Cross Entropy:[0.77356064 1.3359804  0.14081684 ... 0.13095912 0.20599002 0.34020993]\n",
      "VE Training loss:1.0171122550964355\n",
      "check (1023,) ()\n",
      "epoch 406\n",
      "====================================\n",
      "Epoch:  406 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1023.0\n",
      "Mean Reward of that batch 170.5\n",
      "Average Reward of all training: 177.58028686643945\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1023.0\n",
      "Training loss:0.006034892052412033\n",
      "Cross Entropy:[0.5257208  0.9509588  0.24397588 ... 0.12956624 0.16742568 1.6045392 ]\n",
      "VE Training loss:1.0355485677719116\n",
      "check (1075,) ()\n",
      "epoch 407\n",
      "====================================\n",
      "Epoch:  407 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1075.0\n",
      "Mean Reward of that batch 179.16666666666666\n",
      "Average Reward of all training: 177.58418460550638\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1075.0\n",
      "Training loss:-0.0021315428894013166\n",
      "Cross Entropy:[0.8163306  1.342037   0.1535978  ... 0.18869573 1.2904059  0.13551529]\n",
      "VE Training loss:0.9932205080986023\n",
      "check (1200,) ()\n",
      "epoch 408\n",
      "====================================\n",
      "Epoch:  408 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 177.63912532951247\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.007104970049113035\n",
      "Cross Entropy:[0.5038764  0.92556775 0.25032684 ... 0.3415908  0.61721873 0.42624134]\n",
      "VE Training loss:1.0189913511276245\n",
      "check (1020,) ()\n",
      "epoch 409\n",
      "====================================\n",
      "Epoch:  409 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1020.0\n",
      "Mean Reward of that batch 170.0\n",
      "Average Reward of all training: 177.62044776146965\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1020.0\n",
      "Training loss:-0.005544494371861219\n",
      "Cross Entropy:[0.6173356  1.0682213  0.2115916  ... 1.0760658  0.1609029  0.22500083]\n",
      "VE Training loss:1.0226876735687256\n",
      "check (1061,) ()\n",
      "epoch 410\n",
      "====================================\n",
      "Epoch:  410 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1061.0\n",
      "Mean Reward of that batch 176.83333333333334\n",
      "Average Reward of all training: 177.6185279701815\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1061.0\n",
      "Training loss:-2.134486749127973e-05\n",
      "Cross Entropy:[0.70515716 0.33490193 0.62951756 ... 1.2783468  0.14579372 0.25185573]\n",
      "VE Training loss:0.9661123156547546\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 411\n",
      "====================================\n",
      "Epoch:  411 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 177.6729841065071\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.012463671155273914\n",
      "Cross Entropy:[0.76673025 1.2839391  0.16304725 ... 0.5143123  0.9353525  0.24245808]\n",
      "VE Training loss:0.9818941354751587\n",
      "check (1200,) ()\n",
      "epoch 412\n",
      "====================================\n",
      "Epoch:  412 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 177.72717589265636\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.003691592952236533\n",
      "Cross Entropy:[0.626261   1.149806   0.1740347  ... 0.34814566 0.6377996  0.4063595 ]\n",
      "VE Training loss:0.9965880513191223\n",
      "check (1200,) ()\n",
      "epoch 413\n",
      "====================================\n",
      "Epoch:  413 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 177.78110524884846\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.005418469198048115\n",
      "Cross Entropy:[0.6775782  0.35853586 0.73837274 ... 1.1588867  0.17449486 0.35975108]\n",
      "VE Training loss:1.0113967657089233\n",
      "check (1200,) ()\n",
      "epoch 414\n",
      "====================================\n",
      "Epoch:  414 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 177.8347740767498\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0011794384336099029\n",
      "Cross Entropy:[0.62512946 0.37975213 0.5990466  ... 0.1468357  1.5337116  0.10236704]\n",
      "VE Training loss:0.9793758988380432\n",
      "check (1200,) ()\n",
      "epoch 415\n",
      "====================================\n",
      "Epoch:  415 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 177.8881842596974\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.01010997872799635\n",
      "Cross Entropy:[0.7972436  0.26684058 0.84419584 ... 0.64726514 0.3627699  0.62646294]\n",
      "VE Training loss:0.9683937430381775\n",
      "check (1200,) ()\n",
      "epoch 416\n",
      "====================================\n",
      "Epoch:  416 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 177.94133766291927\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0015578801976516843\n",
      "Cross Entropy:[0.47020203 0.49668092 0.45546705 ... 0.15042622 1.3707271  0.12418223]\n",
      "VE Training loss:0.9874103665351868\n",
      "check (1200,) ()\n",
      "epoch 417\n",
      "====================================\n",
      "Epoch:  417 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 177.9942361337516\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.01250335294753313\n",
      "Cross Entropy:[0.7686962  1.4217263  0.1179051  ... 0.19361426 1.12464    1.9755461 ]\n",
      "VE Training loss:0.9541975855827332\n",
      "check (1200,) ()\n",
      "epoch 418\n",
      "====================================\n",
      "Epoch:  418 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 178.04688150185268\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.008571223355829716\n",
      "Cross Entropy:[0.55980337 0.41574404 0.9187089  ... 0.40998152 0.5384059  0.40623182]\n",
      "VE Training loss:0.973319411277771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1200,) ()\n",
      "epoch 419\n",
      "====================================\n",
      "Epoch:  419 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 178.0992755794139\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.011900408193469048\n",
      "Cross Entropy:[0.6350927  1.2053525  0.15847105 ... 0.6172737  1.2123246  0.13729301]\n",
      "VE Training loss:0.9679028391838074\n",
      "check (1200,) ()\n",
      "epoch 420\n",
      "====================================\n",
      "Epoch:  420 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 178.15142016136767\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0013401213800534606\n",
      "Cross Entropy:[0.83728087 1.6018106  0.08347283 ... 0.18710592 0.3669547  0.6878756 ]\n",
      "VE Training loss:0.9665055274963379\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 421\n",
      "====================================\n",
      "Epoch:  421 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 178.20331702559244\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0025092600844800472\n",
      "Cross Entropy:[0.6405908  0.35296223 0.5888157  ... 0.14206012 0.31055078 0.7451238 ]\n",
      "VE Training loss:0.9659799933433533\n",
      "check (1092,) ()\n",
      "epoch 422\n",
      "====================================\n",
      "Epoch:  422 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1092.0\n",
      "Mean Reward of that batch 182.0\n",
      "Average Reward of all training: 178.21231390467872\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1092.0\n",
      "Training loss:-0.00922979786992073\n",
      "Cross Entropy:[0.886419   0.20519221 0.45080537 ... 0.03209437 0.0444044  0.06277828]\n",
      "VE Training loss:0.9731371402740479\n",
      "check (1200,) ()\n",
      "epoch 423\n",
      "====================================\n",
      "Epoch:  423 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 178.26382143681897\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0012732545146718621\n",
      "Cross Entropy:[0.5991864  0.37317958 0.55030537 ... 0.06969332 0.13563634 0.2661154 ]\n",
      "VE Training loss:0.9321499466896057\n",
      "check (1200,) ()\n",
      "epoch 424\n",
      "====================================\n",
      "Epoch:  424 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 178.31508600890194\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.010943016968667507\n",
      "Cross Entropy:[1.1239066  2.0080135  0.05101481 ... 0.17808199 0.36192596 0.66109   ]\n",
      "VE Training loss:0.9714314937591553\n",
      "check (1200,) ()\n",
      "epoch 425\n",
      "====================================\n",
      "Epoch:  425 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 178.3661093359398\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0010241619311273098\n",
      "Cross Entropy:[0.6130603  0.33446214 0.76250947 ... 0.06017966 0.12408525 1.4401724 ]\n",
      "VE Training loss:0.9620299339294434\n",
      "check (1200,) ()\n",
      "epoch 426\n",
      "====================================\n",
      "Epoch:  426 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 178.41689311684135\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.004738735966384411\n",
      "Cross Entropy:[0.68249065 0.28221208 0.717029   ... 1.0026677  0.1873499  0.4809624 ]\n",
      "VE Training loss:0.9313998222351074\n",
      "check (1200,) ()\n",
      "epoch 427\n",
      "====================================\n",
      "Epoch:  427 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 178.46743903460052\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.003815480973571539\n",
      "Cross Entropy:[0.6819484  0.30193663 0.62902135 ... 0.34735012 0.5668048  0.37259516]\n",
      "VE Training loss:0.9626896381378174\n",
      "check (1200,) ()\n",
      "epoch 428\n",
      "====================================\n",
      "Epoch:  428 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 178.5177487564823\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.007933005690574646\n",
      "Cross Entropy:[0.6006689  1.2138658  0.14075439 ... 0.5585242  0.33026347 0.64659256]\n",
      "VE Training loss:0.9366220831871033\n",
      "check (1200,) ()\n",
      "epoch 429\n",
      "====================================\n",
      "Epoch:  429 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 178.5678239342061\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.012094794772565365\n",
      "Cross Entropy:[0.47162068 0.44664264 0.4198456  ... 0.78481007 0.23001051 0.8546584 ]\n",
      "VE Training loss:0.9533325433731079\n",
      "check (1200,) ()\n",
      "epoch 430\n",
      "====================================\n",
      "Epoch:  430 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 178.61766620412655\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.002230450278148055\n",
      "Cross Entropy:[1.0425647  1.9348574  0.05362693 ... 1.3684013  0.12531753 0.33255458]\n",
      "VE Training loss:0.9480822682380676\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 431\n",
      "====================================\n",
      "Epoch:  431 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 178.66727718741166\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.001530530396848917\n",
      "Cross Entropy:[1.051237   0.14690898 0.34950513 ... 0.82660747 0.22639573 0.5418048 ]\n",
      "VE Training loss:0.963501513004303\n",
      "check (1200,) ()\n",
      "epoch 432\n",
      "====================================\n",
      "Epoch:  432 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 178.71665849021858\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0007435782463289797\n",
      "Cross Entropy:[0.41540214 0.96358097 0.18856    ... 0.9203433  0.21665002 0.82737803]\n",
      "VE Training loss:0.9479628205299377\n",
      "check (1200,) ()\n",
      "epoch 433\n",
      "====================================\n",
      "Epoch:  433 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 178.76581170386703\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.008176819421350956\n",
      "Cross Entropy:[0.6025537  0.3273601  0.8221225  ... 1.2219818  0.1244671  0.31200224]\n",
      "VE Training loss:0.9402336478233337\n",
      "check (1200,) ()\n",
      "epoch 434\n",
      "====================================\n",
      "Epoch:  434 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 178.8147384050102\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.005720684304833412\n",
      "Cross Entropy:[0.7622875  0.23336507 0.8186225  ... 0.14185533 0.31691608 0.66009784]\n",
      "VE Training loss:0.9353265166282654\n",
      "check (1138,) ()\n",
      "epoch 435\n",
      "====================================\n",
      "Epoch:  435 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1138.0\n",
      "Mean Reward of that batch 189.66666666666666\n",
      "Average Reward of all training: 178.83968536653126\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1138.0\n",
      "Training loss:0.004593970254063606\n",
      "Cross Entropy:[0.5952835  0.3182211  0.7732444  ... 0.54914767 1.1855676  0.1286777 ]\n",
      "VE Training loss:0.9612038731575012\n",
      "check (1200,) ()\n",
      "epoch 436\n",
      "====================================\n",
      "Epoch:  436 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 178.88821819825938\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.008248736150562763\n",
      "Cross Entropy:[0.65070164 0.30093688 0.7807814  ... 0.40288553 0.51800394 0.3544963 ]\n",
      "VE Training loss:0.9605482816696167\n",
      "check (1200,) ()\n",
      "epoch 437\n",
      "====================================\n",
      "Epoch:  437 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 178.9365289117645\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0036796831991523504\n",
      "Cross Entropy:[0.8128337  0.22740473 0.80445313 ... 0.07902756 0.20920785 0.55125415]\n",
      "VE Training loss:0.9700894355773926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1200,) ()\n",
      "epoch 438\n",
      "====================================\n",
      "Epoch:  438 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 178.98461902840432\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.002897935686632991\n",
      "Cross Entropy:[0.37784377 0.857779   0.23083976 ... 0.34191784 0.8300946  1.6173806 ]\n",
      "VE Training loss:0.9745643138885498\n",
      "check (1200,) ()\n",
      "epoch 439\n",
      "====================================\n",
      "Epoch:  439 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 179.03249005567446\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.011899157427251339\n",
      "Cross Entropy:[0.5775623  0.33244002 0.7861768  ... 0.23028183 0.47263953 0.5109935 ]\n",
      "VE Training loss:1.004057765007019\n",
      "check (1200,) ()\n",
      "epoch 440\n",
      "====================================\n",
      "Epoch:  440 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 179.08014348736612\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.003939531743526459\n",
      "Cross Entropy:[0.5795929  1.2204067  0.13326417 ... 2.1418276  0.04135099 0.09898409]\n",
      "VE Training loss:0.9761849045753479\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 441\n",
      "====================================\n",
      "Epoch:  441 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 179.1275808037213\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.011052882298827171\n",
      "Cross Entropy:[0.6390038  0.32062143 0.6053073  ... 0.40072885 0.7830503  0.2788975 ]\n",
      "VE Training loss:0.9538525342941284\n",
      "check (1200,) ()\n",
      "epoch 442\n",
      "====================================\n",
      "Epoch:  442 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 179.1748034715862\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.006000701803714037\n",
      "Cross Entropy:[0.9759046  0.17362072 0.41297373 ... 1.0041902  0.18222553 1.1631559 ]\n",
      "VE Training loss:0.9765545725822449\n",
      "check (1187,) ()\n",
      "epoch 443\n",
      "====================================\n",
      "Epoch:  443 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1187.0\n",
      "Mean Reward of that batch 197.83333333333334\n",
      "Average Reward of all training: 179.2169220491522\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1187.0\n",
      "Training loss:-0.010231917724013329\n",
      "Cross Entropy:[0.9476278  0.18726015 0.4574503  ... 0.1392385  0.32105458 0.7028982 ]\n",
      "VE Training loss:0.9663944840431213\n",
      "check (1104,) ()\n",
      "epoch 444\n",
      "====================================\n",
      "Epoch:  444 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1104.0\n",
      "Mean Reward of that batch 184.0\n",
      "Average Reward of all training: 179.2276947472397\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1104.0\n",
      "Training loss:-0.014089412987232208\n",
      "Cross Entropy:[0.66052353 1.2940444  0.12877716 ... 0.01792201 0.0255288  0.03718253]\n",
      "VE Training loss:0.9419082403182983\n",
      "check (1200,) ()\n",
      "epoch 445\n",
      "====================================\n",
      "Epoch:  445 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 179.27437408488635\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0034008510410785675\n",
      "Cross Entropy:[0.57640374 0.3812089  0.5196028  ... 0.20201275 0.5145377  1.1518507 ]\n",
      "VE Training loss:0.9819240570068359\n",
      "check (1200,) ()\n",
      "epoch 446\n",
      "====================================\n",
      "Epoch:  446 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 179.3208440981489\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0015471176011487842\n",
      "Cross Entropy:[0.6355247  1.2740533  0.13400406 ... 0.09586502 1.7248843  0.05881898]\n",
      "VE Training loss:0.9824292063713074\n",
      "check (1200,) ()\n",
      "epoch 447\n",
      "====================================\n",
      "Epoch:  447 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 179.3671061918891\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0006725430721417069\n",
      "Cross Entropy:[0.691255   0.2926071  0.7074538  ... 0.24898215 0.76419985 1.4672939 ]\n",
      "VE Training loss:0.9411725997924805\n",
      "check (1200,) ()\n",
      "epoch 448\n",
      "====================================\n",
      "Epoch:  448 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 179.41316175842505\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.010024345479905605\n",
      "Cross Entropy:[0.43939358 0.49484742 0.3963855  ... 0.25801033 0.52372086 0.45336133]\n",
      "VE Training loss:0.9534192681312561\n",
      "check (1200,) ()\n",
      "epoch 449\n",
      "====================================\n",
      "Epoch:  449 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 179.45901217767133\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.007448045536875725\n",
      "Cross Entropy:[0.76276743 0.24139467 0.54266435 ... 0.16404735 0.3714411  0.60338295]\n",
      "VE Training loss:0.9450755715370178\n",
      "check (1200,) ()\n",
      "epoch 450\n",
      "====================================\n",
      "Epoch:  450 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 179.5046588172765\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0036009217146784067\n",
      "Cross Entropy:[0.893683   1.659392   0.07956032 ... 0.74380106 1.4371964  0.0997379 ]\n",
      "VE Training loss:0.9446485638618469\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 451\n",
      "====================================\n",
      "Epoch:  451 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 179.55010303275924\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0038103186525404453\n",
      "Cross Entropy:[0.37618086 0.5806478  1.2616937  ... 1.6799862  0.08955842 0.18994254]\n",
      "VE Training loss:0.9695838689804077\n",
      "check (1200,) ()\n",
      "epoch 452\n",
      "====================================\n",
      "Epoch:  452 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 179.59534616764253\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.001801233272999525\n",
      "Cross Entropy:[0.40212056 0.86679876 0.23986545 ... 1.2684023  2.138996   0.04578289]\n",
      "VE Training loss:0.934647798538208\n",
      "check (1200,) ()\n",
      "epoch 453\n",
      "====================================\n",
      "Epoch:  453 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 179.64038955358592\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.006341472268104553\n",
      "Cross Entropy:[0.61088353 0.3421695  0.8226943  ... 0.23367134 0.8928207  0.18550062]\n",
      "VE Training loss:0.9222960472106934\n",
      "check (1200,) ()\n",
      "epoch 454\n",
      "====================================\n",
      "Epoch:  454 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 179.68523451051635\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.011541738174855709\n",
      "Cross Entropy:[0.97499317 0.1761766  0.41483054 ... 0.43695614 0.46786362 0.4561581 ]\n",
      "VE Training loss:0.9548466205596924\n",
      "check (1200,) ()\n",
      "epoch 455\n",
      "====================================\n",
      "Epoch:  455 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 179.72988234675697\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.002250456716865301\n",
      "Cross Entropy:[0.533471   1.1359829  1.937226   ... 0.74741435 0.2686416  0.8116801 ]\n",
      "VE Training loss:0.9493061304092407\n",
      "check (1200,) ()\n",
      "epoch 456\n",
      "====================================\n",
      "Epoch:  456 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 179.77433435915444\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.011331497691571712\n",
      "Cross Entropy:[0.8314026  0.22655341 0.869197   ... 0.5907112  0.35318685 0.57876587]\n",
      "VE Training loss:0.9451611042022705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1200,) ()\n",
      "epoch 457\n",
      "====================================\n",
      "Epoch:  457 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 179.8185918332044\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0030764976982027292\n",
      "Cross Entropy:[0.39279258 0.55666435 0.35214978 ... 0.14687523 1.161823   0.15596227]\n",
      "VE Training loss:0.9407721161842346\n",
      "check (1200,) ()\n",
      "epoch 458\n",
      "====================================\n",
      "Epoch:  458 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 179.86265604317558\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.005504713859409094\n",
      "Cross Entropy:[0.56316453 1.135895   0.16478261 ... 0.11216615 0.21921328 0.41874686]\n",
      "VE Training loss:0.9326401948928833\n",
      "check (1200,) ()\n",
      "epoch 459\n",
      "====================================\n",
      "Epoch:  459 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 179.90652825223185\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0006847762851975858\n",
      "Cross Entropy:[0.5376655  0.4132266  0.47944552 ... 0.28742993 0.71654934 0.2789956 ]\n",
      "VE Training loss:0.9498291015625\n",
      "check (1200,) ()\n",
      "epoch 460\n",
      "====================================\n",
      "Epoch:  460 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 179.9502097125531\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0019511246355250478\n",
      "Cross Entropy:[0.74973786 1.4545314  0.10188816 ... 0.36931187 0.5862055  0.33870262]\n",
      "VE Training loss:0.9256445169448853\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 461\n",
      "====================================\n",
      "Epoch:  461 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 179.99370166545427\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.00534697063267231\n",
      "Cross Entropy:[0.38928917 0.5441993  0.3736836  ... 0.16561744 0.38853815 0.5450574 ]\n",
      "VE Training loss:0.9884126782417297\n",
      "check (1175,) ()\n",
      "epoch 462\n",
      "====================================\n",
      "Epoch:  462 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1175.0\n",
      "Mean Reward of that batch 195.83333333333334\n",
      "Average Reward of all training: 180.02798658248435\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1175.0\n",
      "Training loss:-0.004767644219100475\n",
      "Cross Entropy:[0.37963748 0.5712578  1.2380918  ... 0.46070296 1.0543668  0.15112206]\n",
      "VE Training loss:0.955814003944397\n",
      "check (1200,) ()\n",
      "epoch 463\n",
      "====================================\n",
      "Epoch:  463 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 180.07112268057833\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.008090718649327755\n",
      "Cross Entropy:[0.7010212  0.29303476 0.73104775 ... 0.5481767  1.1586794  0.15023719]\n",
      "VE Training loss:1.0122100114822388\n",
      "check (1200,) ()\n",
      "epoch 464\n",
      "====================================\n",
      "Epoch:  464 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 180.114072847215\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0030498155392706394\n",
      "Cross Entropy:[0.60575056 0.31780806 0.7247646  ... 1.3579279  0.09711935 0.19514993]\n",
      "VE Training loss:0.9829313158988953\n",
      "check (1200,) ()\n",
      "epoch 465\n",
      "====================================\n",
      "Epoch:  465 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 180.1568382819522\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0017968241591006517\n",
      "Cross Entropy:[0.58381814 1.2693005  0.12310624 ... 0.5614574  0.3712819  0.5506046 ]\n",
      "VE Training loss:0.9516128301620483\n",
      "check (1200,) ()\n",
      "epoch 466\n",
      "====================================\n",
      "Epoch:  466 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 180.199420174051\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.009275984950363636\n",
      "Cross Entropy:[1.1378186  0.13334294 0.31943753 ... 0.19011062 0.4778888  0.4271633 ]\n",
      "VE Training loss:0.97811359167099\n",
      "check (1200,) ()\n",
      "epoch 467\n",
      "====================================\n",
      "Epoch:  467 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 180.2418197025862\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.008595853112637997\n",
      "Cross Entropy:[0.781269   0.22136126 0.5129102  ... 1.3723515  0.08681327 0.18889825]\n",
      "VE Training loss:0.9792155027389526\n",
      "check (1200,) ()\n",
      "epoch 468\n",
      "====================================\n",
      "Epoch:  468 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 180.28403803655505\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.009586079977452755\n",
      "Cross Entropy:[0.81124854 1.6064261  0.07914321 ... 0.25085202 0.49839467 0.48231894]\n",
      "VE Training loss:0.9673791527748108\n",
      "check (1200,) ()\n",
      "epoch 469\n",
      "====================================\n",
      "Epoch:  469 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 180.32607633498458\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.011904044076800346\n",
      "Cross Entropy:[0.33271432 0.62032837 0.2953614  ... 0.16479333 0.39213425 0.56038415]\n",
      "VE Training loss:0.9781650900840759\n",
      "check (1200,) ()\n",
      "epoch 470\n",
      "====================================\n",
      "Epoch:  470 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 180.3679357470378\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.004357691388577223\n",
      "Cross Entropy:[0.95380753 0.17724982 1.0296203  ... 0.18520944 0.40248242 0.57685983]\n",
      "VE Training loss:0.9824751615524292\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 471\n",
      "====================================\n",
      "Epoch:  471 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 180.4096174121184\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.004848279990255833\n",
      "Cross Entropy:[0.7323216  0.25246456 0.7939225  ... 0.32585618 0.7808353  0.23846932]\n",
      "VE Training loss:0.9385512471199036\n",
      "check (1200,) ()\n",
      "epoch 472\n",
      "====================================\n",
      "Epoch:  472 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 180.45112245997407\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.005993973463773727\n",
      "Cross Entropy:[0.96113014 0.1787079  0.44817793 ... 0.6000219  0.3091545  0.6842124 ]\n",
      "VE Training loss:0.9208549857139587\n",
      "check (1200,) ()\n",
      "epoch 473\n",
      "====================================\n",
      "Epoch:  473 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 180.49245201079864\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.005039169918745756\n",
      "Cross Entropy:[0.71172285 0.26662886 0.64420074 ... 0.73151195 0.28957707 0.6672318 ]\n",
      "VE Training loss:1.0129940509796143\n",
      "check (1200,) ()\n",
      "epoch 474\n",
      "====================================\n",
      "Epoch:  474 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 180.53360717533283\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.005688242614269257\n",
      "Cross Entropy:[0.6749107  1.4490733  0.09485363 ... 0.541764   0.36149272 0.5562108 ]\n",
      "VE Training loss:0.9748374223709106\n",
      "check (1200,) ()\n",
      "epoch 475\n",
      "====================================\n",
      "Epoch:  475 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 180.57458905496372\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.003630115883424878\n",
      "Cross Entropy:[0.5015719  0.38386354 0.5480007  ... 1.046608   0.15225127 0.32320982]\n",
      "VE Training loss:0.91525799036026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1200,) ()\n",
      "epoch 476\n",
      "====================================\n",
      "Epoch:  476 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 180.61539874182304\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.011585857719182968\n",
      "Cross Entropy:[0.8247249  0.22709748 0.5689227  ... 0.18191074 0.38635945 0.6174105 ]\n",
      "VE Training loss:0.924801230430603\n",
      "check (1168,) ()\n",
      "epoch 477\n",
      "====================================\n",
      "Epoch:  477 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1168.0\n",
      "Mean Reward of that batch 194.66666666666666\n",
      "Average Reward of all training: 180.6448563265711\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1168.0\n",
      "Training loss:-0.0004931018920615315\n",
      "Cross Entropy:[0.8502166  0.21590006 0.54750115 ... 2.5569952  0.02396497 0.03710729]\n",
      "VE Training loss:0.9765118956565857\n",
      "check (1200,) ()\n",
      "epoch 478\n",
      "====================================\n",
      "Epoch:  478 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 180.6853482589423\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.007512424141168594\n",
      "Cross Entropy:[0.64972705 0.30145526 0.73364586 ... 1.547006   0.08557887 0.19303437]\n",
      "VE Training loss:1.0231883525848389\n",
      "check (1200,) ()\n",
      "epoch 479\n",
      "====================================\n",
      "Epoch:  479 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 180.72567112270232\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0005643840413540602\n",
      "Cross Entropy:[0.692517   1.4246521  0.0995428  ... 0.2517892  0.78798944 0.23230366]\n",
      "VE Training loss:0.9767535328865051\n",
      "check (1200,) ()\n",
      "epoch 480\n",
      "====================================\n",
      "Epoch:  480 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 180.76582597453003\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0054983459413051605\n",
      "Cross Entropy:[0.9461701  0.18418756 0.98444366 ... 0.05465411 0.14398853 0.36514387]\n",
      "VE Training loss:0.9685103297233582\n",
      "Model saved\n",
      "check (1121,) ()\n",
      "epoch 481\n",
      "====================================\n",
      "Epoch:  481 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1121.0\n",
      "Mean Reward of that batch 186.83333333333334\n",
      "Average Reward of all training: 180.77844033494338\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1121.0\n",
      "Training loss:-0.0012888329802080989\n",
      "Cross Entropy:[0.7242482  0.2592     0.73599476 ... 0.42897797 0.9758211  0.16873513]\n",
      "VE Training loss:0.9324958920478821\n",
      "check (1200,) ()\n",
      "epoch 482\n",
      "====================================\n",
      "Epoch:  482 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 180.8183190894352\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.006770188920199871\n",
      "Cross Entropy:[0.6814383  1.398039   0.10398461 ... 0.2592847  0.6259854  1.3126719 ]\n",
      "VE Training loss:0.9904914498329163\n",
      "check (1200,) ()\n",
      "epoch 483\n",
      "====================================\n",
      "Epoch:  483 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 180.85803271450882\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.001629832200706005\n",
      "Cross Entropy:[0.71316737 0.28074974 0.6500396  ... 0.8490586  0.21263695 0.5166057 ]\n",
      "VE Training loss:0.9299644827842712\n",
      "check (1200,) ()\n",
      "epoch 484\n",
      "====================================\n",
      "Epoch:  484 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 180.89758223369373\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.006716147996485233\n",
      "Cross Entropy:[0.9408306  0.1704711  1.0839205  ... 0.73327446 0.27361095 0.7245664 ]\n",
      "VE Training loss:0.9713109135627747\n",
      "check (1200,) ()\n",
      "epoch 485\n",
      "====================================\n",
      "Epoch:  485 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 180.93696866207787\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.004805825650691986\n",
      "Cross Entropy:[0.41786578 0.49943322 0.3694428  ... 0.30856395 0.7093309  0.2647325 ]\n",
      "VE Training loss:0.9414740800857544\n",
      "check (1200,) ()\n",
      "epoch 486\n",
      "====================================\n",
      "Epoch:  486 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 180.97619300639457\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.009711106307804585\n",
      "Cross Entropy:[0.989946   0.16019912 0.39527792 ... 0.0579745  0.12967598 0.3016079 ]\n",
      "VE Training loss:0.9724205732345581\n",
      "check (1200,) ()\n",
      "epoch 487\n",
      "====================================\n",
      "Epoch:  487 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 181.01525626510835\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.011547870934009552\n",
      "Cross Entropy:[0.7670359  0.22935212 0.84698343 ... 1.2806736  0.11478836 0.29136264]\n",
      "VE Training loss:1.0385093688964844\n",
      "check (1200,) ()\n",
      "epoch 488\n",
      "====================================\n",
      "Epoch:  488 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 181.0541594284995\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.00356342107988894\n",
      "Cross Entropy:[0.6342482  0.32103226 0.5649071  ... 1.3315483  0.0978884  0.23062313]\n",
      "VE Training loss:0.9771516919136047\n",
      "check (1200,) ()\n",
      "epoch 489\n",
      "====================================\n",
      "Epoch:  489 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 181.09290347874798\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.00031960965134203434\n",
      "Cross Entropy:[0.6688361  0.28074667 0.6976569  ... 0.09997577 0.25044703 0.80208915]\n",
      "VE Training loss:0.9776153564453125\n",
      "check (1200,) ()\n",
      "epoch 490\n",
      "====================================\n",
      "Epoch:  490 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 181.13148939001584\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.008639969862997532\n",
      "Cross Entropy:[0.94711685 0.17402868 1.0511997  ... 0.26733673 0.6964126  1.4641438 ]\n",
      "VE Training loss:0.972659170627594\n",
      "Model saved\n",
      "check (1200,) ()\n",
      "epoch 491\n",
      "====================================\n",
      "Epoch:  491 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 181.16991812852905\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0051455190405249596\n",
      "Cross Entropy:[0.56403    1.1992992  0.14068024 ... 0.5398699  0.36128792 0.5509122 ]\n",
      "VE Training loss:1.021543025970459\n",
      "check (1200,) ()\n",
      "epoch 492\n",
      "====================================\n",
      "Epoch:  492 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 181.20819065265806\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.002786105964332819\n",
      "Cross Entropy:[0.60821086 0.3146493  0.7482452  ... 0.5209628  0.39395776 0.5194552 ]\n",
      "VE Training loss:1.0106748342514038\n",
      "check (1200,) ()\n",
      "epoch 493\n",
      "====================================\n",
      "Epoch:  493 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 181.24630791299748\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0032244285102933645\n",
      "Cross Entropy:[0.4030929  0.5103868  0.37805262 ... 0.6505209  0.34419635 0.87741655]\n",
      "VE Training loss:0.9700868725776672\n",
      "check (1200,) ()\n",
      "epoch 494\n",
      "====================================\n",
      "Epoch:  494 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 181.28427085244488\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:-0.0030738688074052334\n",
      "Cross Entropy:[0.77422124 1.5037416  0.09809765 ... 0.26595715 0.61320823 1.2514112 ]\n",
      "VE Training loss:1.0305935144424438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1200,) ()\n",
      "epoch 495\n",
      "====================================\n",
      "Epoch:  495 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 181.3220804062783\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.0039510042406618595\n",
      "Cross Entropy:[0.6684302  0.28378874 0.7311922  ... 0.07962484 0.14701273 0.27921763]\n",
      "VE Training loss:1.020334243774414\n",
      "check (1200,) ()\n",
      "epoch 496\n",
      "====================================\n",
      "Epoch:  496 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 181.3597375022334\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.003392260055989027\n",
      "Cross Entropy:[0.6695649  0.30122763 0.6561282  ... 0.4801795  0.38902536 0.82368636]\n",
      "VE Training loss:0.9883935451507568\n",
      "check (1200,) ()\n",
      "epoch 497\n",
      "====================================\n",
      "Epoch:  497 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 181.397243060579\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.006957101635634899\n",
      "Cross Entropy:[0.79989636 0.23865187 0.5749536  ... 0.20077895 0.49181056 0.41141087]\n",
      "VE Training loss:1.0574685335159302\n",
      "check (1200,) ()\n",
      "epoch 498\n",
      "====================================\n",
      "Epoch:  498 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 181.4345979941923\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.005966764874756336\n",
      "Cross Entropy:[0.65207255 0.29160535 0.7409874  ... 0.7456589  0.2761039  0.65041196]\n",
      "VE Training loss:1.0097728967666626\n",
      "check (1200,) ()\n",
      "epoch 499\n",
      "====================================\n",
      "Epoch:  499 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 181.4718032086328\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.006717161275446415\n",
      "Cross Entropy:[0.8230421  0.22536582 0.5046667  ... 0.92197806 0.2209582  0.5150602 ]\n",
      "VE Training loss:1.0368727445602417\n",
      "check (1200,) ()\n",
      "epoch 500\n",
      "====================================\n",
      "Epoch:  500 / 500\n",
      "------------\n",
      "Number of training episodes: 6\n",
      "Total reward:1200.0\n",
      "Mean Reward of that batch 200.0\n",
      "Average Reward of all training: 181.50885960221552\n",
      "Max reward for a batch so far: 1200.0\n",
      "check 1200.0\n",
      "Training loss:0.011496528051793575\n",
      "Cross Entropy:[0.63376933 1.276313   0.13257876 ... 0.57829    1.2099187  0.13245367]\n",
      "VE Training loss:1.0354831218719482\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# training and print sth\n",
    "allRewards = []\n",
    "\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "mean_reward_total = []\n",
    "average_reward = []\n",
    "epoch = 1\n",
    "# for plotting\n",
    "epoch_1 = []\n",
    "average_reward = []\n",
    "saver = tf.train.Saver()\n",
    "# while we have epoch/episode to train\n",
    "\n",
    "if training:\n",
    "    # number of iterations\n",
    "    while epoch < number_epoch +1:\n",
    "        states_mb, actions_mb, rewards_of_batch, discounted_rewards_mb, nb_episodes_mb = make_batch(batch_size)\n",
    "   \n",
    "        total_reward_of_that_batch = np.sum(rewards_of_batch)\n",
    "        print('check',rewards_of_batch.shape, np.array(total_reward_of_that_batch).shape)\n",
    "        allRewards.append(total_reward_of_that_batch)\n",
    "        \n",
    "        mean_reward_of_that_batch = np.divide(total_reward_of_that_batch, nb_episodes_mb)\n",
    "        mean_reward_total.append(mean_reward_of_that_batch)\n",
    "        average_reward_of_all_training = np.divide(np.sum(mean_reward_total), epoch)\n",
    "        maximumRewardRecorded = np.amax(allRewards)\n",
    "        print('epoch',epoch)\n",
    "        epoch_1.append(epoch)\n",
    "        print('====================================')\n",
    "        print(\"Epoch: \", epoch, '/', number_epoch)\n",
    "        print('------------')\n",
    "        print(\"Number of training episodes: {}\".format(nb_episodes_mb))\n",
    "        print(\"Total reward:{}\".format(total_reward_of_that_batch, nb_episodes_mb))\n",
    "        print(\"Mean Reward of that batch {}\".format(mean_reward_of_that_batch))\n",
    "        print(\"Average Reward of all training: {}\".format(average_reward_of_all_training))\n",
    "        print(\"Max reward for a batch so far: {}\".format(maximumRewardRecorded))\n",
    "\n",
    "        loss_, cross, _= sess.run([PGN.loss, PGN.cross_entropy, PGN.train_opt],feed_dict = {PGN.inputs: states_mb.reshape([len(states_mb), state_size]), PGN.actions: actions_mb,PGN.discounted_episode_rewards: discounted_rewards_mb})    \n",
    "        loss_VE, _= sess.run([VEN.loss, VEN.train_opt],feed_dict = {VEN.inputs: states_mb.reshape([len(states_mb), state_size]), VEN.discounted_episode_rewards: discounted_rewards_mb})    \n",
    "\n",
    "        print('check',total_reward_of_that_batch)\n",
    "        print('Training loss:{}'.format(loss_) )\n",
    "        print('Cross Entropy:{}'.format(cross) )\n",
    "        print('VE Training loss:{}'.format(loss_VE) )\n",
    "              \n",
    "        if epoch % 10 == 0:\n",
    "              saver.save(sess, \"./models/model.ckpt\")\n",
    "              print('Model saved')\n",
    "        epoch += 1\n",
    "        average_reward.append(mean_reward_of_that_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'average_reward_each_episode')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f991041a438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(epoch_1, average_reward)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('average_reward_each_episode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pong game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2, 1: 3}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the environment of the game\n",
    "import gym\n",
    "env = gym.make('Pong-v0')\n",
    "\n",
    "env.action_space.n\n",
    "env.unwrapped.get_action_meanings()\n",
    "\n",
    "# our agents only care about left and right move\n",
    "Right_ACTION = 2\n",
    "Left_ACTION = 3\n",
    "action_dict = { 0:Right_ACTION, 1:Left_ACTION}\n",
    "action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# set up the training hyperparameters\n",
    "\n",
    "state_size = [80,80,1] # our input an image\n",
    "action_size = 2  # number of actions (push to left or push to right)\n",
    "#action_size = 1  # number of actions (push to left or push to right)\n",
    "# training hyperparameters\n",
    "learning_rate = 0.0001\n",
    "\n",
    "number_epoch = 10000 # number of epochs for training\n",
    "batch_size = 100 # defines number of samples work though\n",
    "hidden_size_1 = 53\n",
    "hidden_size_2 = 34\n",
    "\n",
    "training = True \n",
    "\n",
    "\n",
    "max_steps = 200 # Max steps per episode\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the discounted some of reward from current step onward\n",
    "def discount_rewards(r, gamma = 0.99, constant_baseline = False):\n",
    "    discounted_r = np.zeros_like(r) #make a vector of zeros with the size of input\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, len(r))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "        \n",
    "    if constant_baseline: # do normalization for reward to have more smooth gradient\n",
    "        mean = np.mean(discounted_r)\n",
    "        std = np.std(discounted_r)\n",
    "        discounted_r = (discounted_r - mean)/(std)\n",
    "        \n",
    "    return discounted_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array \"\"\"\n",
    "    image = image[35:195] # crop\n",
    "    image = image[::2,::2,0] # downsample by factor of 2\n",
    "    image[image == 144] = 0 # erase background (background type 1)\n",
    "    image[image == 109] = 0 # erase background (background type 2)\n",
    "    image[image != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    \n",
    "    return np.reshape(image.astype(np.float).ravel(), [80,80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the policy network \n",
    "\n",
    "class PGNetwork():\n",
    "    def __init__(self, state_size, action_size, learning_rate, hidden_size_1, hidden_size_2, name = 'PGNetwork'):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        #self.hidden_size_1 = hidden_size_1\n",
    "        self.hidden_size_1 = 53\n",
    "        self.hidden_size_2 = 34\n",
    "        \n",
    "        # generate a network such that with a given state, the policy gives an action\n",
    "        with tf.variable_scope(name):\n",
    "            with tf.name_scope('inputs'):\n",
    "            # we create placeholder\n",
    "                self.inputs = tf.placeholder(tf.float32, shape = [None, *state_size], name = 'inputs')\n",
    "                self.actions = tf.placeholder(tf.int32, shape = [None, action_size], name = 'actions')\n",
    "                #self.actions = tf.placeholder(tf.float32, shape = [None, action_size], name = 'actions')\n",
    "                self.discounted_episode_rewards = tf.placeholder(tf.float32, shape = [None, ], name = 'discounted_episode_rewards')\n",
    "            # CNN is often used for image process\n",
    "            #with tf.name_scope('conv_layer1'):\n",
    "              \n",
    "                # filters gives the number of filters in the convolution nn\n",
    "            #    self.conv1 = tf.layers.conv2d(inputs = self.inputs,\n",
    "            #                    filters = 32,\n",
    "            #                    kernel_size = [3,3],\n",
    "            #                    strides = [1,1],\n",
    "            #                    padding = 'VALID',\n",
    "            #                    kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(), name = 'conv1')\n",
    "                \n",
    "            #    self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1, training = True,\n",
    "            #                                                epsilon = 1e-5, name = 'batch_norm1')\n",
    "            #    \n",
    "            #    self.conv1_out = tf.nn.relu(self.conv1_batchnorm, name = 'conv1_out')\n",
    "            \n",
    "                \n",
    "            with tf.name_scope('layer1'):\n",
    "                # filters gives the number of filters in the hidden nn\n",
    "                #self.flatten = tf.contrib.layers.flatten(self.conv1_out)\n",
    "                self.flatten = tf.contrib.layers.flatten(self.inputs)\n",
    "                self.layer1 = tf.contrib.layers.fully_connected(inputs = self.flatten,\n",
    "                                             num_outputs = self.hidden_size_1,\n",
    "                                             activation_fn = tf.nn.elu,\n",
    "                                             weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            with tf.name_scope('layer2'):\n",
    "                # filters gives the number of filters in the hidden nn\n",
    "                self.layer2 = tf.contrib.layers.fully_connected(inputs = self.layer1,\n",
    "                                             num_outputs = self.hidden_size_2,\n",
    "                                             activation_fn = tf.nn.elu,\n",
    "                                             weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "                                       \n",
    "                                          \n",
    "            with tf.name_scope('logits'):\n",
    "                # get the action distribution from the fully connected NN\n",
    "                self.logits = tf.layers.dense(inputs = self.layer2,\n",
    "                                             kernel_initializer = tf.contrib.layers.xavier_initializer(),      \n",
    "                                             units = self.action_size, \n",
    "                                             # units = 1, \n",
    "                                             activation = None)\n",
    "            \n",
    "               \n",
    "                #self.out = tf.sigmoid(self.logits, name=\"sigmoid\")\n",
    "                \n",
    "            with tf.name_scope('softmax'):\n",
    "                self.action_distribution = tf.nn.softmax(self.logits)\n",
    "                \n",
    "            # define the loss function\n",
    "            with tf.name_scope('loss'):\n",
    "                #self.cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits( labels = self.actions,logits = self.logits)\n",
    "                self.cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits = self.logits, labels = self.actions)\n",
    "                #self.log_action_probability = tf.nn.softmax_cross_entropy_with_logits(logits = self.outputlayer, labels = self.actions)\n",
    "                self.weighted_likelihoods = tf.multiply(self.cross_entropy, self.discounted_episode_rewards)\n",
    "                self.loss = tf.reduce_mean(self.weighted_likelihoods)\n",
    "\n",
    "            with tf.name_scope('train'):\n",
    "                #self.optimizer = tf.train.RMSPropOptimizer(1e-3, decay = 0.99)\n",
    "                self.optimizer = tf.train.AdamOptimizer(self.learning_rate,  beta1=0.9, beta2=0.99)\n",
    "                self.train_opt = self.optimizer.minimize(self.loss)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value estimator network\n",
    "class VENetwork():\n",
    "    def __init__(self, state_size, learning_rate,  name = 'VENetwork'):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.output_size = 1\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_size_1 = 64\n",
    "        #self.hidden_size_2 = hidden_size_2\n",
    "     \n",
    "        \n",
    "        # generate a network such that with a given state, the policy gives an action\n",
    "        with tf.variable_scope(name):\n",
    "            with tf.name_scope('inputs'):\n",
    "            # we create placeholder\n",
    "                self.inputs = tf.placeholder(tf.float32, shape = [None, *state_size], name = 'inputs')\n",
    "                self.discounted_episode_rewards = tf.placeholder(tf.float32, shape = [None, ], name = 'discounted_episode_rewards')\n",
    "                \n",
    "            #  we use general fully connected layers\n",
    "            with tf.name_scope('layer1'):\n",
    "              \n",
    "                self.inputs1 = tf.contrib.layers.flatten(self.inputs)\n",
    "                # filters gives the number of filters in the convolution nn\n",
    "                self.layer1 = tf.contrib.layers.fully_connected(inputs = self.inputs1,\n",
    "                                             num_outputs = self.hidden_size_1,\n",
    "                                             activation_fn = tf.nn.elu,\n",
    "                                             weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "                                        \n",
    "            with tf.name_scope('output'):\n",
    "                # get the action distribution from the fully connected NN\n",
    "                self.output_layer = tf.layers.dense(inputs = self.layer1,\n",
    "                                             kernel_initializer = tf.contrib.layers.xavier_initializer(),      \n",
    "                                             units = self.output_size, \n",
    "                                             activation = None)\n",
    "                \n",
    "                self.state_value_estimation = tf.squeeze(self.output_layer)\n",
    "                \n",
    "            # define the loss function\n",
    "            with tf.name_scope('loss'):\n",
    "\n",
    "                self.loss = tf.reduce_mean(tf.squared_difference(self.state_value_estimation, self.discounted_episode_rewards))\n",
    "                \n",
    "           \n",
    "                \n",
    "            with tf.name_scope('train'):\n",
    "                #self.optimizer = tf.train.RMSPropOptimizer(1e-3, decay = 0.99)\n",
    "                self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "                self.train_opt = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the environment \n",
    "# initialize network and session\n",
    "tf.reset_default_graph()\n",
    "PGN = PGNetwork(state_size, action_size, learning_rate,hidden_size_1, hidden_size_2)\n",
    "VEN = VENetwork(state_size, learning_rate)\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the policy until it reached maximum batch number and outputs information of each step (batch number)\n",
    "# for each episode\n",
    "def make_batch(batch_size):\n",
    "    states, actions, rewards_of_episode, rewards_of_batch, rewards_of_episode0, discounted_rewards = [],[],[], [], [],[]\n",
    "    \n",
    "    # keep track of how many episodes in our batch (useful when we need to calculate the average reward)\n",
    "    episode_num = 1\n",
    "    \n",
    "    # get a new state\n",
    "    state = env.reset()\n",
    "    #state = preprocess(state)\n",
    "    while True:\n",
    "        state = preprocess(state)\n",
    "        action_probability_distribution = sess.run(PGN.action_distribution, feed_dict = {PGN.inputs: state.reshape(1,*state_size)})\n",
    "        #action_probability = sess.run(PGN.out, feed_dict = {PGN.inputs: state.reshape(1,*state_size)})\n",
    "        #state_value_estimation = sess.run(VEN.state_value_estimation, feed_dict = {VEN.inputs: state.reshape(1,*state_size)})\n",
    "        #action_probability = action_probability[0][0]\n",
    "                                                                                      \n",
    "        action = np.random.choice(range(action_probability_distribution.shape[1]), p = action_probability_distribution.ravel())\n",
    "        #action = np.random.choice(range(2,4), p = [1-action_probability, action_probability])\n",
    "        action_ = [0,0]\n",
    "        action_[action] = 1\n",
    "        \n",
    "        action = action_dict[action]\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action_)\n",
    "        rewards_of_episode.append(reward)\n",
    "        #rewards_of_episode0.append(reward-state_value_estimation)\n",
    "        \n",
    "        if done:\n",
    "            rewards_of_batch.append(rewards_of_episode)\n",
    "            discounted_rewards.append(discount_rewards(rewards_of_episode, gamma = 0.99, constant_baseline = True))\n",
    "            \n",
    "            if len(np.concatenate(rewards_of_batch)) > batch_size:\n",
    "                break\n",
    "                \n",
    "            rewards_of_episode = []\n",
    "            rewards_of_episode0 = []\n",
    "            episode_num +=1\n",
    "            \n",
    "            state = env.reset()\n",
    "        else:\n",
    "            state = next_state\n",
    "    return np.stack(np.array(states)), np.stack(np.array(actions)), np.concatenate(rewards_of_batch), np.concatenate(discounted_rewards), episode_num\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and print sth\n",
    "allRewards = []\n",
    "\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "mean_reward_total = []\n",
    "average_reward = []\n",
    "epoch = 1\n",
    "# for plotting\n",
    "epoch_1 = []\n",
    "average_reward = []\n",
    "saver = tf.train.Saver()\n",
    "# while we have epoch/episode to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1354,) ()\n",
      "epoch 1\n",
      "====================================\n",
      "Epoch:  1 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: -21.0\n",
      "check -21.0\n",
      "Training loss:-2.5571713194949552e-05\n",
      "check (1427,) ()\n",
      "epoch 2\n",
      "====================================\n",
      "Epoch:  2 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: -21.0\n",
      "check -21.0\n",
      "Training loss:0.002816486405208707\n",
      "check (1322,) ()\n",
      "epoch 3\n",
      "====================================\n",
      "Epoch:  3 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: -21.0\n",
      "check -21.0\n",
      "Training loss:0.0008049574098549783\n",
      "check (1516,) ()\n",
      "epoch 4\n",
      "====================================\n",
      "Epoch:  4 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.5\n",
      "Max reward for a batch so far: -19.0\n",
      "check -19.0\n",
      "Training loss:-0.0018892992520704865\n",
      "check (1591,) ()\n",
      "epoch 5\n",
      "====================================\n",
      "Epoch:  5 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.2\n",
      "Max reward for a batch so far: -19.0\n",
      "check -19.0\n",
      "Training loss:0.0036380344536155462\n",
      "check (1259,) ()\n",
      "epoch 6\n",
      "====================================\n",
      "Epoch:  6 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.333333333333332\n",
      "Max reward for a batch so far: -19.0\n",
      "check -21.0\n",
      "Training loss:0.0020090213511139154\n",
      "check (1335,) ()\n",
      "epoch 7\n",
      "====================================\n",
      "Epoch:  7 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.428571428571427\n",
      "Max reward for a batch so far: -19.0\n",
      "check -21.0\n",
      "Training loss:-0.0013063251972198486\n",
      "check (1343,) ()\n",
      "epoch 8\n",
      "====================================\n",
      "Epoch:  8 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.5\n",
      "Max reward for a batch so far: -19.0\n",
      "check -21.0\n",
      "Training loss:-0.0012753700139001012\n",
      "check (1479,) ()\n",
      "epoch 9\n",
      "====================================\n",
      "Epoch:  9 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.444444444444443\n",
      "Max reward for a batch so far: -19.0\n",
      "check -20.0\n",
      "Training loss:0.0011948604369536042\n",
      "check (1315,) ()\n",
      "epoch 10\n",
      "====================================\n",
      "Epoch:  10 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.4\n",
      "Max reward for a batch so far: -19.0\n",
      "check -20.0\n",
      "Training loss:-0.0002999773423653096\n",
      "check (1570,) ()\n",
      "epoch 11\n",
      "====================================\n",
      "Epoch:  11 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.454545454545453\n",
      "Max reward for a batch so far: -19.0\n",
      "check -21.0\n",
      "Training loss:-0.00286137405782938\n",
      "check (1171,) ()\n",
      "epoch 12\n",
      "====================================\n",
      "Epoch:  12 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.5\n",
      "Max reward for a batch so far: -19.0\n",
      "check -21.0\n",
      "Training loss:0.0013804571935907006\n",
      "check (1573,) ()\n",
      "epoch 13\n",
      "====================================\n",
      "Epoch:  13 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.46153846153846\n",
      "Max reward for a batch so far: -19.0\n",
      "check -20.0\n",
      "Training loss:-0.0017056716606020927\n",
      "check (1318,) ()\n",
      "epoch 14\n",
      "====================================\n",
      "Epoch:  14 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.428571428571427\n",
      "Max reward for a batch so far: -19.0\n",
      "check -20.0\n",
      "Training loss:-0.0003506815410219133\n",
      "check (1493,) ()\n",
      "epoch 15\n",
      "====================================\n",
      "Epoch:  15 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.466666666666665\n",
      "Max reward for a batch so far: -19.0\n",
      "check -21.0\n",
      "Training loss:0.00041170368785969913\n",
      "check (1250,) ()\n",
      "epoch 16\n",
      "====================================\n",
      "Epoch:  16 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.5\n",
      "Max reward for a batch so far: -19.0\n",
      "check -21.0\n",
      "Training loss:9.12780797079904e-06\n",
      "check (1472,) ()\n",
      "epoch 17\n",
      "====================================\n",
      "Epoch:  17 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.470588235294116\n",
      "Max reward for a batch so far: -19.0\n",
      "check -20.0\n",
      "Training loss:-0.0023373041767627\n",
      "check (1305,) ()\n",
      "epoch 18\n",
      "====================================\n",
      "Epoch:  18 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.444444444444443\n",
      "Max reward for a batch so far: -19.0\n",
      "check -20.0\n",
      "Training loss:0.0018941477173939347\n",
      "check (1456,) ()\n",
      "epoch 19\n",
      "====================================\n",
      "Epoch:  19 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.42105263157895\n",
      "Max reward for a batch so far: -19.0\n",
      "check -20.0\n",
      "Training loss:-0.002644389634951949\n",
      "check (1344,) ()\n",
      "epoch 20\n",
      "====================================\n",
      "Epoch:  20 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.45\n",
      "Max reward for a batch so far: -19.0\n",
      "check -21.0\n",
      "Training loss:-0.006280431989580393\n",
      "check (1430,) ()\n",
      "epoch 21\n",
      "====================================\n",
      "Epoch:  21 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.476190476190474\n",
      "Max reward for a batch so far: -19.0\n",
      "check -21.0\n",
      "Training loss:-0.0003454548423178494\n",
      "check (1334,) ()\n",
      "epoch 22\n",
      "====================================\n",
      "Epoch:  22 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.5\n",
      "Max reward for a batch so far: -19.0\n",
      "check -21.0\n",
      "Training loss:-0.0005286901723593473\n",
      "check (1171,) ()\n",
      "epoch 23\n",
      "====================================\n",
      "Epoch:  23 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.52173913043478\n",
      "Max reward for a batch so far: -19.0\n",
      "check -21.0\n",
      "Training loss:-0.0013712969375774264\n",
      "check (1094,) ()\n",
      "epoch 24\n",
      "====================================\n",
      "Epoch:  24 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.541666666666668\n",
      "Max reward for a batch so far: -19.0\n",
      "check -21.0\n",
      "Training loss:-0.0012478645658120513\n",
      "check (1259,) ()\n",
      "epoch 25\n",
      "====================================\n",
      "Epoch:  25 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.56\n",
      "Max reward for a batch so far: -19.0\n",
      "check -21.0\n",
      "Training loss:0.000859893043525517\n",
      "check (1333,) ()\n",
      "epoch 26\n",
      "====================================\n",
      "Epoch:  26 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.576923076923077\n",
      "Max reward for a batch so far: -19.0\n",
      "check -21.0\n",
      "Training loss:-0.0032040937803685665\n",
      "check (2167,) ()\n",
      "epoch 27\n",
      "====================================\n",
      "Epoch:  27 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-15.0\n",
      "Mean Reward of that batch -15.0\n",
      "Average Reward of all training: -20.37037037037037\n",
      "Max reward for a batch so far: -15.0\n",
      "check -15.0\n",
      "Training loss:0.002491165418177843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1349,) ()\n",
      "epoch 28\n",
      "====================================\n",
      "Epoch:  28 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.392857142857142\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.0015200593043118715\n",
      "check (1589,) ()\n",
      "epoch 29\n",
      "====================================\n",
      "Epoch:  29 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.413793103448278\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.00011994090164080262\n",
      "check (1191,) ()\n",
      "epoch 30\n",
      "====================================\n",
      "Epoch:  30 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.433333333333334\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.0032211279030889273\n",
      "check (1275,) ()\n",
      "epoch 31\n",
      "====================================\n",
      "Epoch:  31 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.451612903225808\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.00118688540533185\n",
      "check (1624,) ()\n",
      "epoch 32\n",
      "====================================\n",
      "Epoch:  32 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.4375\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.0021649652626365423\n",
      "check (1396,) ()\n",
      "epoch 33\n",
      "====================================\n",
      "Epoch:  33 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.424242424242426\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.0022955064196139574\n",
      "check (1922,) ()\n",
      "epoch 34\n",
      "====================================\n",
      "Epoch:  34 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.38235294117647\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.0017879517981782556\n",
      "check (1457,) ()\n",
      "epoch 35\n",
      "====================================\n",
      "Epoch:  35 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.37142857142857\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.005553932394832373\n",
      "check (1709,) ()\n",
      "epoch 36\n",
      "====================================\n",
      "Epoch:  36 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.36111111111111\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.00024131698592100292\n",
      "check (1265,) ()\n",
      "epoch 37\n",
      "====================================\n",
      "Epoch:  37 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.37837837837838\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:3.128504249616526e-05\n",
      "check (1395,) ()\n",
      "epoch 38\n",
      "====================================\n",
      "Epoch:  38 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.36842105263158\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.0032211358193308115\n",
      "check (1265,) ()\n",
      "epoch 39\n",
      "====================================\n",
      "Epoch:  39 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.384615384615383\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.002188300248235464\n",
      "check (1435,) ()\n",
      "epoch 40\n",
      "====================================\n",
      "Epoch:  40 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.35\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.00849124789237976\n",
      "check (1897,) ()\n",
      "epoch 41\n",
      "====================================\n",
      "Epoch:  41 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -20.29268292682927\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.004783639218658209\n",
      "check (1352,) ()\n",
      "epoch 42\n",
      "====================================\n",
      "Epoch:  42 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.30952380952381\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.0006285926792770624\n",
      "check (1958,) ()\n",
      "epoch 43\n",
      "====================================\n",
      "Epoch:  43 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -20.25581395348837\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.00018940614245366305\n",
      "check (1241,) ()\n",
      "epoch 44\n",
      "====================================\n",
      "Epoch:  44 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.272727272727273\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.009281612932682037\n",
      "check (1366,) ()\n",
      "epoch 45\n",
      "====================================\n",
      "Epoch:  45 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.244444444444444\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.0006010158685967326\n",
      "check (1415,) ()\n",
      "epoch 46\n",
      "====================================\n",
      "Epoch:  46 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.26086956521739\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.005825509317219257\n",
      "check (1423,) ()\n",
      "epoch 47\n",
      "====================================\n",
      "Epoch:  47 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.27659574468085\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.006415159907191992\n",
      "check (1412,) ()\n",
      "epoch 48\n",
      "====================================\n",
      "Epoch:  48 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.291666666666668\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.006268855184316635\n",
      "check (1646,) ()\n",
      "epoch 49\n",
      "====================================\n",
      "Epoch:  49 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.285714285714285\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.0009361860575154424\n",
      "check (1591,) ()\n",
      "epoch 50\n",
      "====================================\n",
      "Epoch:  50 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.3\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.0039671557024121284\n",
      "Model saved\n",
      "check (1466,) ()\n",
      "epoch 51\n",
      "====================================\n",
      "Epoch:  51 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.294117647058822\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.0036848050076514482\n",
      "check (1676,) ()\n",
      "epoch 52\n",
      "====================================\n",
      "Epoch:  52 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.26923076923077\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:0.00445285439491272\n",
      "check (1473,) ()\n",
      "epoch 53\n",
      "====================================\n",
      "Epoch:  53 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.28301886792453\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.0013540155487135053\n",
      "check (1374,) ()\n",
      "epoch 54\n",
      "====================================\n",
      "Epoch:  54 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.27777777777778\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.006356195546686649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (2147,) ()\n",
      "epoch 55\n",
      "====================================\n",
      "Epoch:  55 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.272727272727273\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.0017003279644995928\n",
      "check (1428,) ()\n",
      "epoch 56\n",
      "====================================\n",
      "Epoch:  56 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.285714285714285\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.003849796252325177\n",
      "check (1492,) ()\n",
      "epoch 57\n",
      "====================================\n",
      "Epoch:  57 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.29824561403509\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.0010275956010445952\n",
      "check (1353,) ()\n",
      "epoch 58\n",
      "====================================\n",
      "Epoch:  58 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.310344827586206\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.00045591272646561265\n",
      "check (1424,) ()\n",
      "epoch 59\n",
      "====================================\n",
      "Epoch:  59 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.28813559322034\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.00039911805652081966\n",
      "check (1473,) ()\n",
      "epoch 60\n",
      "====================================\n",
      "Epoch:  60 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.283333333333335\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.00650556618347764\n",
      "check (1550,) ()\n",
      "epoch 61\n",
      "====================================\n",
      "Epoch:  61 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.278688524590162\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:3.881023803842254e-05\n",
      "check (1404,) ()\n",
      "epoch 62\n",
      "====================================\n",
      "Epoch:  62 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.29032258064516\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.00592611450701952\n",
      "check (1091,) ()\n",
      "epoch 63\n",
      "====================================\n",
      "Epoch:  63 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.3015873015873\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.002404907951131463\n",
      "check (1277,) ()\n",
      "epoch 64\n",
      "====================================\n",
      "Epoch:  64 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.3125\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.0026518520899116993\n",
      "check (1670,) ()\n",
      "epoch 65\n",
      "====================================\n",
      "Epoch:  65 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.29230769230769\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.002071254886686802\n",
      "check (1830,) ()\n",
      "epoch 66\n",
      "====================================\n",
      "Epoch:  66 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.272727272727273\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.002689678454771638\n",
      "check (1566,) ()\n",
      "epoch 67\n",
      "====================================\n",
      "Epoch:  67 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.28358208955224\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.006651353556662798\n",
      "check (1509,) ()\n",
      "epoch 68\n",
      "====================================\n",
      "Epoch:  68 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.294117647058822\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.004268563352525234\n",
      "check (1113,) ()\n",
      "epoch 69\n",
      "====================================\n",
      "Epoch:  69 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.304347826086957\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.0011260948376730084\n",
      "check (1534,) ()\n",
      "epoch 70\n",
      "====================================\n",
      "Epoch:  70 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.3\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.0007815926801413298\n",
      "check (1641,) ()\n",
      "epoch 71\n",
      "====================================\n",
      "Epoch:  71 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.295774647887324\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.0016897801542654634\n",
      "check (1190,) ()\n",
      "epoch 72\n",
      "====================================\n",
      "Epoch:  72 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.305555555555557\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:9.11985116545111e-05\n",
      "check (1233,) ()\n",
      "epoch 73\n",
      "====================================\n",
      "Epoch:  73 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.301369863013697\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.001831462257541716\n",
      "check (1241,) ()\n",
      "epoch 74\n",
      "====================================\n",
      "Epoch:  74 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.2972972972973\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.0029952905606478453\n",
      "check (2001,) ()\n",
      "epoch 75\n",
      "====================================\n",
      "Epoch:  75 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.28\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:0.0005200103623792529\n",
      "check (1593,) ()\n",
      "epoch 76\n",
      "====================================\n",
      "Epoch:  76 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.263157894736842\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.0019462906057015061\n",
      "check (1865,) ()\n",
      "epoch 77\n",
      "====================================\n",
      "Epoch:  77 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.25974025974026\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.00471147708594799\n",
      "check (1317,) ()\n",
      "epoch 78\n",
      "====================================\n",
      "Epoch:  78 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.256410256410255\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.005717628635466099\n",
      "check (1519,) ()\n",
      "epoch 79\n",
      "====================================\n",
      "Epoch:  79 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.240506329113924\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.005632883869111538\n",
      "check (1504,) ()\n",
      "epoch 80\n",
      "====================================\n",
      "Epoch:  80 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.25\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.00025847365031950176\n",
      "check (1501,) ()\n",
      "epoch 81\n",
      "====================================\n",
      "Epoch:  81 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.25925925925926\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.004850368015468121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1353,) ()\n",
      "epoch 82\n",
      "====================================\n",
      "Epoch:  82 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.24390243902439\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-3.970611942349933e-05\n",
      "check (1583,) ()\n",
      "epoch 83\n",
      "====================================\n",
      "Epoch:  83 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.253012048192772\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.0019089945126324892\n",
      "check (1552,) ()\n",
      "epoch 84\n",
      "====================================\n",
      "Epoch:  84 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.25\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.009283862076699734\n",
      "check (1462,) ()\n",
      "epoch 85\n",
      "====================================\n",
      "Epoch:  85 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.24705882352941\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.006745730992406607\n",
      "check (1498,) ()\n",
      "epoch 86\n",
      "====================================\n",
      "Epoch:  86 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.25581395348837\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.0010477942414581776\n",
      "check (1438,) ()\n",
      "epoch 87\n",
      "====================================\n",
      "Epoch:  87 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.264367816091955\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.012718654237687588\n",
      "check (1504,) ()\n",
      "epoch 88\n",
      "====================================\n",
      "Epoch:  88 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.272727272727273\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.0016320334980264306\n",
      "check (1710,) ()\n",
      "epoch 89\n",
      "====================================\n",
      "Epoch:  89 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.269662921348313\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.007232076022773981\n",
      "check (1885,) ()\n",
      "epoch 90\n",
      "====================================\n",
      "Epoch:  90 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -20.244444444444444\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:0.002897002035751939\n",
      "check (1602,) ()\n",
      "epoch 91\n",
      "====================================\n",
      "Epoch:  91 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.252747252747252\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.005917900241911411\n",
      "check (1423,) ()\n",
      "epoch 92\n",
      "====================================\n",
      "Epoch:  92 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.26086956521739\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.004175766371190548\n",
      "check (1592,) ()\n",
      "epoch 93\n",
      "====================================\n",
      "Epoch:  93 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.247311827956988\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.0008611918892711401\n",
      "check (1098,) ()\n",
      "epoch 94\n",
      "====================================\n",
      "Epoch:  94 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.25531914893617\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.003969067241996527\n",
      "check (1542,) ()\n",
      "epoch 95\n",
      "====================================\n",
      "Epoch:  95 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.25263157894737\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.0022944810334593058\n",
      "check (1428,) ()\n",
      "epoch 96\n",
      "====================================\n",
      "Epoch:  96 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.260416666666668\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.00030083482852205634\n",
      "check (1608,) ()\n",
      "epoch 97\n",
      "====================================\n",
      "Epoch:  97 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.257731958762886\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.0041621383279562\n",
      "check (1826,) ()\n",
      "epoch 98\n",
      "====================================\n",
      "Epoch:  98 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.244897959183675\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:0.0030917786061763763\n",
      "check (1552,) ()\n",
      "epoch 99\n",
      "====================================\n",
      "Epoch:  99 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.242424242424242\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.0012330911122262478\n",
      "check (1850,) ()\n",
      "epoch 100\n",
      "====================================\n",
      "Epoch:  100 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.24\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.010056783445179462\n",
      "Model saved\n",
      "check (1753,) ()\n",
      "epoch 101\n",
      "====================================\n",
      "Epoch:  101 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.22772277227723\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.0002123065060004592\n",
      "check (1394,) ()\n",
      "epoch 102\n",
      "====================================\n",
      "Epoch:  102 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.225490196078432\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.004646400921046734\n",
      "check (1688,) ()\n",
      "epoch 103\n",
      "====================================\n",
      "Epoch:  103 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.21359223300971\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.008468382060527802\n",
      "check (1591,) ()\n",
      "epoch 104\n",
      "====================================\n",
      "Epoch:  104 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.221153846153847\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.002345350570976734\n",
      "check (1741,) ()\n",
      "epoch 105\n",
      "====================================\n",
      "Epoch:  105 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.228571428571428\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.010690297931432724\n",
      "check (1715,) ()\n",
      "epoch 106\n",
      "====================================\n",
      "Epoch:  106 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -20.20754716981132\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.0028012050315737724\n",
      "check (1591,) ()\n",
      "epoch 107\n",
      "====================================\n",
      "Epoch:  107 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.214953271028037\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.0022879024036228657\n",
      "check (1231,) ()\n",
      "epoch 108\n",
      "====================================\n",
      "Epoch:  108 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.212962962962962\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.0018418573308736086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1925,) ()\n",
      "epoch 109\n",
      "====================================\n",
      "Epoch:  109 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.201834862385322\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:0.0005045071011409163\n",
      "check (1588,) ()\n",
      "epoch 110\n",
      "====================================\n",
      "Epoch:  110 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.20909090909091\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.001050771214067936\n",
      "check (1588,) ()\n",
      "epoch 111\n",
      "====================================\n",
      "Epoch:  111 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.216216216216218\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.0043388912454247475\n",
      "check (1842,) ()\n",
      "epoch 112\n",
      "====================================\n",
      "Epoch:  112 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.223214285714285\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-3.756814112421125e-05\n",
      "check (1453,) ()\n",
      "epoch 113\n",
      "====================================\n",
      "Epoch:  113 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.221238938053098\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.006972962059080601\n",
      "check (2318,) ()\n",
      "epoch 114\n",
      "====================================\n",
      "Epoch:  114 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.228070175438596\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.002827409887686372\n",
      "check (1879,) ()\n",
      "epoch 115\n",
      "====================================\n",
      "Epoch:  115 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -20.208695652173912\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.0018050847575068474\n",
      "check (1776,) ()\n",
      "epoch 116\n",
      "====================================\n",
      "Epoch:  116 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.198275862068964\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:0.006575077306479216\n",
      "check (1669,) ()\n",
      "epoch 117\n",
      "====================================\n",
      "Epoch:  117 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.188034188034187\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.0017066827276721597\n",
      "check (1842,) ()\n",
      "epoch 118\n",
      "====================================\n",
      "Epoch:  118 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.194915254237287\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.007429677993059158\n",
      "check (1596,) ()\n",
      "epoch 119\n",
      "====================================\n",
      "Epoch:  119 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.18487394957983\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.0038212535437196493\n",
      "check (1838,) ()\n",
      "epoch 120\n",
      "====================================\n",
      "Epoch:  120 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.183333333333334\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.004556882195174694\n",
      "check (1976,) ()\n",
      "epoch 121\n",
      "====================================\n",
      "Epoch:  121 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -20.165289256198346\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.005736063234508038\n",
      "check (1557,) ()\n",
      "epoch 122\n",
      "====================================\n",
      "Epoch:  122 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.16393442622951\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.002356962999328971\n",
      "check (1871,) ()\n",
      "epoch 123\n",
      "====================================\n",
      "Epoch:  123 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.16260162601626\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.007020884193480015\n",
      "check (1336,) ()\n",
      "epoch 124\n",
      "====================================\n",
      "Epoch:  124 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.169354838709676\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.015705451369285583\n",
      "check (1707,) ()\n",
      "epoch 125\n",
      "====================================\n",
      "Epoch:  125 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -20.152\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.001852156245149672\n",
      "check (1707,) ()\n",
      "epoch 126\n",
      "====================================\n",
      "Epoch:  126 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.150793650793652\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.0030723358504474163\n",
      "check (1898,) ()\n",
      "epoch 127\n",
      "====================================\n",
      "Epoch:  127 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.141732283464567\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:0.00497650820761919\n",
      "check (1880,) ()\n",
      "epoch 128\n",
      "====================================\n",
      "Epoch:  128 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.140625\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.010785411112010479\n",
      "check (1867,) ()\n",
      "epoch 129\n",
      "====================================\n",
      "Epoch:  129 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.13953488372093\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.0032522205729037523\n",
      "check (1262,) ()\n",
      "epoch 130\n",
      "====================================\n",
      "Epoch:  130 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.146153846153847\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.0034600996877998114\n",
      "check (1426,) ()\n",
      "epoch 131\n",
      "====================================\n",
      "Epoch:  131 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.15267175572519\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.0006873637903481722\n",
      "check (1813,) ()\n",
      "epoch 132\n",
      "====================================\n",
      "Epoch:  132 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -20.136363636363637\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.00786126870661974\n",
      "check (2241,) ()\n",
      "epoch 133\n",
      "====================================\n",
      "Epoch:  133 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.127819548872182\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.0010796678252518177\n",
      "check (1251,) ()\n",
      "epoch 134\n",
      "====================================\n",
      "Epoch:  134 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.134328358208954\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.0068433852866292\n",
      "check (1629,) ()\n",
      "epoch 135\n",
      "====================================\n",
      "Epoch:  135 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.133333333333333\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.0028433422558009624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1901,) ()\n",
      "epoch 136\n",
      "====================================\n",
      "Epoch:  136 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.139705882352942\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.006101552862673998\n",
      "check (1439,) ()\n",
      "epoch 137\n",
      "====================================\n",
      "Epoch:  137 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.145985401459853\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.010783376172184944\n",
      "check (1561,) ()\n",
      "epoch 138\n",
      "====================================\n",
      "Epoch:  138 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -20.130434782608695\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.0033890176564455032\n",
      "check (1780,) ()\n",
      "epoch 139\n",
      "====================================\n",
      "Epoch:  139 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.1294964028777\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.011936777271330357\n",
      "check (2007,) ()\n",
      "epoch 140\n",
      "====================================\n",
      "Epoch:  140 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.135714285714286\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.006963654421269894\n",
      "check (1864,) ()\n",
      "epoch 141\n",
      "====================================\n",
      "Epoch:  141 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -20.120567375886523\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.005899124313145876\n",
      "check (1984,) ()\n",
      "epoch 142\n",
      "====================================\n",
      "Epoch:  142 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.12676056338028\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.004744804929941893\n",
      "check (1898,) ()\n",
      "epoch 143\n",
      "====================================\n",
      "Epoch:  143 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -20.111888111888113\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:0.0006829996127635241\n",
      "check (1720,) ()\n",
      "epoch 144\n",
      "====================================\n",
      "Epoch:  144 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.11111111111111\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.009513434953987598\n",
      "check (2159,) ()\n",
      "epoch 145\n",
      "====================================\n",
      "Epoch:  145 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.103448275862068\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:0.013357996009290218\n",
      "check (2242,) ()\n",
      "epoch 146\n",
      "====================================\n",
      "Epoch:  146 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.095890410958905\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.004035830497741699\n",
      "check (2075,) ()\n",
      "epoch 147\n",
      "====================================\n",
      "Epoch:  147 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.08843537414966\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.0011539123952388763\n",
      "check (2091,) ()\n",
      "epoch 148\n",
      "====================================\n",
      "Epoch:  148 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.094594594594593\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.008544333279132843\n",
      "check (1534,) ()\n",
      "epoch 149\n",
      "====================================\n",
      "Epoch:  149 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.093959731543624\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.005433565005660057\n",
      "check (1636,) ()\n",
      "epoch 150\n",
      "====================================\n",
      "Epoch:  150 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.093333333333334\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.007850364781916142\n",
      "Model saved\n",
      "check (1910,) ()\n",
      "epoch 151\n",
      "====================================\n",
      "Epoch:  151 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.09933774834437\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.0019438927993178368\n",
      "check (1719,) ()\n",
      "epoch 152\n",
      "====================================\n",
      "Epoch:  152 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.098684210526315\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.0006118051242083311\n",
      "check (1632,) ()\n",
      "epoch 153\n",
      "====================================\n",
      "Epoch:  153 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.098039215686274\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.0020074774511158466\n",
      "check (1835,) ()\n",
      "epoch 154\n",
      "====================================\n",
      "Epoch:  154 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.103896103896105\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.0070482539013028145\n",
      "check (2005,) ()\n",
      "epoch 155\n",
      "====================================\n",
      "Epoch:  155 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.096774193548388\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:0.001220155623741448\n",
      "check (1605,) ()\n",
      "epoch 156\n",
      "====================================\n",
      "Epoch:  156 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.08974358974359\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.007429333403706551\n",
      "check (1437,) ()\n",
      "epoch 157\n",
      "====================================\n",
      "Epoch:  157 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.095541401273884\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.0023068126756697893\n",
      "check (2165,) ()\n",
      "epoch 158\n",
      "====================================\n",
      "Epoch:  158 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-17.0\n",
      "Mean Reward of that batch -17.0\n",
      "Average Reward of all training: -20.075949367088608\n",
      "Max reward for a batch so far: -15.0\n",
      "check -17.0\n",
      "Training loss:0.003284288803115487\n",
      "check (1961,) ()\n",
      "epoch 159\n",
      "====================================\n",
      "Epoch:  159 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.07547169811321\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.007520513143390417\n",
      "check (1510,) ()\n",
      "epoch 160\n",
      "====================================\n",
      "Epoch:  160 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.08125\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.008038272149860859\n",
      "check (1579,) ()\n",
      "epoch 161\n",
      "====================================\n",
      "Epoch:  161 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.074534161490682\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.0007658071117475629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (2459,) ()\n",
      "epoch 162\n",
      "====================================\n",
      "Epoch:  162 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-16.0\n",
      "Mean Reward of that batch -16.0\n",
      "Average Reward of all training: -20.049382716049383\n",
      "Max reward for a batch so far: -15.0\n",
      "check -16.0\n",
      "Training loss:-0.00017795624444261193\n",
      "check (1596,) ()\n",
      "epoch 163\n",
      "====================================\n",
      "Epoch:  163 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.05521472392638\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.006633633282035589\n",
      "check (1727,) ()\n",
      "epoch 164\n",
      "====================================\n",
      "Epoch:  164 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.0609756097561\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.005825598258525133\n",
      "check (1685,) ()\n",
      "epoch 165\n",
      "====================================\n",
      "Epoch:  165 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.054545454545455\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.000725192076060921\n",
      "check (1689,) ()\n",
      "epoch 166\n",
      "====================================\n",
      "Epoch:  166 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.06024096385542\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.0064206901006400585\n",
      "check (1795,) ()\n",
      "epoch 167\n",
      "====================================\n",
      "Epoch:  167 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.059880239520957\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.0043420009315013885\n",
      "check (1839,) ()\n",
      "epoch 168\n",
      "====================================\n",
      "Epoch:  168 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.06547619047619\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.0013997122878208756\n",
      "check (1742,) ()\n",
      "epoch 169\n",
      "====================================\n",
      "Epoch:  169 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.071005917159763\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.012498468160629272\n",
      "check (1685,) ()\n",
      "epoch 170\n",
      "====================================\n",
      "Epoch:  170 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.064705882352943\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.011729861609637737\n",
      "check (1921,) ()\n",
      "epoch 171\n",
      "====================================\n",
      "Epoch:  171 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.07017543859649\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.004075891803950071\n",
      "check (1683,) ()\n",
      "epoch 172\n",
      "====================================\n",
      "Epoch:  172 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.063953488372093\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:1.1250296665821224e-05\n",
      "check (1622,) ()\n",
      "epoch 173\n",
      "====================================\n",
      "Epoch:  173 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.063583815028903\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.006722155027091503\n",
      "check (1962,) ()\n",
      "epoch 174\n",
      "====================================\n",
      "Epoch:  174 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.063218390804597\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.001435871934518218\n",
      "check (2091,) ()\n",
      "epoch 175\n",
      "====================================\n",
      "Epoch:  175 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.057142857142857\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.0068725100718438625\n",
      "check (1873,) ()\n",
      "epoch 176\n",
      "====================================\n",
      "Epoch:  176 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -20.045454545454547\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:0.0050781103782355785\n",
      "check (2265,) ()\n",
      "epoch 177\n",
      "====================================\n",
      "Epoch:  177 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -20.033898305084747\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:0.0046012625098228455\n",
      "check (1559,) ()\n",
      "epoch 178\n",
      "====================================\n",
      "Epoch:  178 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.03370786516854\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.004805747885257006\n",
      "check (1540,) ()\n",
      "epoch 179\n",
      "====================================\n",
      "Epoch:  179 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.033519553072626\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.004677038174122572\n",
      "check (1807,) ()\n",
      "epoch 180\n",
      "====================================\n",
      "Epoch:  180 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.033333333333335\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.012471768073737621\n",
      "check (1845,) ()\n",
      "epoch 181\n",
      "====================================\n",
      "Epoch:  181 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.03867403314917\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.010992437601089478\n",
      "check (1845,) ()\n",
      "epoch 182\n",
      "====================================\n",
      "Epoch:  182 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.032967032967033\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.010028690099716187\n",
      "check (1276,) ()\n",
      "epoch 183\n",
      "====================================\n",
      "Epoch:  183 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.038251366120218\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.00011151113722007722\n",
      "check (2120,) ()\n",
      "epoch 184\n",
      "====================================\n",
      "Epoch:  184 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -20.027173913043477\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.014876828528940678\n",
      "check (2105,) ()\n",
      "epoch 185\n",
      "====================================\n",
      "Epoch:  185 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.027027027027028\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.000128800849779509\n",
      "check (1693,) ()\n",
      "epoch 186\n",
      "====================================\n",
      "Epoch:  186 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.026881720430108\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.0011835576733574271\n",
      "check (1796,) ()\n",
      "epoch 187\n",
      "====================================\n",
      "Epoch:  187 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.02673796791444\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.0004486942198127508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1646,) ()\n",
      "epoch 188\n",
      "====================================\n",
      "Epoch:  188 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.02659574468085\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.017741849645972252\n",
      "check (1924,) ()\n",
      "epoch 189\n",
      "====================================\n",
      "Epoch:  189 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.03174603174603\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.021157171577215195\n",
      "check (1403,) ()\n",
      "epoch 190\n",
      "====================================\n",
      "Epoch:  190 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.03684210526316\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.00945881474763155\n",
      "check (2072,) ()\n",
      "epoch 191\n",
      "====================================\n",
      "Epoch:  191 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.031413612565444\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.004243801813572645\n",
      "check (1742,) ()\n",
      "epoch 192\n",
      "====================================\n",
      "Epoch:  192 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.03125\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.010890062898397446\n",
      "check (1645,) ()\n",
      "epoch 193\n",
      "====================================\n",
      "Epoch:  193 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -20.020725388601036\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:0.0028615703340619802\n",
      "check (2185,) ()\n",
      "epoch 194\n",
      "====================================\n",
      "Epoch:  194 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.02061855670103\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.0019489838741719723\n",
      "check (1520,) ()\n",
      "epoch 195\n",
      "====================================\n",
      "Epoch:  195 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.015384615384615\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.00477564474567771\n",
      "check (1801,) ()\n",
      "epoch 196\n",
      "====================================\n",
      "Epoch:  196 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.01530612244898\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.007828828878700733\n",
      "check (2604,) ()\n",
      "epoch 197\n",
      "====================================\n",
      "Epoch:  197 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.01522842639594\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.0010892689460888505\n",
      "check (2162,) ()\n",
      "epoch 198\n",
      "====================================\n",
      "Epoch:  198 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.01010101010101\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.005042531993240118\n",
      "check (1906,) ()\n",
      "epoch 199\n",
      "====================================\n",
      "Epoch:  199 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.01005025125628\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.0029552746564149857\n",
      "check (1709,) ()\n",
      "epoch 200\n",
      "====================================\n",
      "Epoch:  200 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.01\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.0007843924104236066\n",
      "Model saved\n",
      "check (1887,) ()\n",
      "epoch 201\n",
      "====================================\n",
      "Epoch:  201 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.00995024875622\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.009317965246737003\n",
      "check (1500,) ()\n",
      "epoch 202\n",
      "====================================\n",
      "Epoch:  202 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.004950495049506\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:0.0038594005163758993\n",
      "check (1636,) ()\n",
      "epoch 203\n",
      "====================================\n",
      "Epoch:  203 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.004926108374384\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.019893541932106018\n",
      "check (2139,) ()\n",
      "epoch 204\n",
      "====================================\n",
      "Epoch:  204 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -19.995098039215687\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.003151498967781663\n",
      "check (1972,) ()\n",
      "epoch 205\n",
      "====================================\n",
      "Epoch:  205 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.0\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.00952046550810337\n",
      "check (2196,) ()\n",
      "epoch 206\n",
      "====================================\n",
      "Epoch:  206 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.0\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.010016866959631443\n",
      "check (2245,) ()\n",
      "epoch 207\n",
      "====================================\n",
      "Epoch:  207 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.995169082125603\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:0.0035071752499789\n",
      "check (1953,) ()\n",
      "epoch 208\n",
      "====================================\n",
      "Epoch:  208 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -19.985576923076923\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.003594211535528302\n",
      "check (1887,) ()\n",
      "epoch 209\n",
      "====================================\n",
      "Epoch:  209 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -19.976076555023923\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.0011352787259966135\n",
      "check (2058,) ()\n",
      "epoch 210\n",
      "====================================\n",
      "Epoch:  210 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -19.966666666666665\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:0.008598127402365208\n",
      "check (2098,) ()\n",
      "epoch 211\n",
      "====================================\n",
      "Epoch:  211 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-17.0\n",
      "Mean Reward of that batch -17.0\n",
      "Average Reward of all training: -19.95260663507109\n",
      "Max reward for a batch so far: -15.0\n",
      "check -17.0\n",
      "Training loss:-0.004681635182350874\n",
      "check (1757,) ()\n",
      "epoch 212\n",
      "====================================\n",
      "Epoch:  212 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.94811320754717\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.00854899175465107\n",
      "check (2005,) ()\n",
      "epoch 213\n",
      "====================================\n",
      "Epoch:  213 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.943661971830984\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.00767795043066144\n",
      "check (2094,) ()\n",
      "epoch 214\n",
      "====================================\n",
      "Epoch:  214 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.94392523364486\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.006458005402237177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1929,) ()\n",
      "epoch 215\n",
      "====================================\n",
      "Epoch:  215 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.94418604651163\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.0059195104986429214\n",
      "check (2035,) ()\n",
      "epoch 216\n",
      "====================================\n",
      "Epoch:  216 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.944444444444443\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.0038395302835851908\n",
      "check (1911,) ()\n",
      "epoch 217\n",
      "====================================\n",
      "Epoch:  217 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-17.0\n",
      "Mean Reward of that batch -17.0\n",
      "Average Reward of all training: -19.930875576036865\n",
      "Max reward for a batch so far: -15.0\n",
      "check -17.0\n",
      "Training loss:-0.011540875770151615\n",
      "check (1889,) ()\n",
      "epoch 218\n",
      "====================================\n",
      "Epoch:  218 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.931192660550458\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.02147153578698635\n",
      "check (2202,) ()\n",
      "epoch 219\n",
      "====================================\n",
      "Epoch:  219 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -19.922374429223744\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.012649455107748508\n",
      "check (1959,) ()\n",
      "epoch 220\n",
      "====================================\n",
      "Epoch:  220 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -19.913636363636364\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:0.0156551580876112\n",
      "check (1751,) ()\n",
      "epoch 221\n",
      "====================================\n",
      "Epoch:  221 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.90950226244344\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.017776058986783028\n",
      "check (1746,) ()\n",
      "epoch 222\n",
      "====================================\n",
      "Epoch:  222 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -19.914414414414413\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.009375795722007751\n",
      "check (1935,) ()\n",
      "epoch 223\n",
      "====================================\n",
      "Epoch:  223 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.91031390134529\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:0.0010188119485974312\n",
      "check (2060,) ()\n",
      "epoch 224\n",
      "====================================\n",
      "Epoch:  224 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -19.915178571428573\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.0007738057756796479\n",
      "check (2309,) ()\n",
      "epoch 225\n",
      "====================================\n",
      "Epoch:  225 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -19.906666666666666\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.0021121734753251076\n",
      "check (1919,) ()\n",
      "epoch 226\n",
      "====================================\n",
      "Epoch:  226 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.902654867256636\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.004409648012369871\n",
      "check (2133,) ()\n",
      "epoch 227\n",
      "====================================\n",
      "Epoch:  227 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -19.894273127753305\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.009353521279990673\n",
      "check (1845,) ()\n",
      "epoch 228\n",
      "====================================\n",
      "Epoch:  228 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.890350877192983\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.01981150358915329\n",
      "check (2223,) ()\n",
      "epoch 229\n",
      "====================================\n",
      "Epoch:  229 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.88646288209607\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.002436436712741852\n",
      "check (2047,) ()\n",
      "epoch 230\n",
      "====================================\n",
      "Epoch:  230 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.88695652173913\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.0066106803715229034\n",
      "check (2167,) ()\n",
      "epoch 231\n",
      "====================================\n",
      "Epoch:  231 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.883116883116884\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.008286717347800732\n",
      "check (1504,) ()\n",
      "epoch 232\n",
      "====================================\n",
      "Epoch:  232 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -19.887931034482758\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.008765813894569874\n",
      "check (1799,) ()\n",
      "epoch 233\n",
      "====================================\n",
      "Epoch:  233 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -19.892703862660944\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.012795480899512768\n",
      "check (1811,) ()\n",
      "epoch 234\n",
      "====================================\n",
      "Epoch:  234 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.88888888888889\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.009483113884925842\n",
      "check (1995,) ()\n",
      "epoch 235\n",
      "====================================\n",
      "Epoch:  235 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.885106382978723\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.005198552738875151\n",
      "check (2259,) ()\n",
      "epoch 236\n",
      "====================================\n",
      "Epoch:  236 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.88135593220339\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:0.00797741673886776\n",
      "check (1715,) ()\n",
      "epoch 237\n",
      "====================================\n",
      "Epoch:  237 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.88185654008439\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.007800171617418528\n",
      "check (1893,) ()\n",
      "epoch 238\n",
      "====================================\n",
      "Epoch:  238 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -19.886554621848738\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.001871428918093443\n",
      "check (2254,) ()\n",
      "epoch 239\n",
      "====================================\n",
      "Epoch:  239 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-17.0\n",
      "Mean Reward of that batch -17.0\n",
      "Average Reward of all training: -19.8744769874477\n",
      "Max reward for a batch so far: -15.0\n",
      "check -17.0\n",
      "Training loss:-0.0031378744170069695\n",
      "check (1895,) ()\n",
      "epoch 240\n",
      "====================================\n",
      "Epoch:  240 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.875\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.005760720930993557\n",
      "check (2220,) ()\n",
      "epoch 241\n",
      "====================================\n",
      "Epoch:  241 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -19.86721991701245\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:0.004633784759789705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (1949,) ()\n",
      "epoch 242\n",
      "====================================\n",
      "Epoch:  242 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.867768595041323\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.006432111840695143\n",
      "check (1599,) ()\n",
      "epoch 243\n",
      "====================================\n",
      "Epoch:  243 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -19.872427983539094\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.012854065746068954\n",
      "check (2012,) ()\n",
      "epoch 244\n",
      "====================================\n",
      "Epoch:  244 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.868852459016395\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.01517198421061039\n",
      "check (1781,) ()\n",
      "epoch 245\n",
      "====================================\n",
      "Epoch:  245 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.86938775510204\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.01251991093158722\n",
      "check (1605,) ()\n",
      "epoch 246\n",
      "====================================\n",
      "Epoch:  246 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.865853658536587\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.006050598341971636\n",
      "check (2411,) ()\n",
      "epoch 247\n",
      "====================================\n",
      "Epoch:  247 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.86234817813765\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:0.00015442325093317777\n",
      "check (2484,) ()\n",
      "epoch 248\n",
      "====================================\n",
      "Epoch:  248 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-17.0\n",
      "Mean Reward of that batch -17.0\n",
      "Average Reward of all training: -19.850806451612904\n",
      "Max reward for a batch so far: -15.0\n",
      "check -17.0\n",
      "Training loss:-0.010577847249805927\n",
      "check (1782,) ()\n",
      "epoch 249\n",
      "====================================\n",
      "Epoch:  249 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.85140562248996\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.005385537166148424\n",
      "check (2083,) ()\n",
      "epoch 250\n",
      "====================================\n",
      "Epoch:  250 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.848\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.0010833318810909986\n",
      "Model saved\n",
      "check (1975,) ()\n",
      "epoch 251\n",
      "====================================\n",
      "Epoch:  251 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -19.852589641434264\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.01140151359140873\n",
      "check (1807,) ()\n",
      "epoch 252\n",
      "====================================\n",
      "Epoch:  252 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -19.845238095238095\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.0018517326097935438\n",
      "check (2034,) ()\n",
      "epoch 253\n",
      "====================================\n",
      "Epoch:  253 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.845849802371543\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.0015389996115118265\n",
      "check (2241,) ()\n",
      "epoch 254\n",
      "====================================\n",
      "Epoch:  254 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.84251968503937\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.004398229997605085\n",
      "check (2473,) ()\n",
      "epoch 255\n",
      "====================================\n",
      "Epoch:  255 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -19.83529411764706\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:0.0060350592248141766\n",
      "check (2419,) ()\n",
      "epoch 256\n",
      "====================================\n",
      "Epoch:  256 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.83203125\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.006063676439225674\n",
      "check (2081,) ()\n",
      "epoch 257\n",
      "====================================\n",
      "Epoch:  257 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -19.836575875486382\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.026853390038013458\n",
      "check (1545,) ()\n",
      "epoch 258\n",
      "====================================\n",
      "Epoch:  258 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.837209302325583\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.0016621481627225876\n",
      "check (2325,) ()\n",
      "epoch 259\n",
      "====================================\n",
      "Epoch:  259 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -19.841698841698843\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.0035553856287151575\n",
      "check (2775,) ()\n",
      "epoch 260\n",
      "====================================\n",
      "Epoch:  260 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -19.834615384615386\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.0009333196212537587\n",
      "check (1742,) ()\n",
      "epoch 261\n",
      "====================================\n",
      "Epoch:  261 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -19.839080459770116\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.013321062549948692\n",
      "check (2156,) ()\n",
      "epoch 262\n",
      "====================================\n",
      "Epoch:  262 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.83587786259542\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.01510333176702261\n",
      "check (2576,) ()\n",
      "epoch 263\n",
      "====================================\n",
      "Epoch:  263 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-17.0\n",
      "Mean Reward of that batch -17.0\n",
      "Average Reward of all training: -19.825095057034222\n",
      "Max reward for a batch so far: -15.0\n",
      "check -17.0\n",
      "Training loss:-0.0033381429966539145\n",
      "check (2127,) ()\n",
      "epoch 264\n",
      "====================================\n",
      "Epoch:  264 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -19.818181818181817\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:4.314266698202118e-05\n",
      "check (2353,) ()\n",
      "epoch 265\n",
      "====================================\n",
      "Epoch:  265 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -19.81132075471698\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.011126341298222542\n",
      "check (2590,) ()\n",
      "epoch 266\n",
      "====================================\n",
      "Epoch:  266 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-16.0\n",
      "Mean Reward of that batch -16.0\n",
      "Average Reward of all training: -19.796992481203006\n",
      "Max reward for a batch so far: -15.0\n",
      "check -16.0\n",
      "Training loss:-0.006523529998958111\n",
      "check (2394,) ()\n",
      "epoch 267\n",
      "====================================\n",
      "Epoch:  267 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -19.801498127340825\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.0027914373204112053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (2229,) ()\n",
      "epoch 268\n",
      "====================================\n",
      "Epoch:  268 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -19.80597014925373\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:0.0002413116890238598\n",
      "check (2084,) ()\n",
      "epoch 269\n",
      "====================================\n",
      "Epoch:  269 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.802973977695167\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.010626292787492275\n",
      "check (1977,) ()\n",
      "epoch 270\n",
      "====================================\n",
      "Epoch:  270 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -19.807407407407407\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.0006981996120885015\n",
      "check (2583,) ()\n",
      "epoch 271\n",
      "====================================\n",
      "Epoch:  271 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-15.0\n",
      "Mean Reward of that batch -15.0\n",
      "Average Reward of all training: -19.789667896678967\n",
      "Max reward for a batch so far: -15.0\n",
      "check -15.0\n",
      "Training loss:-0.0025081937201321125\n",
      "check (1997,) ()\n",
      "epoch 272\n",
      "====================================\n",
      "Epoch:  272 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -19.794117647058822\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.008774396032094955\n",
      "check (2410,) ()\n",
      "epoch 273\n",
      "====================================\n",
      "Epoch:  273 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-17.0\n",
      "Mean Reward of that batch -17.0\n",
      "Average Reward of all training: -19.783882783882785\n",
      "Max reward for a batch so far: -15.0\n",
      "check -17.0\n",
      "Training loss:-0.012887291610240936\n",
      "check (2138,) ()\n",
      "epoch 274\n",
      "====================================\n",
      "Epoch:  274 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.784671532846716\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.009772945195436478\n",
      "check (2453,) ()\n",
      "epoch 275\n",
      "====================================\n",
      "Epoch:  275 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -19.778181818181817\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.007849987596273422\n",
      "check (1856,) ()\n",
      "epoch 276\n",
      "====================================\n",
      "Epoch:  276 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.77536231884058\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.014742481522262096\n",
      "check (2072,) ()\n",
      "epoch 277\n",
      "====================================\n",
      "Epoch:  277 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.772563176895307\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:0.004126136191189289\n",
      "check (2190,) ()\n",
      "epoch 278\n",
      "====================================\n",
      "Epoch:  278 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.77338129496403\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.007043831516057253\n",
      "check (2173,) ()\n",
      "epoch 279\n",
      "====================================\n",
      "Epoch:  279 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.770609318996417\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:0.00020444091933313757\n",
      "check (2359,) ()\n",
      "epoch 280\n",
      "====================================\n",
      "Epoch:  280 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -19.764285714285716\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.0032661648001521826\n",
      "check (2593,) ()\n",
      "epoch 281\n",
      "====================================\n",
      "Epoch:  281 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-15.0\n",
      "Mean Reward of that batch -15.0\n",
      "Average Reward of all training: -19.747330960854093\n",
      "Max reward for a batch so far: -15.0\n",
      "check -15.0\n",
      "Training loss:0.004295488819479942\n",
      "check (2813,) ()\n",
      "epoch 282\n",
      "====================================\n",
      "Epoch:  282 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-16.0\n",
      "Mean Reward of that batch -16.0\n",
      "Average Reward of all training: -19.73404255319149\n",
      "Max reward for a batch so far: -15.0\n",
      "check -16.0\n",
      "Training loss:-0.006736951880156994\n",
      "check (2233,) ()\n",
      "epoch 283\n",
      "====================================\n",
      "Epoch:  283 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.731448763250885\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.021643802523612976\n",
      "check (1907,) ()\n",
      "epoch 284\n",
      "====================================\n",
      "Epoch:  284 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -19.735915492957748\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.0238652266561985\n",
      "check (2229,) ()\n",
      "epoch 285\n",
      "====================================\n",
      "Epoch:  285 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.733333333333334\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.013596166856586933\n",
      "check (2117,) ()\n",
      "epoch 286\n",
      "====================================\n",
      "Epoch:  286 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.734265734265733\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.006242153700441122\n",
      "check (2131,) ()\n",
      "epoch 287\n",
      "====================================\n",
      "Epoch:  287 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -19.72822299651568\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:0.004239964298903942\n",
      "check (2281,) ()\n",
      "epoch 288\n",
      "====================================\n",
      "Epoch:  288 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.729166666666668\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.001183340442366898\n",
      "check (2294,) ()\n",
      "epoch 289\n",
      "====================================\n",
      "Epoch:  289 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.730103806228374\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.0247481819242239\n",
      "check (2496,) ()\n",
      "epoch 290\n",
      "====================================\n",
      "Epoch:  290 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.72758620689655\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.0057838186621665955\n",
      "check (2317,) ()\n",
      "epoch 291\n",
      "====================================\n",
      "Epoch:  291 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-17.0\n",
      "Mean Reward of that batch -17.0\n",
      "Average Reward of all training: -19.718213058419245\n",
      "Max reward for a batch so far: -15.0\n",
      "check -17.0\n",
      "Training loss:-0.0013691206695511937\n",
      "check (2269,) ()\n",
      "epoch 292\n",
      "====================================\n",
      "Epoch:  292 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.71917808219178\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.0012813566718250513\n",
      "check (2573,) ()\n",
      "epoch 293\n",
      "====================================\n",
      "Epoch:  293 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-16.0\n",
      "Mean Reward of that batch -16.0\n",
      "Average Reward of all training: -19.706484641638227\n",
      "Max reward for a batch so far: -15.0\n",
      "check -16.0\n",
      "Training loss:0.011604513972997665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check (2233,) ()\n",
      "epoch 294\n",
      "====================================\n",
      "Epoch:  294 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -19.700680272108844\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.0004645741428248584\n",
      "check (2480,) ()\n",
      "epoch 295\n",
      "====================================\n",
      "Epoch:  295 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.698305084745762\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.000412762543419376\n",
      "check (2323,) ()\n",
      "epoch 296\n",
      "====================================\n",
      "Epoch:  296 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.695945945945947\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:0.00043820985592901707\n",
      "check (2230,) ()\n",
      "epoch 297\n",
      "====================================\n",
      "Epoch:  297 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.693602693602692\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.014404524117708206\n",
      "check (2269,) ()\n",
      "epoch 298\n",
      "====================================\n",
      "Epoch:  298 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.69463087248322\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.00019592753960750997\n",
      "check (2365,) ()\n",
      "epoch 299\n",
      "====================================\n",
      "Epoch:  299 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -19.68896321070234\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:-0.0010976907797157764\n",
      "check (2174,) ()\n",
      "epoch 300\n",
      "====================================\n",
      "Epoch:  300 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.69\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.018113944679498672\n",
      "Model saved\n",
      "check (2484,) ()\n",
      "epoch 301\n",
      "====================================\n",
      "Epoch:  301 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-17.0\n",
      "Mean Reward of that batch -17.0\n",
      "Average Reward of all training: -19.68106312292359\n",
      "Max reward for a batch so far: -15.0\n",
      "check -17.0\n",
      "Training loss:0.026630213484168053\n",
      "check (2230,) ()\n",
      "epoch 302\n",
      "====================================\n",
      "Epoch:  302 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.678807947019866\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.004622690845280886\n",
      "check (2640,) ()\n",
      "epoch 303\n",
      "====================================\n",
      "Epoch:  303 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.676567656765677\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:0.005966475699096918\n",
      "check (1583,) ()\n",
      "epoch 304\n",
      "====================================\n",
      "Epoch:  304 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -19.68092105263158\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.01717076264321804\n",
      "check (2315,) ()\n",
      "epoch 305\n",
      "====================================\n",
      "Epoch:  305 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.678688524590164\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.011926817707717419\n",
      "check (1750,) ()\n",
      "epoch 306\n",
      "====================================\n",
      "Epoch:  306 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -19.68300653594771\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.00958729162812233\n",
      "check (1884,) ()\n",
      "epoch 307\n",
      "====================================\n",
      "Epoch:  307 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.684039087947884\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.007370310369879007\n",
      "check (2041,) ()\n",
      "epoch 308\n",
      "====================================\n",
      "Epoch:  308 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -19.678571428571427\n",
      "Max reward for a batch so far: -15.0\n",
      "check -18.0\n",
      "Training loss:0.021112751215696335\n",
      "check (2096,) ()\n",
      "epoch 309\n",
      "====================================\n",
      "Epoch:  309 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.676375404530745\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.003775703953579068\n",
      "check (2936,) ()\n",
      "epoch 310\n",
      "====================================\n",
      "Epoch:  310 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-15.0\n",
      "Mean Reward of that batch -15.0\n",
      "Average Reward of all training: -19.661290322580644\n",
      "Max reward for a batch so far: -15.0\n",
      "check -15.0\n",
      "Training loss:-0.00981118343770504\n",
      "check (2189,) ()\n",
      "epoch 311\n",
      "====================================\n",
      "Epoch:  311 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.662379421221864\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:0.006304668262600899\n",
      "check (2601,) ()\n",
      "epoch 312\n",
      "====================================\n",
      "Epoch:  312 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.66346153846154\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.007628979627043009\n",
      "check (2401,) ()\n",
      "epoch 313\n",
      "====================================\n",
      "Epoch:  313 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -19.66773162939297\n",
      "Max reward for a batch so far: -15.0\n",
      "check -21.0\n",
      "Training loss:-0.004256741609424353\n",
      "check (2158,) ()\n",
      "epoch 314\n",
      "====================================\n",
      "Epoch:  314 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.6656050955414\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.009299258701503277\n",
      "check (2487,) ()\n",
      "epoch 315\n",
      "====================================\n",
      "Epoch:  315 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -19.663492063492065\n",
      "Max reward for a batch so far: -15.0\n",
      "check -19.0\n",
      "Training loss:-0.017438989132642746\n",
      "check (2360,) ()\n",
      "epoch 316\n",
      "====================================\n",
      "Epoch:  316 / 10000\n",
      "------------\n",
      "Number of training episodes: 1\n",
      "Total reward:-20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -19.664556962025316\n",
      "Max reward for a batch so far: -15.0\n",
      "check -20.0\n",
      "Training loss:-0.008052718825638294\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if training:\n",
    "    # number of iterations\n",
    "    while epoch < number_epoch +1:\n",
    "        states_mb, actions_mb, rewards_of_batch, discounted_rewards_mb, nb_episodes_mb = make_batch(batch_size)\n",
    "        \n",
    "        total_reward_of_that_batch = np.sum(rewards_of_batch)\n",
    "        print('check',rewards_of_batch.shape, np.array(total_reward_of_that_batch).shape)\n",
    "        allRewards.append(total_reward_of_that_batch)\n",
    "        \n",
    "        mean_reward_of_that_batch = np.divide(total_reward_of_that_batch, nb_episodes_mb)\n",
    "        mean_reward_total.append(mean_reward_of_that_batch)\n",
    "        average_reward_of_all_training = np.divide(np.sum(mean_reward_total), epoch)\n",
    "        maximumRewardRecorded = np.amax(allRewards)\n",
    "        print('epoch',epoch)\n",
    "        epoch_1.append(epoch)\n",
    "        print('====================================')\n",
    "        print(\"Epoch: \", epoch, '/', number_epoch)\n",
    "        print('------------')\n",
    "        print(\"Number of training episodes: {}\".format(nb_episodes_mb))\n",
    "        print(\"Total reward:{}\".format(total_reward_of_that_batch, nb_episodes_mb))\n",
    "        print(\"Mean Reward of that batch {}\".format(mean_reward_of_that_batch))\n",
    "        print(\"Average Reward of all training: {}\".format(average_reward_of_all_training))\n",
    "        print(\"Max reward for a batch so far: {}\".format(maximumRewardRecorded))\n",
    "\n",
    "        loss_,cross, _= sess.run([PGN.loss, PGN.cross_entropy, PGN.train_opt],feed_dict = {PGN.inputs: states_mb.reshape([len(states_mb), *state_size]), PGN.actions: actions_mb,PGN.discounted_episode_rewards: discounted_rewards_mb})    \n",
    "        #loss_VE, _= sess.run([VEN.loss, VEN.train_opt],feed_dict = {VEN.inputs: states_mb.reshape([len(states_mb), *state_size]), VEN.discounted_episode_rewards: discounted_rewards_mb})    \n",
    "\n",
    "        print('check',total_reward_of_that_batch)\n",
    "        print('Training loss:{}'.format(loss_) )\n",
    "        #print('Cross Entropy:{}'.format(cross) )\n",
    "        #print('VE Training loss:{}'.format(loss_VE) )\n",
    "              \n",
    "        if epoch % 50 == 0:\n",
    "            saver.save(sess, \"./models/model2.ckpt\")\n",
    "            print('Model saved')\n",
    "            with open('mean_reward2.txt', 'w') as myFile:\n",
    "                print('{}'.format(average_reward), file=myFile)\n",
    "        epoch += 1\n",
    "        average_reward.append(mean_reward_of_that_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'average_reward_each_episode')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8fa7b513c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_1[:1000], average_reward[:1000])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('average_reward_each_episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"./models/model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personal commnent on the second problem: \n",
    "It is hard to train the Pong game (adjust its hyperparameters to get the convergence result), because its state_size is large. The above figure is what I got with constant baseline and learning rate 1e-3. The line goes up and down means the model is at least learning. It does not lead to convergence result may be because the hyperparameters and the models I tried are not very well. Training the Pong game needs to do a lot of experiments, and the above result is the best I got, even though it does not lead to convergence. My number of epoch is set to be 10000, but I only plotted the first 1000 result because I did not finish training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
